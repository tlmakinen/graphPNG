{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a972d5d9-aa2d-40ea-899b-bf31b149a396",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "a972d5d9-aa2d-40ea-899b-bf31b149a396"
   },
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import imnn.lfi\n",
    "from imnn.utils import value_and_jacrev, value_and_jacfwd\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jraph\n",
    "import jax_cosmo as jc\n",
    "\n",
    "import numpy as onp\n",
    "\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#sns.set_style('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f7f090-bc04-4d2f-a5ff-b368d0056795",
   "metadata": {
    "id": "98f7f090-bc04-4d2f-a5ff-b368d0056795",
    "tags": []
   },
   "source": [
    "# Assembling Dark Matter Halo Graphs\n",
    "\n",
    "To get a better sense of what graphs look like, let's pick apart a single cosmic graph, assembled from a dark matter halo catalogue. This catalogue was constructed from the [*Quijote* simulations](https://quijote-simulations.readthedocs.io/en/latest/index.html). These simulations evolve the Dark Matter field on a $(1\\rm \\ Gpc)^3$ box under gravity from initial conditions to large-scale structure formation. Collections of dark matter particles are then catalogued into hierarchical \"halos\" of different sizes using the Friend of Friends (FoF) algorithm.\n",
    "\n",
    "We're going to use the [`jraph` library](https://github.com/deepmind/jraph) to construct our graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "715fd1cc-2c39-4657-a41f-5f81a2e5a31d",
   "metadata": {
    "cellView": "form",
    "id": "715fd1cc-2c39-4657-a41f-5f81a2e5a31d"
   },
   "outputs": [],
   "source": [
    "#@title graph utils module <font color='lightgreen'>[run me]</font>\n",
    "import jax.random as jrnd\n",
    "from functools import partial\n",
    "import scipy.spatial as SS\n",
    "\n",
    "import numpy as onp\n",
    "\n",
    "from functools import partial\n",
    "import jax\n",
    "import jax.random as jrnd\n",
    "import jax.numpy as jnp\n",
    "import jraph\n",
    "from scipy.sparse import csgraph\n",
    "import numpy as np\n",
    "import os, sys\n",
    "from struct import unpack\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import cloudpickle as pickle\n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "        \n",
    "def load_obj(name):\n",
    "    with open(name, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "#####################\n",
    "\n",
    "\n",
    "def get_distances(X):\n",
    "    nx = X.shape[0]\n",
    "    return (X[:, None, :] - X[None, :, :])[jnp.tril_indices(nx, k=-1)]\n",
    "\n",
    "\n",
    "def get_receivers_senders(nx, dists, connect_radius=0.15):\n",
    "    '''connect nodes within `connect_radius` units'''\n",
    "    \n",
    "    senders,receivers = jnp.tril_indices(nx, k=-1)\n",
    "    dists = dists[jnp.tril_indices(nx, k=-1)]\n",
    "    mask = dists < connect_radius\n",
    "    # pad dummy s,r with n_node\n",
    "    senders = jnp.where(mask > 0, senders, nx)\n",
    "    receivers = jnp.where(mask > 0, receivers, nx)\n",
    "    dists = jnp.where(mask > 0, dists, 0.)\n",
    "    return senders, receivers, dists\n",
    "\n",
    "def l2norm_einsum(X, eps=1e-9):\n",
    "    \"\"\"calculaute eucl distance with einsum\"\"\"\n",
    "    a_min_b = X[:, None, :] - X[None, :, :]\n",
    "    norm_sq = jnp.einsum(\"ijk,ijk->ij\", a_min_b, a_min_b)\n",
    "    return jnp.where(norm_sq < eps, 0, jnp.sqrt(norm_sq))\n",
    "\n",
    "def get_r2(X):\n",
    "    \"\"\"calculate euclidean distance from positional information\"\"\"\n",
    "    nx = X.shape[0]\n",
    "    #alldists = l2norm(X[:, None, :] - X[None, :, :])\n",
    "    alldists = l2norm_einsum(X)\n",
    "    return alldists #[jnp.tril_indices(nx, k=-1)]\n",
    "\n",
    "\n",
    "##### GET EDGES #####\n",
    "def edge_builder(pos, r_connect, n_node=None, invert_edges=True,\n",
    "                     boxsize=1.0001, leafsize=16):\n",
    "    \n",
    "    if n_node is not None:\n",
    "        pos = pos[:n_node]\n",
    "    \n",
    "    else:\n",
    "      n_node = pos.shape[0]\n",
    "\n",
    "    r_connect = r_connect #simulator_args[\"connect_radius\"] / simulator_args[\"L\"]\n",
    "\n",
    "    # mask out halos with distances < connect_radius\n",
    "    dists = get_r2(pos)\n",
    "    \n",
    "    _receivers, _senders, dists = get_receivers_senders(n_node, \n",
    "                                                        dists, \n",
    "                                                        connect_radius=r_connect)\n",
    "\n",
    "    diff = pos[_senders] - pos[_receivers]\n",
    "\n",
    "    num_pairs = dists.shape[0]\n",
    "    row = _senders\n",
    "    col = _receivers\n",
    "    \n",
    "    # Distance\n",
    "    dist = dists\n",
    "    \n",
    "    # Centroid of galaxy catalogue\n",
    "    centroid = jnp.mean(pos,axis=0)\n",
    "    \n",
    "    # Unit vectors of node, neighbor and difference vector\n",
    "    unitrow = (pos[row]-centroid)/jnp.linalg.norm((pos[row]-centroid), axis=1).reshape(-1,1)\n",
    "    unitcol = (pos[col]-centroid)/jnp.linalg.norm((pos[col]-centroid), axis=1).reshape(-1,1)\n",
    "    \n",
    "    unitdiff = jnp.where((dist.reshape(-1,1) > 0.), diff/dist.reshape(-1,1), 1.)\n",
    "    \n",
    "    # Dot products between unit vectors\n",
    "    cos1 = jnp.einsum('ij,ij->i', unitrow, unitcol)\n",
    "    cos2 = jnp.einsum('ij,ij->i', unitrow, unitdiff)\n",
    "\n",
    "    # mask out nans\n",
    "    cos1 = jnp.where(dist == 0., 0., cos1)\n",
    "    cos2 = jnp.where(dist == 0., 0., cos2)\n",
    "    \n",
    "    if invert_edges:\n",
    "        # flip the distance\n",
    "        dist = jnp.where((dist > 0.), 1. / (dist*r_connect*100.), dist)\n",
    "        # sort edges from biggest to smallest\n",
    "        idx = jnp.argsort(dist)[::-1]\n",
    "        dist = jnp.sort(dist)[::-1]\n",
    "    \n",
    "    else:\n",
    "        # Normalize distance by linking radius\n",
    "        dist /= r_connect\n",
    "        # pad with large dummy edge\n",
    "        mask = (dist > 0.)\n",
    "        fillval = 100.\n",
    "        dist = jnp.where(mask < 1, fillval, dist)\n",
    "        \n",
    "        # sort edges from SMALLEST to BIGGEST\n",
    "        idx = jnp.argsort(dist) #[::-1]\n",
    "        dist = jnp.sort(dist) #[::-1]\n",
    "        \n",
    "        # replace all dummy distances with zeros again\n",
    "        dist = jnp.where(dist == fillval, 0., dist)\n",
    "\n",
    "\n",
    "\n",
    "    cos1 = cos1[idx]\n",
    "    cos2 = cos2[idx]\n",
    "\n",
    "    _senders = _senders[idx]\n",
    "    _receivers = _receivers[idx]\n",
    "\n",
    "    edge_attr = jnp.concatenate([dist.reshape(-1,1), cos1.reshape(-1,1), cos2.reshape(-1,1)], axis=1)\n",
    "    \n",
    "    return edge_attr, jnp.array(_senders), jnp.array(_receivers)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### GET GRAPH PADDING #####\n",
    "\n",
    "def padded_graph_builder(catalog, Î¸, simulator_args, r_connect=None, n_node=None):\n",
    "      \n",
    "    pad_nodes_to = simulator_args['pad_nodes_to']\n",
    "    pad_edges_to = simulator_args['pad_edges_to']\n",
    "    include_pos = simulator_args[\"include_pos\"]\n",
    "    node_features = simulator_args[\"node_features\"]\n",
    "\n",
    "    # get empty arrays\n",
    "    if include_pos:\n",
    "        nodes = jnp.zeros((pad_nodes_to, node_features))\n",
    "        edges = jnp.zeros((pad_edges_to, 1)) # dist\n",
    "\n",
    "    else:\n",
    "        nodes = jnp.zeros((pad_nodes_to, 4)) # density, neighborhood info\n",
    "        edges = jnp.zeros((pad_edges_to, 3)) # dist, angle, angle\n",
    "    \n",
    "    senders = (jnp.ones((pad_edges_to), dtype=int)*pad_nodes_to).astype(int)\n",
    "    receivers = (jnp.ones((pad_edges_to), dtype=int)*pad_nodes_to).astype(int)\n",
    "    \n",
    "    \n",
    "    # dealing with the padded catalog\n",
    "    # set the dummy nodes' positions to way outside the box\n",
    "    catalog = jnp.where((catalog[:, 0] == 0.)[:, jnp.newaxis],\n",
    "                        jnp.array([0., 0., 0., 0., 100., 100., 100.]),\n",
    "                        catalog)\n",
    "    \n",
    "    # unpack node attributes from catalog\n",
    "    pos = catalog[:, 4:]\n",
    "    \n",
    "    # reset dummy positions to zero after passing to pos\n",
    "    catalog = jnp.where((catalog[:, 0] > 0.)[:, jnp.newaxis], catalog, 0.)\n",
    "    \n",
    "    if include_pos:\n",
    "        node_attr = catalog[:, :]\n",
    "\n",
    "    else:\n",
    "        node_attr = catalog[:, :4]\n",
    "      \n",
    "    n_node = jnp.sum(node_attr[:, 0] > 0.)\n",
    "    \n",
    "    # GET EDGE INFORMATION\n",
    "    edge_attr, _s, _r = edge_builder(jnp.array(pos), r_connect=r_connect,\n",
    "                                     invert_edges=simulator_args[\"invert_edges\"],\n",
    "                                     boxsize=1.0001)\n",
    "    \n",
    "    if include_pos:\n",
    "        edge_attr = edge_attr[:, :1]\n",
    "    \n",
    "    n_edge = jnp.sum(edge_attr[:, 0] > 0.)\n",
    "    \n",
    "    # fill in jax arrays\n",
    "    \n",
    "    # edge information\n",
    "    edges = edges.at[:pad_edges_to, :].set(edge_attr[:pad_edges_to, :])\n",
    "    senders = senders.at[:pad_edges_to].set(_s[:pad_edges_to])\n",
    "    receivers = receivers.at[:pad_edges_to].set(_r[:pad_edges_to])\n",
    "\n",
    "    # add in node information\n",
    "    nodes = nodes.at[:pad_nodes_to, :].set(node_attr[:pad_nodes_to])\n",
    "\n",
    "    if simulator_args[\"squeeze\"]:\n",
    "\n",
    "        graph = jraph.GraphsTuple(nodes=jnp.squeeze(nodes), edges=jnp.squeeze(edges),\n",
    "                              senders=jnp.squeeze(senders), receivers=jnp.squeeze(receivers),\n",
    "                              n_node=jnp.array([n_node]), n_edge=jnp.array([n_edge]), globals=jnp.array([Î¸])\n",
    "                              )\n",
    "        return graph\n",
    "\n",
    "    else:\n",
    "        graph = jraph.GraphsTuple(nodes=nodes, edges=edges,\n",
    "                                senders=senders, receivers=receivers,\n",
    "                                n_node=jnp.array([n_node]), n_edge=jnp.array([n_edge]), globals=jnp.array([Î¸])\n",
    "                                )\n",
    "        return graph\n",
    "\n",
    "\n",
    "####################### VISUALIZE GRAPH #######################\n",
    "\n",
    "def plot_graph(graph,\n",
    "                ax=None,\n",
    "                nodesize=45,\n",
    "                nodealpha=0.5,\n",
    "                edgewidth=1.5,\n",
    "                edgealpha=0.5,\n",
    "                labelaxes=False,\n",
    "                removeticks=False,\n",
    "                removebox=False):\n",
    "    \"\"\"visualize jraph graph using Networkx\"\"\"\n",
    "    \n",
    "    send_receive = [(int(graph.senders[l]), int(graph.receivers[l])) for l in range(len(graph.receivers))]\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(list(np.arange(graph.nodes[:, :1].shape[0])))\n",
    "    G.add_edges_from(send_receive)\n",
    "\n",
    "\n",
    "    # 3d spring layout\n",
    "    pos = graph.nodes[:, 1:4] #X \n",
    "    \n",
    "    masses = graph.nodes[:, 0]\n",
    "    \n",
    "    \n",
    "    # Extract node and edge positions from the layout\n",
    "    node_xyz = np.array([pos[v] for v in sorted(G)])\n",
    "    edge_xyz = np.array([(pos[u], pos[v]) for u, v in G.edges()])\n",
    "    \n",
    "    if ax is None:\n",
    "            # Create the 3D figure\n",
    "        fig = plt.figure(figsize=(7,4))\n",
    "        ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "    # Plot the nodes - alpha is scaled by \"depth\" automatically\n",
    "    sc = ax.scatter(*node_xyz.T, s=nodesize, ec='w', \n",
    "                    c=masses, cmap='gist_gray', alpha=nodealpha)\n",
    "\n",
    "\n",
    "\n",
    "    # Plot the edges\n",
    "    for vizedge in edge_xyz:\n",
    "        ax.plot(*vizedge.T, color=\"tab:gray\", lw=edgewidth, alpha=edgealpha)\n",
    "\n",
    "\n",
    "    def _format_axes(ax):\n",
    "        \"\"\"Visualization options for the 3D axes.\"\"\"\n",
    "        # Turn gridlines off\n",
    "        ax.grid(False)\n",
    "        # Suppress tick labels\n",
    "        if removeticks:\n",
    "            for dim in (ax.xaxis, ax.yaxis, ax.zaxis):\n",
    "                dim.set_ticks([])\n",
    "                \n",
    "        if removebox:\n",
    "            for key, spine in ax.spines.items():\n",
    "                spine.set_visible(False)\n",
    "            ax.axis('off')\n",
    "        if labelaxes:\n",
    "            # Set axes labels\n",
    "            ax.set_xlabel(r\"$x\\ \\rm [Mpc/h]$\", fontsize=15)\n",
    "            ax.set_ylabel(r\"$y$\", fontsize=15)\n",
    "            ax.set_zlabel(r\"$z$\", fontsize=15)\n",
    "\n",
    "\n",
    "    ax.xaxis.pane.fill = False\n",
    "    ax.yaxis.pane.fill = False\n",
    "    ax.zaxis.pane.fill = False\n",
    "    ax.xaxis.pane.set_edgecolor('black')\n",
    "    ax.yaxis.pane.set_edgecolor('black')\n",
    "    ax.zaxis.pane.set_edgecolor('black')\n",
    "\n",
    "\n",
    "    ax.view_init(azim=15, elev=20)\n",
    "    _format_axes(ax)  \n",
    "    #plt.show()\n",
    "    \n",
    "    return ax\n",
    "\n",
    "def load_single_unpadded_graph(fname, idx, masscut, r_connect):\n",
    "  _graph = load_obj(fname)\n",
    "  catalog = _graph.nodes[idx] # take first simulation\n",
    "  catalog = catalog[catalog[:, 0] > masscut] # remove padding, with masscut\n",
    "\n",
    "  pos = catalog[:, 4:]\n",
    "\n",
    "  catalog = jnp.concatenate([catalog[:, :1], catalog[:, 4:]], axis=-1)\n",
    "\n",
    "  edge_attr, _s, _r = edge_builder(jnp.array(pos), r_connect=r_connect,\n",
    "                                  invert_edges=simulator_args[\"invert_edges\"],\n",
    "                                  boxsize=1.0001)\n",
    "  \n",
    "  edgemask = edge_attr[:, 0] > 0\n",
    "  edge_attr = edge_attr[edgemask, 0]\n",
    "  _s = _s[edgemask]\n",
    "  _r = _r[edgemask]\n",
    "\n",
    "  return jraph.GraphsTuple(nodes=catalog,\n",
    "                           edges=edge_attr,\n",
    "                           senders=_s,\n",
    "                           receivers=_r,\n",
    "                           n_node=_graph.n_node[idx],\n",
    "                           n_edge=_graph.n_edge[idx],\n",
    "                           globals=_graph.globals[idx])\n",
    "\n",
    "# default simulator args\n",
    "N=512\n",
    "shape=(N,N,N)\n",
    "simulator_args = dict(\n",
    "        connect_radius=0.2,\n",
    "        mass_cut=1.5,             # 10^15 Msun\n",
    "        do_noise=False,\n",
    "        nbar=0.0015,    # GIVES US A LOT MORE NODES\n",
    "        pad_nodes_to=200,\n",
    "        pad_edges_to= 500, # 1150, 500, 100 for 0.3, 0.2, 0.1\n",
    "        include_pos=False,\n",
    "        invert_edges=False,\n",
    "        squeeze=False,\n",
    "        boxsize=None, # or 1.0001\n",
    "        return_cat=False,\n",
    "        node_features=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nla78gfeHZjN",
   "metadata": {
    "id": "nla78gfeHZjN"
   },
   "source": [
    "## visualize a dark matter graph\n",
    "\n",
    "Here we're going to construct a single, unpadded graph from a halo catalogue. We'll plot both a catalogue with halos of mass $M_i>M_{\\rm cut}=1.1\\times10^{15} M_{\\odot}$, as well as those with masses $M_i > M_{\\rm cut}=1.5\\times10^{15} M_{\\odot} $. We'll connect every halo to every neighboring halo within a radius of $r_{\\rm connect}=200\\ \\textrm{Mpc} = 0.2\\ \\textrm{Gpc} $.\n",
    "\n",
    "These two physical parameters change the *cardinality* of the graph, or the length of the set of edges and nodes. The helper function `load_single_unpadded_graph()` takes in a set of nodes (here loaded as a `.pkl` and computes edges between halo positions. The output graph has features\n",
    "$$ \\begin{align}\n",
    "\\textbf{e}_{ij} &= d_{ij}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\   \\texttt{# the distance between halos i and j}  \\\\\n",
    "  \\textbf{v}_i &= [M_i, \\textbf{p}_i],\\ \\ \\ \\ \\ \\  \\texttt{# mass, (x,y,z) position} \n",
    "  \\end{align}\n",
    "$$\n",
    "In the `jraph` representation, we flatten the $i,j$ edge index into a $k$ index, with senders and receivers indexes denoted $s_k$ and $r_k$, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3051dc41-dcca-4941-a2f3-89ee811b00a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = '/data80/makinen/quijote/graphs/png/z_1/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8FRzpL1q2unE",
   "metadata": {
    "id": "8FRzpL1q2unE"
   },
   "source": [
    "# Forward-simulating catalogue cuts\n",
    "\n",
    "Furthermore, since we're learning from simulations, we're free to add all sorts of noise and effects to the data during network training. In this tutorial we'll add *new white noise* to the halo masses every training epoch to mimic more realistic data collection from galaxy clusters.\n",
    "\n",
    "Our noise model will look like:\n",
    "$$ \\hat{m}_i = m_i + \\mathcal{N}(0,\\sigma^2_{\\rm noise}) $$\n",
    "with $\\sigma_{\\rm noise} = A_{\\rm noise}M_{\\rm cut}$ and catalogue cuts performed on imperfectly-known $\\hat{m}_i$. Feel free to vary the noise amplitude below !\n",
    " \n",
    "Let's use an IMNN trained on cosmic graphs to see how much information we can extract and what sort of constraints we can get. We will use 200 simulations to estimate the covariance and use all of their derivatives and we'll summarise the whole cosmological structure using 2 summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0c926c4-fd96-431b-a6ed-942aa1158bcc",
   "metadata": {
    "id": "e0c926c4-fd96-431b-a6ed-942aa1158bcc"
   },
   "outputs": [],
   "source": [
    "# IMNN-specific parameters for dataset in (train, validation):\n",
    "\n",
    "n_s = 500  # number of sims to compute covariance\n",
    "n_d = 250  # number of sims to calculate finite differences\n",
    "\n",
    "# number of params and summaries to compress to\n",
    "n_params = 2\n",
    "n_summaries = n_params\n",
    "\n",
    "# fiducial (Quijote) parameters and finite-differences\n",
    "Î¸_fid = jnp.array([0.0, 0.0])\n",
    "Î´Î¸ = 2*jnp.array([100, 100])\n",
    "\n",
    "Î¸_der = (Î¸_fid + jnp.einsum(\"i,jk->ijk\", jnp.array([-1., 1.]), jnp.diag(Î´Î¸) / 2.)).reshape((-1, n_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67XGyHGGaHG",
   "metadata": {
    "id": "b67XGyHGGaHG"
   },
   "source": [
    "This functionality also works over graphs ! (provided we batch and pad nodes and edge features carefully. We've done this for you in the `utils` code block, but highlight the last two functions here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea28146-660a-441f-bb05-8fe628bc7426",
   "metadata": {
    "id": "bea28146-660a-441f-bb05-8fe628bc7426"
   },
   "source": [
    "# graph assembly from Quijote data using vmap\n",
    "\n",
    "ORDER OF FEATURES: mass, velx3, posx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee30c95a-892f-4ac1-ab89-83c9bfae19e8",
   "metadata": {
    "id": "ee30c95a-892f-4ac1-ab89-83c9bfae19e8"
   },
   "outputs": [],
   "source": [
    "# a random key\n",
    "key = jax.random.PRNGKey(44)\n",
    "\n",
    "# some default simulator_args to define our setup\n",
    "simulator_args = dict(\n",
    "          connect_radius=0.2,\n",
    "          mass_cut=1.85,             # 10^15 Msun\n",
    "          do_noise=False,\n",
    "          pad_nodes_to=200,\n",
    "          pad_edges_to=500, # 1150, 500, 100 for 0.3, 0.2, 0.1\n",
    "          include_pos=False,\n",
    "          invert_edges=False,\n",
    "          squeeze=False,\n",
    "          boxsize=None,\n",
    "          return_cat=False,\n",
    "          node_features=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b071aabf-eb13-4089-a175-ed776c61c874",
   "metadata": {
    "cellView": "form",
    "id": "b071aabf-eb13-4089-a175-ed776c61c874"
   },
   "outputs": [],
   "source": [
    "#@title  functions for building padded graphs <font color='lightgreen'>[run me]</font>\n",
    "def padded_catalog(catalog, simulator_args=simulator_args):\n",
    "    pad_nodes_to = simulator_args['pad_nodes_to']\n",
    "    \n",
    "    nodes = jnp.zeros((pad_nodes_to, 7))\n",
    "    nodes = nodes.at[:catalog.shape[0], :].set(catalog[:pad_nodes_to])\n",
    "\n",
    "    return nodes, catalog.shape[0]\n",
    "\n",
    "# build graph \"simulator\"\n",
    "def graph_simulator(key, Î¸, catalog=None, \n",
    "                    simulator_args=simulator_args, \n",
    "                    r_connect=0.15,\n",
    "                    num_halos=None):\n",
    "\n",
    "    # do mass cut\n",
    "    if simulator_args[\"do_noise\"]:\n",
    "        noise = jax.random.normal(key, \n",
    "                    shape=catalog[:, 0].shape)*simulator_args['noise_scale']*simulator_args[\"mass_cut\"]\n",
    "        \n",
    "        catalog = catalog.at[:, 0].set(catalog[:, 0] + noise)\n",
    "    \n",
    "    # if we want to truncate to a FIXED-length catalog (bad idea !)\n",
    "    if num_halos is not None:\n",
    "        masses = catalog[:, 0]\n",
    "        inds = jnp.argsort(masses)[::-1] # sort largest to smallest\n",
    "        inds = inds[:num_halos]          # take first few indices\n",
    "        catalog = catalog[inds]\n",
    "    \n",
    "    else:\n",
    "        mass_cut = simulator_args[\"mass_cut\"]\n",
    "        mask = (catalog[:, 0] < mass_cut)\n",
    "        catalog = jnp.where(mask[:, jnp.newaxis],\n",
    "                                0.,\n",
    "                                catalog)\n",
    "    \n",
    "    catalog,n_node = padded_catalog(jnp.squeeze(catalog), simulator_args=simulator_args)\n",
    "    \n",
    "    graph = padded_graph_builder(catalog, Î¸, simulator_args,\\\n",
    "                                                  r_connect=r_connect)\n",
    "    \n",
    "    return graph\n",
    "\n",
    "def getgraphs(key, graphs, r_connect=0.15, \n",
    "              num_halos=None, \n",
    "              simulator_args=simulator_args, verbose=True):\n",
    "    \n",
    "    num = graphs.nodes.shape[0]\n",
    "    keys = jax.random.split(key, num=num)\n",
    "    Î¸s = graphs.globals\n",
    "    \n",
    "    if verbose:\n",
    "        print('assembling with r_connect = ', r_connect)\n",
    "    \n",
    "    gs = lambda k,Î¸,cat: graph_simulator(k, Î¸, cat, \n",
    "                                         simulator_args=simulator_args,\n",
    "                                         r_connect=r_connect, num_halos=num_halos)\n",
    "    \n",
    "    graphs = jax.vmap(gs)(keys, Î¸s, graphs.nodes)\n",
    "    \n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b79f423-5019-4f17-a6c3-c3271db199aa",
   "metadata": {
    "id": "2b79f423-5019-4f17-a6c3-c3271db199aa"
   },
   "source": [
    "# load padded data for IMNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8268e3fb-79d9-47e2-93dc-faf33dd4edad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "#Path(\"/my/directory\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_graph_attributes(halograph, folder, name):\n",
    "    # make folder for each graph\n",
    "    fname = folder + name\n",
    "    print(\"saving to\", fname)\n",
    "    Path(fname).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    np.save(fname + \"nodes\", halograph.nodes)\n",
    "    np.save(fname + \"globals\", halograph.globals)\n",
    "    np.save(fname + \"n_node\", halograph.n_node)\n",
    "    \n",
    "def load_graph_attributes(folder, name):\n",
    "    \n",
    "    # make folder for each graph\n",
    "    fname = folder + name\n",
    "    print(\"loading from\", fname)\n",
    "    Path(fname).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    nodes = np.load(fname + \"nodes.npy\")\n",
    "    globalz = np.load(fname + \"globals.npy\")\n",
    "    n_node = np.load(fname + \"n_node.npy\")\n",
    "    \n",
    "    return jraph.GraphsTuple(\n",
    "                        nodes=jnp.array(nodes),\n",
    "                        n_node=jnp.array(n_node),\n",
    "                        edges=None,\n",
    "                        n_edge=jnp.array([0.]),\n",
    "                        senders=None,\n",
    "                        receivers=None,\n",
    "                        globals=jnp.array(globalz)\n",
    "                            )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e3e7d6d2-477d-4a36-9202-557a368b1e47",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3e7d6d2-477d-4a36-9202-557a368b1e47",
    "outputId": "8113f81c-fd69-42f5-b2c6-4bebb8df431b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from /data80/makinen/quijote/graphs/png/z_1/lc_eq_fid/\n",
      "loading from /data80/makinen/quijote/graphs/png/z_1/lc_eq_val_fid/\n",
      "loading from /data80/makinen/quijote/graphs/png/z_1/lc_eq_derv/\n",
      "loading from /data80/makinen/quijote/graphs/png/z_1/lc_eq_val_derv/\n",
      "assembling with r_connect =  0.2\n",
      "assembling with r_connect =  0.2\n",
      "assembling with r_connect =  0.2\n",
      "assembling with r_connect =  0.2\n"
     ]
    }
   ],
   "source": [
    "#@title create noise-free sims at lower mass cut <font color='lightgreen'>[run me]</font>\n",
    "#-----------------------------------------------------------------------------------\n",
    "####### SETUP ARGUMENTS FOR NOISE-FREE SIMS [DO NOT CHANGE] #######\n",
    "np = jnp\n",
    "graphdir = '/data80/makinen/quijote/graphs/png/z_1/'\n",
    "\n",
    "# load derivative datasets with new functions\n",
    "\n",
    "fid = load_graph_attributes(folder=graphdir, name='lc_eq_fid/')\n",
    "val_fid = load_graph_attributes(folder=graphdir, name='lc_eq_val_fid/')\n",
    "\n",
    "derv = load_graph_attributes(folder=graphdir, name='lc_eq_derv/')\n",
    "val_derv = load_graph_attributes(folder=graphdir, name='lc_eq_val_derv/')\n",
    "\n",
    "\n",
    "# then construct edges\n",
    "r_connect = 0.2     # connection radius\n",
    "N_int = 2           # number of GNN interaction blocks\n",
    "\n",
    "num_halos = None    # for variable length inputs\n",
    "include_mass = True # include mass labels on nodes\n",
    "\n",
    "# SETUP ARGS: DO NOT CHANGE\n",
    "# include_pos in nodes or not. Set to True here to obtain catalogues with positional information.\n",
    "simulator_args[\"include_pos\"] = True\n",
    "simulator_args[\"pad_edges_to\"] = 200 #500\n",
    "simulator_args[\"connect_radius\"] = r_connect\n",
    "simulator_args[\"do_noise\"] = False\n",
    "simulator_args[\"pad_nodes_to\"] = 500 #500 #200 #500\n",
    "\n",
    "# CREATE NOISE-FREE SIMS TO ADD NOISE TO ON-THE-FLY\n",
    "# include mass cut below what we will consider in the actual catalog\n",
    "simulator_args['mass_cut'] = 1.35\n",
    "\n",
    "noisefree_fid = getgraphs(key, fid, r_connect, num_halos, simulator_args)\n",
    "noisefree_val_fid = getgraphs(key, val_fid, r_connect, num_halos, simulator_args)\n",
    "\n",
    "noisefree_derv = getgraphs(key, derv, r_connect, num_halos, simulator_args)\n",
    "noisefree_val_derv = getgraphs(key, val_derv, r_connect, num_halos, simulator_args)\n",
    "\n",
    "#-----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "37125911-4f7f-402b-9316-9af8eda16e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 2000, 7)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fid.nodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5763418e-1446-419a-838c-df31f49366a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([866., 689., 348.,  61.,  23.,  10.,   0.,   2.,   0.,   1.]),\n",
       " array([0.        , 0.68459946, 1.3691989 , 2.0537984 , 2.7383978 ,\n",
       "        3.4229972 , 4.107597  , 4.7921963 , 5.4767957 , 6.161395  ,\n",
       "        6.8459945 ], dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(onp.array(onp.squeeze(fid.nodes[0, :, 0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9a20e094-d04f-47d6-a72e-5a54aba57a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  2.,   8.,  12.,  49.,  75., 112., 100.,  79.,  45.,  18.]),\n",
       " array([382. , 393.8, 405.6, 417.4, 429.2, 441. , 452.8, 464.6, 476.4,\n",
       "        488.2, 500. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(onp.squeeze(onp.array(noisefree_derv.n_node)))\n",
    "plt.hist(onp.squeeze(onp.array(noisefree_val_derv.n_node)))\n",
    "plt.hist(onp.squeeze(onp.array(noisefree_fid.n_node)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YLaQt1ze3Map",
   "metadata": {
    "id": "YLaQt1ze3Map"
   },
   "source": [
    "setup arguments for noisy sims ! feel free to change below if re-training an IMNN (Colab Pro only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "q2_pHyGck3mF",
   "metadata": {
    "id": "q2_pHyGck3mF"
   },
   "outputs": [],
   "source": [
    "####### SETUP ARGUMENTS FOR NOISY SIMS [feel free to change !] #######\n",
    "\n",
    "# make changes below ! \n",
    "# ARGS FOR ADDING NOISE ON TOP OF SIMS\n",
    "# whether or not to do noise\n",
    "# switch to true for sake of simulation\n",
    "simulator_args[\"include_pos\"] = False\n",
    "simulator_args[\"do_noise\"] = True\n",
    "simulator_args[\"noise_scale\"] = 0.0  # noise amplitude, A_noise. set to 0 for noise-free\n",
    "\n",
    "\n",
    "# set mass cut again to 1.5\n",
    "simulator_args['mass_cut'] = 2.0\n",
    "simulator_args[\"pad_nodes_to\"] = 200\n",
    "simulator_args[\"pad_edges_to\"] = 450\n",
    "\n",
    "\n",
    "# then define edge features\n",
    "r_connect = 0.2     # connection radius\n",
    "N_int = 2           # number of GNN interaction blocks\n",
    "\n",
    "num_halos = None    # for variable length inputs\n",
    "include_mass = True # include mass labels on nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8rc-nFTEbcpA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8rc-nFTEbcpA",
    "outputId": "71a1e3d6-349f-49ff-d7ee-47810edaa6d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max num edges 3744\n",
      "max num nodes 500\n"
     ]
    }
   ],
   "source": [
    "_maxedge = onp.max([np.max(noisefree_fid.n_edge), np.max(noisefree_val_fid.n_edge), np.max(noisefree_derv.n_edge), np.max(noisefree_val_derv.n_edge)])\n",
    "_maxnode = onp.max([np.max(noisefree_fid.n_node), np.max(noisefree_val_fid.n_node), np.max(noisefree_derv.n_node), np.max(noisefree_val_derv.n_node)])\n",
    "\n",
    "print('max num edges', _maxedge)\n",
    "print('max num nodes', _maxnode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EyIkITgRJPXU",
   "metadata": {
    "id": "EyIkITgRJPXU"
   },
   "source": [
    "# adding noise: on-the-fly simulator\n",
    "Here we'll define two functions to add noise to our graphs on-the-fly. Every epoch our network trains we'll add a new noise realization **and catalogue cuts performed on the noisy data**. This will yield differently-lengthed catalogues every time (although the padding will remain the same).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e0e159e1-8b1e-4647-b83b-419dbfaf3542",
   "metadata": {
    "cellView": "form",
    "id": "e0e159e1-8b1e-4647-b83b-419dbfaf3542"
   },
   "outputs": [],
   "source": [
    "#@title noise simulator code <font color='lightgreen'>[run me]</font>\n",
    "def seedmatch_getgraph_single(key, graph, r_connect=0.15, \n",
    "              num_halos=None, \n",
    "              simulator_args=simulator_args):\n",
    "    num = graph.nodes.shape[0]\n",
    "    Î¸s = graph.globals\n",
    "    gs = lambda k,Î¸,cat: graph_simulator(k, Î¸, cat, \n",
    "                                         simulator_args=simulator_args,\n",
    "                                         r_connect=r_connect, num_halos=num_halos)\n",
    "    graph = gs(key, Î¸s, graph.nodes)\n",
    "    \n",
    "    return graph\n",
    "\n",
    "def noise_simulator(key, graph, simulator_args=simulator_args):\n",
    "    \n",
    "    return seedmatch_getgraph_single(key, graph, simulator_args[\"connect_radius\"], None, simulator_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "edb4d08a-c135-4cc6-aedc-1eca0b79ba8f",
   "metadata": {
    "id": "edb4d08a-c135-4cc6-aedc-1eca0b79ba8f"
   },
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d3ccba75-7e00-4369-a3de-af470df2f4e3",
   "metadata": {
    "id": "d3ccba75-7e00-4369-a3de-af470df2f4e3"
   },
   "outputs": [],
   "source": [
    "np = jnp\n",
    "keys = jax.random.split(key, num=n_s)\n",
    "\n",
    "# vmap over noise-free fiducial simulation\n",
    "noisyfid = jax.vmap(noise_simulator)(keys, noisefree_fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4b6fc6df-b58f-41ff-9509-01bf7a438e04",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "4b6fc6df-b58f-41ff-9509-01bf7a438e04",
    "outputId": "e888ce87-0934-4676-b9db-8f677067cce7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fed36236d10>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the two mass cuts\n",
    "ind = 67 # sim number 67\n",
    "plt.hist(onp.array(noisefree_fid.nodes[ind, :, 0]), label='noise-free catalogue')\n",
    "plt.hist(onp.array(noisyfid.nodes[ind, :, 0]), label='noisy, with cuts')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Plrm4jstLBQh",
   "metadata": {
    "id": "Plrm4jstLBQh"
   },
   "source": [
    "Here the zeros are just padded, dummy nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h8NxCrL6HRLN",
   "metadata": {
    "id": "h8NxCrL6HRLN"
   },
   "source": [
    "# Graph Neural Networks\n",
    "Here we define our graph neural network architecture. See the paper if you're curious about the architecture ! The key takeaway is: we use neural networks to extract information from padded node $V$ and edge $E$ sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a529cbf8-ee31-4a46-ad6d-34eed6196769",
   "metadata": {
    "id": "a529cbf8-ee31-4a46-ad6d-34eed6196769",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title graph neural network code in `flax` <font color='lightgreen'>[run me]</font>\n",
    "from flax import linen as nn\n",
    "from typing import Any, Callable, Sequence, Optional\n",
    "\n",
    "import optax\n",
    "\n",
    "# custom scaling function for large NN inputs\n",
    "class AsinhLayer(nn.Module):\n",
    "    bias_init: Callable = nn.initializers.zeros\n",
    "    a_init: Callable = nn.initializers.ones\n",
    "    b_init: Callable = nn.initializers.ones\n",
    "    c_init: Callable = nn.initializers.zeros\n",
    "    d_init: Callable = nn.initializers.zeros\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, inputs):\n",
    "\n",
    "        a = self.param('a', self.a_init, (1,)) #*(1./36)\n",
    "        b = self.param('b', self.b_init, (1,))\n",
    "        c = self.param('c', self.c_init, (1,))\n",
    "        d = self.param('d', self.d_init, (1,)) \n",
    "\n",
    "        y = a*jnp.arcsinh(b*inputs + c) + d\n",
    "        return y\n",
    "\n",
    "# define activation\n",
    "act = nn.gelu\n",
    "\n",
    "# fully-connected network\n",
    "class ExplicitMLP(nn.Module):\n",
    "  \"\"\"A flax MLP.\"\"\"\n",
    "  features: Sequence[int]\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, inputs):\n",
    "    x = inputs\n",
    "    for i, lyr in enumerate([nn.Dense(feat) for feat in self.features]):\n",
    "      x = lyr(x)\n",
    "      if i != len(self.features) - 1:\n",
    "        x = act(x)\n",
    "    return x\n",
    "\n",
    "# adapt for jraph library -- need wrappers for linen Modules\n",
    "def make_embed_fn(latent_size):\n",
    "  def embed(inputs):\n",
    "    inputs = AsinhLayer()(inputs)\n",
    "    return nn.Dense(latent_size)(inputs)\n",
    "  return embed\n",
    "\n",
    "\n",
    "def make_mlp(features):\n",
    "  @jraph.concatenated_args\n",
    "  def update_fn(inputs):\n",
    "    return ExplicitMLP(features)(inputs)\n",
    "  return update_fn\n",
    "\n",
    "\n",
    "# custom mean function for padded inputs\n",
    "def custom_segment_mean(\n",
    "                 n_data: int,\n",
    "                 data: jnp.ndarray,\n",
    "                 segment_ids: jnp.ndarray,\n",
    "                 num_segments: Optional[int] = None,\n",
    "                 indices_are_sorted: bool = False,\n",
    "                 unique_indices: bool = False):\n",
    "  \"\"\"Returns mean for each segment.\n",
    "  Args:\n",
    "    n_data: the number of data we want to take the mean of\n",
    "    data: the values which are averaged segment-wise.\n",
    "    segment_ids: indices for the segments.\n",
    "    num_segments: total number of segments.\n",
    "    indices_are_sorted: whether ``segment_ids`` is known to be sorted.\n",
    "    unique_indices: whether ``segment_ids`` is known to be free of duplicates.\n",
    "  \"\"\"\n",
    "  denom = n_data\n",
    "  nominator = jraph.segment_sum(\n",
    "      data,\n",
    "      segment_ids,\n",
    "      num_segments,\n",
    "      indices_are_sorted=indices_are_sorted,\n",
    "      unique_indices=unique_indices)\n",
    "  denominator = jraph.segment_sum(\n",
    "      jnp.ones_like(data),\n",
    "      segment_ids,\n",
    "      num_segments,\n",
    "      indices_are_sorted=indices_are_sorted,\n",
    "      unique_indices=unique_indices)\n",
    "  return nominator / jnp.maximum(denom,\n",
    "                                 jnp.ones(shape=[], dtype=denominator.dtype))\n",
    "\n",
    "# custom variance function for padded inputs\n",
    "def custom_segment_variance(n_data: int,\n",
    "                     data: jnp.ndarray,\n",
    "                     segment_ids: jnp.ndarray,\n",
    "                     num_segments: Optional[int] = None,\n",
    "                     indices_are_sorted: bool = False,\n",
    "                     unique_indices: bool = False):\n",
    "  \"\"\"Returns the variance for each segment.\n",
    "  Args:\n",
    "    n_data: the number of data we want to take the variance of\n",
    "    data: values whose variance will be calculated segment-wise.\n",
    "    segment_ids: indices for segments\n",
    "    num_segments: total number of segments.\n",
    "    indices_are_sorted: whether ``segment_ids`` is known to be sorted.\n",
    "    unique_indices: whether ``segment_ids`` is known to be free of duplicates.\n",
    "  Returns:\n",
    "    num_segments size array containing the variance of each segment.\n",
    "  \"\"\"\n",
    "  means = custom_segment_mean(\n",
    "      n_data,\n",
    "      data,\n",
    "      segment_ids,\n",
    "      num_segments,\n",
    "      indices_are_sorted=indices_are_sorted,\n",
    "      unique_indices=unique_indices)[segment_ids]\n",
    "  counts = jraph.segment_sum(\n",
    "      jnp.ones_like(data),\n",
    "      segment_ids,\n",
    "      num_segments,\n",
    "      indices_are_sorted=indices_are_sorted,\n",
    "      unique_indices=unique_indices)\n",
    "  \n",
    "  counts = n_data*jnp.ones_like(counts)\n",
    "  counts = jnp.maximum(counts, jnp.ones_like(counts))\n",
    "\n",
    "  variances = jraph.segment_sum(\n",
    "      jnp.power(data - means, 2),\n",
    "      segment_ids,\n",
    "      num_segments,\n",
    "      indices_are_sorted=indices_are_sorted,\n",
    "      unique_indices=unique_indices) / counts\n",
    "  return variances\n",
    "\n",
    "# custom aggregation function\n",
    "def custom_aggregation(n_data: int,\n",
    "                     data: jnp.ndarray,\n",
    "                     segment_ids: jnp.ndarray,\n",
    "                     num_segments: Optional[int] = None,\n",
    "                     indices_are_sorted: bool = False,\n",
    "                     unique_indices: bool = False):\n",
    "    \"\"\"Returns the variance for each segment.\n",
    "      Args:\n",
    "        n_data: the number of data we want to take the variance of\n",
    "        data: values whose variance will be calculated segment-wise.\n",
    "        segment_ids: indices for segments\n",
    "        num_segments: total number of segments.\n",
    "        indices_are_sorted: whether ``segment_ids`` is known to be sorted.\n",
    "        unique_indices: whether ``segment_ids`` is known to be free of duplicates.\n",
    "      Returns:\n",
    "        (num_segments, 4) size array containing the [sum, mean, variance, max]\n",
    "        of input data.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    variance = custom_segment_variance(n_data=n_data,\n",
    "                     data=data,\n",
    "                     segment_ids=segment_ids,\n",
    "                     num_segments=num_segments,\n",
    "                     indices_are_sorted=indices_are_sorted,\n",
    "                     unique_indices=unique_indices)\n",
    "    \n",
    "    mean = custom_segment_mean(n_data=n_data,\n",
    "                     data=data,\n",
    "                     segment_ids=segment_ids,\n",
    "                     num_segments=num_segments,\n",
    "                     indices_are_sorted=indices_are_sorted,\n",
    "                     unique_indices=unique_indices)\n",
    "    \n",
    "    _max = jraph.segment_max(\n",
    "                     data=data,\n",
    "                     segment_ids=segment_ids,\n",
    "                     num_segments=num_segments,\n",
    "                     indices_are_sorted=indices_are_sorted,\n",
    "                     unique_indices=unique_indices)   \n",
    "        \n",
    "    _sum = jraph.segment_sum(\n",
    "                     data=data,\n",
    "                     segment_ids=segment_ids,\n",
    "                     num_segments=num_segments,\n",
    "                     indices_are_sorted=indices_are_sorted,\n",
    "                     unique_indices=unique_indices)\n",
    "    \n",
    "    return jnp.concatenate([_sum, mean, variance, _max], axis=-1)\n",
    "\n",
    "\n",
    "class flaxGraphNetwork(nn.Module):\n",
    "  \"\"\"A flax GraphNetwork.\"\"\"\n",
    "  mlp_features: Sequence[int]\n",
    "  latent_size: int\n",
    "  decorate_nodes: bool=False\n",
    "  remove_vel: bool=False\n",
    "  num_nets: int=1\n",
    "  remove_edges: bool=False\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, graph):\n",
    "\n",
    "    _nodes = graph.nodes\n",
    "    \n",
    "    if not self.decorate_nodes:\n",
    "        # the indicator functions are here represented by the edge\n",
    "        # sender and receiver indexes, so we are free to set all nodes\n",
    "        # to zero.\n",
    "        _nodes = _nodes.at[:, 0:4].set(0.)\n",
    "\n",
    "    # in this study we don't consider velocity\n",
    "    if self.remove_vel:\n",
    "        _nodes = _nodes.at[:, 1:4].set(0.)\n",
    "    \n",
    "    # whether or not to remove edge labels. here always False.\n",
    "    if self.remove_edges:\n",
    "        graph = graph._replace(edges=jnp.zeros(graph.edges.shape))\n",
    "    \n",
    "    # replaces graph features with desired masked elements.\n",
    "    # add N^v and N^e as global properties\n",
    "    graph = graph._replace(\n",
    "                           globals=jnp.vstack([jnp.arcsinh(graph.n_node), jnp.arcsinh(graph.n_edge)]).reshape(1,-1),\n",
    "                           senders=graph.senders.astype(int),\n",
    "                           receivers=graph.senders.astype(int),\n",
    "                           n_node = graph.n_node.reshape(-1,1),\n",
    "                           nodes=_nodes)\n",
    "\n",
    "    embedder = jraph.GraphMapFeatures(\n",
    "        embed_node_fn=make_embed_fn(self.latent_size),\n",
    "        embed_edge_fn=make_embed_fn(self.latent_size),\n",
    "        embed_global_fn=make_embed_fn(self.latent_size))\n",
    "    \n",
    "    # rho aggregation functions\n",
    "    aggregate_nodes_for_globals_fn = lambda d,s,n: jnp.arcsinh(custom_aggregation(jnp.squeeze(graph.n_node), d,s,n))\n",
    "    aggregate_edges_for_nodes_fn = jraph.segment_sum #lambda d,s,n: jnp.arcsinh(jraph.segment_sum(d,s,n))\n",
    "    aggregate_edges_for_globals_fn = lambda d,s,n: jnp.arcsinh(custom_aggregation(jnp.squeeze(graph.n_edge), d,s,n))\n",
    "    \n",
    "    update_node_fn = make_mlp(self.mlp_features)\n",
    "    update_edge_fn = make_mlp(self.mlp_features)\n",
    "\n",
    "    # first embed the graph features to a higher dimensionality\n",
    "    graph = embedder(graph)\n",
    "    \n",
    "    # then pass through N^int interaction GNN blocks\n",
    "    for i in range(self.num_nets):\n",
    "        if i == self.num_nets-1:\n",
    "            feats = self.mlp_features + (n_params,)\n",
    "        else:\n",
    "            feats = self.mlp_features\n",
    "            \n",
    "        net = jraph.GraphNetwork(\n",
    "            update_node_fn=make_mlp(self.mlp_features),\n",
    "            update_edge_fn=make_mlp(self.mlp_features),\n",
    "            aggregate_edges_for_nodes_fn=aggregate_edges_for_nodes_fn,\n",
    "            aggregate_nodes_for_globals_fn=aggregate_nodes_for_globals_fn,\n",
    "            aggregate_edges_for_globals_fn=aggregate_edges_for_globals_fn,\n",
    "            update_global_fn=make_mlp(feats))\n",
    "        \n",
    "        graph = net(graph)\n",
    "\n",
    "    # return global properties (gIMNN summaries)\n",
    "    return graph.globals.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "000cffd4-e3de-407f-96bb-7f7f000b8952",
   "metadata": {
    "id": "000cffd4-e3de-407f-96bb-7f7f000b8952"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from /data80/makinen/quijote/graphs/png/z_1/lc_eq_fid/\n"
     ]
    }
   ],
   "source": [
    "# load a dummy graph for initializing the model -- no noise needed here\n",
    "foo = load_graph_attributes(folder=graphdir, name='lc_eq_fid/')\n",
    "graph = padded_graph_builder(foo.nodes[0], Î¸_fid, simulator_args,\\\n",
    "                                  r_connect=r_connect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52VxmI9xMkRC",
   "metadata": {
    "id": "52VxmI9xMkRC"
   },
   "source": [
    "initialize the graph network with a dummy input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b251a9be-fd01-4265-992f-e06b4ce6e50a",
   "metadata": {
    "id": "b251a9be-fd01-4265-992f-e06b4ce6e50a"
   },
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(22)\n",
    "model = flaxGraphNetwork([50, 50], 50, decorate_nodes=include_mass, remove_vel=True, num_nets=N_int, remove_edges=False)\n",
    "initial_w = model.init(key, graph)\n",
    "output1 = model.apply(initial_w, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "otRRmJb-LdjZ",
   "metadata": {
    "id": "otRRmJb-LdjZ"
   },
   "outputs": [],
   "source": [
    "np = jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "79ce3103-fa63-478a-a0b8-7aa981243484",
   "metadata": {
    "cellView": "form",
    "id": "79ce3103-fa63-478a-a0b8-7aa981243484",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title imnn utils <font color='lightgreen'>[run me]</font>\n",
    "def _check_input(input, shape, name, allow_None=False):\n",
    "    \"\"\"Exception raising checks for numpy array shapes\n",
    "\n",
    "    Checks whether input is not ``None`` and if not checks that the input is a\n",
    "    jax numpy array and if not raises a warning. If the input is a jax numpy\n",
    "    array it then checks the shape is the same as the required shape.\n",
    "\n",
    "    Can also allow ``None`` to be passed if it input is not essential.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input: any\n",
    "        The input parameter to be checked\n",
    "    shape: tuple\n",
    "        The shape that the input is required to be\n",
    "    name: str\n",
    "        The name of the variable for printing explicit errors in ``Exception``\n",
    "    allow_None: bool, default=False\n",
    "        Whether a ``None`` input can be returned as None without raising error\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array\n",
    "        Returns the input if all checks pass\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If input is None\n",
    "    ValueError\n",
    "        If input shape is incorrect\n",
    "    TypeError\n",
    "        If input is not a jax array\n",
    "    \"\"\"\n",
    "    if (input is None) and (not allow_None):\n",
    "        raise ValueError(f\"`{name}` is None\")\n",
    "    elif (input is None) and allow_None:\n",
    "        return input\n",
    "    # elif not isinstance(\n",
    "    #         input, (jax.interpreters.xla.device_array, np.ndarray)):\n",
    "    #     raise TypeError(f\"`{name}` must be a jax array\")\n",
    "    else:\n",
    "        if input.shape != shape:\n",
    "            raise ValueError(f\"`{name}` should have shape {shape} but has \" +\n",
    "                             f\"{input.shape}\")\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7936b892-8e80-4521-b629-f475768f8ddb",
   "metadata": {
    "cellView": "form",
    "id": "7936b892-8e80-4521-b629-f475768f8ddb",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title modified IMNN module <font color='lightgreen'>[run me]</font>\n",
    "import math\n",
    "import jax\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from imnn.utils.utils import _check_boolean, _check_type, \\\n",
    "    _check_model, _check_model_output, _check_optimiser, _check_state, \\\n",
    "    _check_statistics_set\n",
    "from imnn.experimental import progress_bar\n",
    "\n",
    "np = jnp\n",
    "\n",
    "class _myIMNN:\n",
    "    \"\"\"Information maximising neural network parent class\n",
    "\n",
    "    This class defines the general fitting framework for information maximising\n",
    "    neural networks. It includes the generic calculations of the Fisher\n",
    "    information matrix from the outputs of a neural network as well as an XLA\n",
    "    compilable fitting routine (with and without a progress bar). This class\n",
    "    also provides a plotting routine for fitting history and a function to\n",
    "    calculate the score compression of network outputs to quasi-maximum\n",
    "    likelihood estimates of model parameter values.\n",
    "\n",
    "    The outline of the fitting procedure is that a set of :math:`i\\\\in[1, n_s]`\n",
    "    simulations and :math:`n_d` derivatives with respect to physical model\n",
    "    parameters are used to calculate network outputs and their derivatives\n",
    "    with respect to the physical model parameters, :math:`{\\\\bf x}^i` and\n",
    "    :math:`\\\\partial{{\\\\bf x}^i}/\\\\partial\\\\theta_\\\\alpha`, where\n",
    "    :math:`\\\\alpha` labels the physical parameter. The exact details of how\n",
    "    these are calculated depend on the type of available data (see list of\n",
    "    different IMNN below). With :math:`{\\\\bf x}^i` and\n",
    "    :math:`\\\\partial{{\\\\bf x}^i}/\\\\partial\\\\theta_\\\\alpha` the covariance\n",
    "\n",
    "    .. math::\n",
    "        C_{ab} = \\\\frac{1}{n_s-1}\\\\sum_{i=1}^{n_s}(x^i_a-\\\\mu^i_a)\n",
    "        (x^i_b-\\\\mu^i_b)\n",
    "\n",
    "    and the derivative of the mean of the network outputs with respect to the\n",
    "    model parameters\n",
    "\n",
    "    .. math::\n",
    "        \\\\frac{\\\\partial\\\\mu_a}{\\\\partial\\\\theta_\\\\alpha} = \\\\frac{1}{n_d}\n",
    "        \\\\sum_{i=1}^{n_d}\\\\frac{\\\\partial{x^i_a}}{\\\\partial\\\\theta_\\\\alpha}\n",
    "\n",
    "    can be calculated and used form the Fisher information matrix\n",
    "\n",
    "    .. math::\n",
    "        F_{\\\\alpha\\\\beta} = \\\\frac{\\\\partial\\\\mu_a}{\\\\partial\\\\theta_\\\\alpha}\n",
    "        C^{-1}_{ab}\\\\frac{\\\\partial\\\\mu_b}{\\\\partial\\\\theta_\\\\beta}.\n",
    "\n",
    "    The loss function is then defined as\n",
    "\n",
    "    .. math::\n",
    "        \\\\Lambda = -\\\\log|{\\\\bf F}| + r(\\\\Lambda_2) \\\\Lambda_2\n",
    "\n",
    "    Since any linear rescaling of a sufficient statistic is also a sufficient\n",
    "    statistic the negative logarithm of the determinant of the Fisher\n",
    "    information matrix needs to be regularised to fix the scale of the network\n",
    "    outputs. We choose to fix this scale by constraining the covariance of\n",
    "    network outputs as\n",
    "\n",
    "    .. math::\n",
    "        \\\\Lambda_2 = ||{\\\\bf C}-{\\\\bf I}|| + ||{\\\\bf C}^{-1}-{\\\\bf I}||\n",
    "\n",
    "    Choosing this constraint is that it forces the covariance to be\n",
    "    approximately parameter independent which justifies choosing the covariance\n",
    "    independent Gaussian Fisher information as above. To avoid having a dual\n",
    "    optimisation objective, we use a smooth and dynamic regularisation strength\n",
    "    which turns off the regularisation to focus on maximising the Fisher\n",
    "    information when the covariance has set the scale\n",
    "\n",
    "    .. math::\n",
    "        r(\\\\Lambda_2) = \\\\frac{\\\\lambda\\\\Lambda_2}{\\\\Lambda_2-\\\\exp\n",
    "        (-\\\\alpha\\\\Lambda_2)}.\n",
    "\n",
    "    Once the loss function is calculated the automatic gradient is then\n",
    "    calculated and used to update the network parameters via the optimiser\n",
    "    function. Note for large input data-sizes, large :math:`n_s` or massive\n",
    "    networks the gradients may need manually accumulating via the\n",
    "    :func:`~imnn.imnn._aggregated_imnn._AggregatedIMNN`.\n",
    "\n",
    "    ``_IMNN`` is designed as the parent class for a range of specific case\n",
    "    IMNNs. There is a helper function (IMNN) which should return the correct\n",
    "    case when provided with the correct data. These different subclasses are:\n",
    "\n",
    "    :func:`~imnn.SimulatorIMNN`:\n",
    "\n",
    "        Fit an IMNN using simulations generated on-the-fly from a jax (XLA\n",
    "        compilable) simulator\n",
    "\n",
    "    :func:`~imnn.GradientIMNN`:\n",
    "\n",
    "        Fit an IMNN using a precalculated set of fiducial simulations and their\n",
    "        derivatives with respect to model parameters\n",
    "\n",
    "    :func:`~imnn.NumericalGradientIMNN`:\n",
    "\n",
    "        Fit an IMNN using a precalculated set of fiducial simulations and\n",
    "        simulations generated using parameter values just above and below the\n",
    "        fiducial parameter values to make a numerical estimate of the\n",
    "        derivatives of the network outputs. Best stability is achieved when\n",
    "        seeds of the simulations are matched between all parameter directions\n",
    "        for the numerical derivative\n",
    "\n",
    "    :func:`~imnn.AggregatedSimulatorIMNN`:\n",
    "\n",
    "        ``SimulatorIMNN`` distributed over multiple jax devices and gradients\n",
    "        aggregated manually. This might be necessary for very large input sizes\n",
    "        as batching cannot be done when calculating the Fisher information\n",
    "        matrix\n",
    "\n",
    "    :func:`~imnn.AggregatedGradientIMNN`:\n",
    "\n",
    "        ``GradientIMNN`` distributed over multiple jax devices and gradients\n",
    "        aggregated manually. This might be necessary for very large input sizes\n",
    "        as batching cannot be done when calculating the Fisher information\n",
    "        matrix\n",
    "\n",
    "    :func:`~imnn.AggregatedNumericalGradientIMNN`:\n",
    "\n",
    "        ``NumericalGradientIMNN`` distributed over multiple jax devices and\n",
    "        gradients aggregated manually. This might be necessary for very large\n",
    "        input sizes as batching cannot be done when calculating the Fisher\n",
    "        information matrix\n",
    "\n",
    "    :func:`~imnn.DatasetGradientIMNN`:\n",
    "\n",
    "        ``AggregatedGradientIMNN`` with prebuilt TensorFlow datasets\n",
    "\n",
    "    :func:`~imnn.DatasetNumericalGradientIMNN`:\n",
    "\n",
    "        ``AggregatedNumericalGradientIMNN`` with prebuilt TensorFlow datasets\n",
    "\n",
    "    There are currently two other parent classes\n",
    "\n",
    "    :func:`~imnn.imnn._aggregated_imnn.AggregatedIMNN`:\n",
    "\n",
    "        This is the parent class which provides the fitting routine when the\n",
    "        gradients of the network parameters are aggregated manually rather than\n",
    "        automatically by jax. This is necessary if the size of an entire batch\n",
    "        of simulations (and their derivatives with respect to model parameters)\n",
    "        and the network parameters and their calculated gradients is too large\n",
    "        to fit into memory. Note there is a significant performance loss from\n",
    "        using the aggregation so it should only be used for these large data\n",
    "        cases\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_s : int\n",
    "        Number of simulations used to calculate network output covariance\n",
    "    n_d : int\n",
    "        Number of simulations used to calculate mean of network output\n",
    "        derivative with respect to the model parameters\n",
    "    n_params : int\n",
    "        Number of model parameters\n",
    "    n_summaries : int\n",
    "        Number of summaries, i.e. outputs of the network\n",
    "    input_shape : tuple\n",
    "        The shape of a single input to the network\n",
    "    Î¸_fid : float(n_params,)\n",
    "        The value of the fiducial parameter values used to generate inputs\n",
    "    validate : bool\n",
    "        Whether a validation set is being used\n",
    "    simulate : bool\n",
    "        Whether input simulations are generated on the fly\n",
    "    _run_with_pbar : bool\n",
    "        Book keeping parameter noting that a progress bar is used when\n",
    "        fitting (induces a performance hit). If ``run_with_pbar = True``\n",
    "        and ``run_without_pbar = True`` then a jit compilation error will\n",
    "        occur and so it is prevented\n",
    "    _run_without_pbar : bool\n",
    "        Book keeping parameter noting that a progress bar is not used when\n",
    "        fitting. If ``run_with_pbar = True`` and ``run_without_pbar = True``\n",
    "        then a jit compilation error will occur and so it is prevented\n",
    "    F : float(n_params, n_params)\n",
    "        Fisher information matrix calculated from the network outputs\n",
    "    invF : float(n_params, n_params)\n",
    "        Inverse Fisher information matrix calculated from the network outputs\n",
    "    C : float(n_summaries, n_summaries)\n",
    "        Covariance of the network outputs\n",
    "    invC : float(n_summaries, n_summaries)\n",
    "        Inverse covariance of the network outputs\n",
    "    Î¼ : float(n_summaries,)\n",
    "        Mean of the network outputs\n",
    "    dÎ¼_dÎ¸ : float(n_summaries, n_params)\n",
    "        Derivative of the mean of the network outputs with respect to model\n",
    "        parameters\n",
    "    state : :obj:state\n",
    "        The optimiser state used for updating the network parameters and\n",
    "        optimisation algorithm\n",
    "    initial_w : list\n",
    "        List of the network parameters values at initialisation (to restart)\n",
    "    final_w : list\n",
    "        List of the network parameters values at the end of fitting\n",
    "    best_w : list\n",
    "        List of the network parameters values which provide the maxmimum value\n",
    "        of the determinant of the Fisher matrix\n",
    "    w : list\n",
    "        List of the network parameters values (either final or best depending\n",
    "        on setting when calling fit(...))\n",
    "    history : dict\n",
    "        A dictionary containing the fitting history. Keys are\n",
    "            - **detF** -- determinant of the Fisher information at the end of\n",
    "              each iteration\n",
    "            - **detC** -- determinant of the covariance of network outputs at\n",
    "              the end of each iteration\n",
    "            - **detinvC** -- determinant of the inverse covariance of network\n",
    "              outputs at the end of each iteration\n",
    "            - **Î2** -- value of the covariance regularisation at the end of\n",
    "              each iteration\n",
    "            - **r** -- value of the regularisation coupling at the end of each\n",
    "              iteration\n",
    "            - **val_detF** -- determinant of the Fisher information of the\n",
    "              validation data at the end of each iteration\n",
    "            - **val_detC** -- determinant of the covariance of network outputs\n",
    "              given the validation data at the end of each iteration\n",
    "            - **val_detinvC** -- determinant of the inverse covariance of\n",
    "              network outputs given the validation data at the end of each\n",
    "              iteration\n",
    "            - **val_Î2** -- value of the covariance regularisation given the\n",
    "              validation data at the end of each iteration\n",
    "            - **val_r** -- value of the regularisation coupling given the\n",
    "              validation data at the end of each iteration\n",
    "            - **max_detF** -- maximum value of the determinant of the Fisher\n",
    "              information on the validation data (if available)\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    model:\n",
    "        Neural network as a function of network parameters and inputs\n",
    "    _get_parameters:\n",
    "        Function which extracts the network parameters from the state\n",
    "    _model_initialiser:\n",
    "        Function to initialise neural network weights from RNG and shape tuple\n",
    "    _opt_initialiser:\n",
    "        Function which generates the optimiser state from network parameters\n",
    "    _update:\n",
    "        Function which updates the state from a gradient\n",
    "\n",
    "    Todo\n",
    "    ----\n",
    "    - Finish all docstrings and documentation\n",
    "    - Update `NoiseNumericalGradientIMNN` to inherit from `_AggregatedIMNN`\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_s, n_d, n_params, n_summaries, input_shape, Î¸_fid,\n",
    "                 model, optimiser, key_or_state, dummy_input=None, no_invC=False, do_reg=True,\n",
    "                 evidence=False):\n",
    "        \"\"\"Constructor method\n",
    "\n",
    "        Initialises all _IMNN attributes, constructs neural network and its\n",
    "        initial parameter values and creates history dictionary\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_s : int\n",
    "            Number of simulations used to calculate summary covariance\n",
    "        n_d : int\n",
    "            Number of simulations used to calculate mean of summary derivative\n",
    "        n_params : int\n",
    "            Number of model parameters\n",
    "        n_summaries : int\n",
    "            Number of summaries, i.e. outputs of the network\n",
    "        input_shape : tuple\n",
    "            The shape of a single input to the network\n",
    "        Î¸_fid : float(n_params,)\n",
    "            The value of the fiducial parameter values used to generate inputs\n",
    "        model : tuple, len=2\n",
    "            Tuple containing functions to initialise neural network\n",
    "            ``fn(rng: int(2), input_shape: tuple) -> tuple, list`` and the\n",
    "            neural network as a function of network parameters and inputs\n",
    "            ``fn(w: list, d: float([None], input_shape)) -> float([None],\n",
    "            n_summaries)``.\n",
    "            (Essentibly stax-like, see `jax.experimental.stax <https://jax.read\n",
    "            thedocs.io/en/stable/jax.experimental.stax.html>`_))\n",
    "        optimiser : tuple, len=3\n",
    "            Tuple containing functions to generate the optimiser state\n",
    "            ``fn(x0: list) -> :obj:state``, to update the state from a list of\n",
    "            gradients ``fn(i: int, g: list, state: :obj:state) -> :obj:state``\n",
    "            and to extract network parameters from the state\n",
    "            ``fn(state: :obj:state) -> list``.\n",
    "            (See `jax.experimental.optimizers <https://jax.readthedocs.io/en/st\n",
    "            able/jax.experimental.optimizers.html>`_)\n",
    "        key_or_state : int(2) or :obj:state\n",
    "            Either a stateless random number generator or the state object of\n",
    "            an preinitialised optimiser\n",
    "        dummy_input : jraph.GraphsTuple or 'jax.numpy.DeviceArray'\n",
    "            Either a (padded) graph input or device array. If supplied ignores \n",
    "            `input_shape` parameter\n",
    "        \"\"\"\n",
    "        self.dummy_input=dummy_input\n",
    "        self._initialise_parameters(\n",
    "            n_s, n_d, n_params, n_summaries, input_shape, Î¸_fid)\n",
    "        self._initialise_model(model, optimiser, key_or_state)\n",
    "        self._initialise_history()\n",
    "        self.no_invC=no_invC\n",
    "        self.do_reg=do_reg\n",
    "        self.evidence=evidence\n",
    "\n",
    "\n",
    "    def _initialise_parameters(self, n_s, n_d, n_params, n_summaries,\n",
    "                               input_shape, Î¸_fid):\n",
    "        \"\"\"Performs type checking and initialisation of class attributes\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_s : int\n",
    "            Number of simulations used to calculate summary covariance\n",
    "        n_d : int\n",
    "            Number of simulations used to calculate mean of summary derivative\n",
    "        n_params : int\n",
    "            Number of model parameters\n",
    "        n_summaries : int\n",
    "            Number of summaries, i.e. outputs of the network\n",
    "        input_shape : tuple\n",
    "            The shape of a single input to the network\n",
    "        Î¸_fid : float(n_params,)\n",
    "            The value of the fiducial parameter values used to generate inputs\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        TypeError\n",
    "            Any of the parameters are not correct type\n",
    "        ValueError\n",
    "            Any of the parameters are ``None``\n",
    "            ``Î_fid`` has the wrong shape\n",
    "        \"\"\"\n",
    "        self.n_s = _check_type(n_s, int, \"n_s\")\n",
    "        self.n_d = _check_type(n_d, int, \"n_d\")\n",
    "        self.n_params = _check_type(n_params, int, \"n_params\")\n",
    "        self.n_summaries = _check_type(n_summaries, int, \"n_summaries\")\n",
    "        self.input_shape = _check_type(input_shape, tuple, \"input_shape\")\n",
    "        self.Î¸_fid = _check_input(Î¸_fid, (self.n_params,), \"Î¸_fid\")\n",
    "\n",
    "        self.validate = False\n",
    "        self.simulate = False\n",
    "        self._run_with_pbar = False\n",
    "        self._run_without_pbar = False\n",
    "\n",
    "        self.F = None\n",
    "        self.invF = None\n",
    "        self.C = None\n",
    "        self.invC = None\n",
    "        self.Î¼ = None\n",
    "        self.dÎ¼_dÎ¸ = None\n",
    "\n",
    "        self._model_initialiser = None\n",
    "        self.model = None\n",
    "        self._opt_initialiser = None\n",
    "        self._update = None\n",
    "        self._get_parameters = None\n",
    "        self.state = None\n",
    "        self.initial_w = None\n",
    "        self.final_w = None\n",
    "        self.best_w = None\n",
    "        self.w = None\n",
    "\n",
    "        self.history = None\n",
    "\n",
    "    def _initialise_model(self, model, optimiser, key_or_state):\n",
    "        \"\"\"Initialises neural network parameters or loads optimiser state\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : tuple, len=2\n",
    "            Tuple containing functions to initialise neural network\n",
    "            ``fn(rng: int(2), input_shape: tuple) -> tuple, list`` and\n",
    "            the neural network as a function of network parameters and inputs\n",
    "            ``fn(w: list, d: float([None], input_shape)) -> float([None],\n",
    "            n_summaries)``. (Essentibly stax-like, see `jax.experimental.stax\n",
    "            <https://jax.readthedocs.io/en/stable/jax.experimental.stax.html>`_\n",
    "            ))\n",
    "        optimiser : tuple or obj, len=3\n",
    "            Tuple containing functions to generate the optimiser state\n",
    "            ``fn(x0: list) -> :obj:state``, to update the state from a list of\n",
    "            gradients ``fn(i: int, g: list, state: :obj:state) -> :obj:state``\n",
    "            and to extract network parameters from the state\n",
    "            ``fn(state: :obj:state) -> list``.\n",
    "            (See `jax.experimental.optimizers <https://jax.readthedocs.io/en/st\n",
    "            able/jax.experimental.optimizers.html>`_)\n",
    "        key_or_state : int(2) or :obj:state\n",
    "            Either a stateless random number generator or the state object of\n",
    "            an preinitialised optimiser\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        The design of the model follows `jax's stax module <https://jax.readth\n",
    "        edocs.io/en/latest/jax.experimental.stax.html>`_ in that the model is\n",
    "        encapsulated by two functions, one to initialise the network and one to\n",
    "        call the model, i.e.::\n",
    "\n",
    "            import jax\n",
    "            from jax.experimental import stax\n",
    "\n",
    "            rng = jax.random.PRNGKey(0)\n",
    "\n",
    "            data_key, model_key = jax.random.split(rng)\n",
    "\n",
    "            input_shape = (10,)\n",
    "            inputs = jax.random.normal(data_key, shape=input_shape)\n",
    "\n",
    "            model = stax.serial(\n",
    "                stax.Dense(10),\n",
    "                stax.LeakyRelu,\n",
    "                stax.Dense(10),\n",
    "                stax.LeakyRelu,\n",
    "                stax.Dense(2))\n",
    "\n",
    "            output_shape, initial_params = model[0](model_key, input_shape)\n",
    "\n",
    "            outputs = model[1](initial_params, inputs)\n",
    "\n",
    "        Note that the model used in the IMNN is assumed to be totally\n",
    "        broadcastable, i.e. any batch shape can be used for inputs. This might\n",
    "        require having a layer which reshapes all batch dimensions into a\n",
    "        single dimension and then unwraps it at the last layer. A model such as\n",
    "        that above is already fully broadcastable.\n",
    "\n",
    "        The optimiser should follow `jax's experimental optimiser module <http\n",
    "        s://jax.readthedocs.io/en/stable/jax.experimental.optimizers.html>`_ in\n",
    "        that the optimiser is encapsulated by three functions, one to\n",
    "        initialise the state, one to update the state from a list of gradients\n",
    "        and one to extract the network parameters from the state, .i.e\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            from jax.experimental import optimizers\n",
    "            import jax.numpy as np\n",
    "\n",
    "            optimiser = optimizers.adam(step_size=1e-3)\n",
    "\n",
    "            initial_state = optimiser[0](initial_params)\n",
    "            params = optimiser[2](initial_state)\n",
    "\n",
    "            def scalar_output(params, inputs):\n",
    "                return np.sum(model[1](params, inputs))\n",
    "\n",
    "            counter = 0\n",
    "            grad = jax.grad(scalar_output, argnums=0)(params, inputs)\n",
    "            state = optimiser[1](counter, grad, state)\n",
    "\n",
    "        This function either initialises the neural network or the state if\n",
    "        passed a stateless random number generator in ``key_or_state`` or loads\n",
    "        a predefined state if the state is passed to ``key_or_state``. The\n",
    "        functions get mapped to the class functions\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            self.model = model[1]\n",
    "            self._model_initialiser = model[0]\n",
    "\n",
    "            self._opt_initialiser = optimiser[0]\n",
    "            self._update = optimiser[1]\n",
    "            self._get_parameters = optimiser[2]\n",
    "\n",
    "        The state is made into the ``state`` class attribute and the parameters\n",
    "        are assigned to ``initial_w``, ``final_w``, ``best_w`` and ``w`` class\n",
    "        attributes (where ``w`` stands for weights).\n",
    "\n",
    "        There is some type checking done, but for freedom of choice of model\n",
    "        there will be very few raised warnings.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        TypeError\n",
    "            If the random number generator is not correct, or if there is no\n",
    "            possible way to construct a model or an optimiser from the passed\n",
    "            parameters\n",
    "        ValueError\n",
    "            If any input is ``None`` or if the functions for the model or\n",
    "            optimiser do not conform to the necessary specifications\n",
    "        \"\"\"\n",
    "\n",
    "        # initialize FLAX model here\n",
    "        self._model_initialiser = model.init\n",
    "        self.model = model.apply\n",
    "\n",
    "        # unpack optimiser\n",
    "        self._opt_initialiser, self._update = optimiser\n",
    "\n",
    "        #state, key = _check_state(key_or_state)\n",
    "        key = key_or_state\n",
    "\n",
    "        if key is not None:\n",
    "            key = _check_input(key, (2,), \"key_or_state\")\n",
    "            if self.dummy_input is None:\n",
    "                dummy_x = jax.random.uniform(key, self.input_shape)\n",
    "            else:\n",
    "                dummy_x = self.dummy_input\n",
    "\n",
    "            # INITIAL PARAMS\n",
    "            self.initial_w = self._model_initialiser(key, dummy_x)\n",
    "            \n",
    "            # DUMMY OUTPUT\n",
    "            output = self.model(self.initial_w, dummy_x)\n",
    "            # check to see if right shape\n",
    "            _check_model_output(output.shape, (self.n_summaries,))\n",
    "            # INITIAL STATE\n",
    "            self.state = self._opt_initialiser(self.initial_w)\n",
    "\n",
    "\n",
    "        else:\n",
    "            self.state = state\n",
    "            try:\n",
    "                self._get_parameters(self.state)\n",
    "            except Exception:\n",
    "                raise TypeError(\"`state` is not valid for extracting \" +\n",
    "                                \"parameters from\")\n",
    "\n",
    "        self.dummy_x = dummy_x\n",
    "        self.initial_w = self._model_initialiser(key, dummy_x)\n",
    "        self.final_w = self._model_initialiser(key, dummy_x)\n",
    "        self.best_w = self._model_initialiser(key, dummy_x)\n",
    "        self.w = self._model_initialiser(key, dummy_x)\n",
    "\n",
    "\n",
    "    def _initialise_history(self):\n",
    "        \"\"\"Initialises history dictionary attribute\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        The contents of the history dictionary are\n",
    "            - **detF** -- determinant of the Fisher information at the end of\n",
    "              each iteration\n",
    "            - **detC** -- determinant of the covariance of network outputs at\n",
    "              the end of each iteration\n",
    "            - **detinvC** -- determinant of the inverse covariance of network\n",
    "              outputs at the end of each iteration\n",
    "            - **Î2** -- value of the covariance regularisation at the end of\n",
    "              each iteration\n",
    "            - **r** -- value of the regularisation coupling at the end of each\n",
    "              iteration\n",
    "            - **val_detF** -- determinant of the Fisher information of the\n",
    "              validation data at the end of each iteration\n",
    "            - **val_detC** -- determinant of the covariance of network outputs\n",
    "              given the validation data at the end of each iteration\n",
    "            - **val_detinvC** -- determinant of the inverse covariance of\n",
    "              network outputs given the validation data at the end of each\n",
    "              iteration\n",
    "            - **val_Î2** -- value of the covariance regularisation given the\n",
    "              validation data at the end of each iteration\n",
    "            - **val_r** -- value of the regularisation coupling given the\n",
    "              validation data at the end of each iteration\n",
    "            - **max_detF** -- maximum value of the determinant of the Fisher\n",
    "              information on the validation data (if available)\n",
    "\n",
    "        \"\"\"\n",
    "        self.history = {\n",
    "            \"detF\": np.zeros((0,)),\n",
    "            \"detC\": np.zeros((0,)),\n",
    "            \"detinvC\": np.zeros((0,)),\n",
    "            \"Î2\": np.zeros((0,)),\n",
    "            \"r\": np.zeros((0,)),\n",
    "            \"val_detF\": np.zeros((0,)),\n",
    "            \"val_detC\": np.zeros((0,)),\n",
    "            \"val_detinvC\": np.zeros((0,)),\n",
    "            \"val_Î2\": np.zeros((0,)),\n",
    "            \"val_r\": np.zeros((0,)),\n",
    "            \"max_detF\": np.float32(0.)\n",
    "        }\n",
    "\n",
    "    def _set_history(self, results):\n",
    "        \"\"\"Places results from fitting into the history dictionary\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        results : list\n",
    "            List of results from fitting procedure. These are:\n",
    "                - **detF** *(float(n_iterations, 2))* -- determinant of the\n",
    "                  Fisher information, ``detF[:, 0]`` for training and\n",
    "                  ``detF[:, 1]`` for validation\n",
    "                - **detC** *(float(n_iterations, 2))* -- determinant of the\n",
    "                  covariance of network outputs, ``detC[:, 0]`` for training\n",
    "                  and ``detC[:, 1]`` for validation\n",
    "                - **detinvC** *(float(n_iterations, 2))* -- determinant of the\n",
    "                  inverse covariance of network outputs, ``detinvC[:, 0]`` for\n",
    "                  training and ``detinvC[:, 1]`` for validation\n",
    "                - **Î2** *(float(n_iterations, 2))* -- value of the covariance\n",
    "                  regularisation, ``Î2[:, 0]`` for training and ``Î2[:, 1]``\n",
    "                  for validation\n",
    "                - **r** *(float(n_iterations, 2))* -- value of the\n",
    "                  regularisation coupling, ``r[:, 0]`` for training and\n",
    "                  ``r[:, 1]`` for validation\n",
    "\n",
    "        \"\"\"\n",
    "        keys = [\"detF\", \"detC\", \"detinvC\", \"Î2\", \"r\"]\n",
    "        for result, key in zip(results, keys):\n",
    "            self.history[key] = np.hstack([self.history[key], result[:, 0]])\n",
    "            if self.validate:\n",
    "                self.history[f\"val_{key}\"] = np.hstack(\n",
    "                    [self.history[f\"val_{key}\"], result[:, 1]])\n",
    "\n",
    "    def _set_inputs(self, rng, max_iterations):\n",
    "        \"\"\"Builds list of inputs for the XLA compilable fitting routine\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        rng : int(2,) or None\n",
    "            A stateless random number generator\n",
    "        max_iterations : int\n",
    "            Maximum number of iterations to run the fitting procedure for\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        The list of inputs to the routine are\n",
    "            - **max_detF** *(float)* -- The maximum value of the determinant of\n",
    "              the Fisher information matrix calculated so far. This is zero if\n",
    "              not run before or the value from previous calls to ``fit``\n",
    "            - **best_w** *(list)* -- The value of the network parameters which\n",
    "              obtained the maxmimum determinant of the Fisher information\n",
    "              matrix. This is the initial network parameter values if not run\n",
    "              before\n",
    "            - **detF** *(float(max_iterations, 1) or\n",
    "              float(max_iterations, 2))* -- A container for all possible values\n",
    "              of the determinant of the Fisher information matrix during each\n",
    "              iteration of fitting. If there is no validation (for simulation\n",
    "              on-the-fly for example) then this container has a shape of\n",
    "              ``(max_iterations, 1)``, otherwise validation values are stored\n",
    "              in ``detF[:, 1]``.\n",
    "            - **detC** *(float(max_iterations, 1) or\n",
    "              float(max_iterations, 2))* -- A container for all possible values\n",
    "              of the determinant of the covariance of network outputs during\n",
    "              each iteration of fitting. If there is no validation (for\n",
    "              simulation on-the-fly for example) then this container has a\n",
    "              shape of ``(max_iterations, 1)``, otherwise validation values are\n",
    "              stored in ``detC[:, 1]``.\n",
    "            - **detF** *(float(max_iterations, 1) or\n",
    "              float(max_iterations, 2))* -- A container for all possible values\n",
    "              of the determinant of the inverse covariance of network outputs\n",
    "              during each iteration of fitting. If there is no validation (for\n",
    "              simulation on-the-fly for example) then this container has a\n",
    "              shape of ``(max_iterations, 1)``, otherwise validation values are\n",
    "              stored in ``detinvC[:, 1]``.\n",
    "            - **Î2** *(float(max_iterations, 1) or\n",
    "              float(max_iterations, 2))* -- A container for all possible values\n",
    "              of the covariance regularisation during each iteration of\n",
    "              fitting. If there is no validation (for simulation on-the-fly for\n",
    "              example) then this container has a shape of\n",
    "              ``(max_iterations, 1)``, otherwise validation values are stored\n",
    "              in ``Î2[:, 1]``.\n",
    "            - **r** *(float(max_iterations, 1) or\n",
    "              float(max_iterations, 2))* -- A container for all possible values\n",
    "              of the regularisation coupling strength during each iteration of\n",
    "              fitting. If there is no validation (for simulation on-the-fly for\n",
    "              example) then this container has a shape of\n",
    "              ``(max_iterations, 1),`` otherwise validation values are stored\n",
    "              in ``r[:, 1]``.\n",
    "            - **counter** *(int)* -- Iteration counter used to note whether the\n",
    "              while loop reaches ``max_iterations``. If not, the history\n",
    "              objects (above) get truncated to length ``counter``. This starts\n",
    "              with value zero\n",
    "            - **patience_counter** *(int)* -- Counts the number of iterations\n",
    "              where there is no increase in the value of the determinant of the\n",
    "              Fisher information matrix, used for early stopping. This starts\n",
    "              with value zero\n",
    "            - **state** *(:obj:state)* -- The current optimiser state used for\n",
    "              updating the network parameters and optimisation algorithm\n",
    "            - **rng** *(int(2,))* -- A stateless random number generator which\n",
    "              gets updated on each iteration\n",
    "\n",
    "        Todo\n",
    "        ----\n",
    "        ``rng`` is currently only used for on-the-fly simulation but could\n",
    "        easily be updated to allow for stochastic models\n",
    "        \"\"\"\n",
    "        if self.validate:\n",
    "            shape = (max_iterations, 2)\n",
    "        else:\n",
    "            shape = (max_iterations, 1)\n",
    "\n",
    "        return (\n",
    "            self.history[\"max_detF\"], self.best_w, np.zeros(shape),\n",
    "            np.zeros(shape), np.zeros(shape), np.zeros(shape), np.zeros(shape),\n",
    "            np.int32(0), np.int32(0), self.state, self.w, rng)\n",
    "\n",
    "    def fit(self, Î», Ïµ, Î³=1000., rng=None, patience=100, min_iterations=100,\n",
    "            max_iterations=int(1e5), print_rate=None, best=True):\n",
    "        \"\"\"Fitting routine for the IMNN\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Î» : float\n",
    "            Coupling strength of the regularisation\n",
    "        Ïµ : float\n",
    "            Closeness criterion describing how close to the 1 the determinant\n",
    "            of the covariance (and inverse covariance) of the network outputs\n",
    "            is desired to be\n",
    "        rng : int(2,) or None, default=None\n",
    "            Stateless random number generator\n",
    "        patience : int, default=10\n",
    "            Number of iterations where there is no increase in the value of the\n",
    "            determinant of the Fisher information matrix, used for early\n",
    "            stopping\n",
    "        min_iterations : int, default=100\n",
    "            Number of iterations that should be run before considering early\n",
    "            stopping using the patience counter\n",
    "        max_iterations : int, default=int(1e5)\n",
    "            Maximum number of iterations to run the fitting procedure for\n",
    "        print_rate : int or None, default=None,\n",
    "            Number of iterations before updating the progress bar whilst\n",
    "            fitting. There is a performance hit from updating the progress bar\n",
    "            more often and there is a large performance hit from using the\n",
    "            progress bar at all. (Possible ``RET_CHECK`` failure if\n",
    "            ``print_rate`` is not ``None`` when using GPUs).\n",
    "            For this reason it is set to None as default\n",
    "        best : bool, default=True\n",
    "            Whether to set the network parameter attribute ``self.w`` to the\n",
    "            parameter values that obtained the maximum determinant of\n",
    "            the Fisher information matrix or the parameter values at the final\n",
    "            iteration of fitting\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "\n",
    "        We are going to summarise the mean and variance of some random Gaussian\n",
    "        noise with 10 data points per example using a SimulatorIMNN. In this\n",
    "        case we are going to generate the simulations on-the-fly with a\n",
    "        simulator written in jax (from the examples directory). We will use\n",
    "        1000 simulations to estimate the covariance of the network outputs and\n",
    "        the derivative of the mean of the network outputs with respect to the\n",
    "        model parameters (Gaussian mean and variance) and generate the\n",
    "        simulations at a fiducial Î¼=0 and Î£=1. The network will be a stax model\n",
    "        with hidden layers of ``[128, 128, 128]`` activated with leaky relu and\n",
    "        outputting 2 summaries. Optimisation will be via Adam with a step size\n",
    "        of ``1e-3``. Rather arbitrarily we'll set the regularisation strength\n",
    "        and covariance identity constraint to Î»=10 and Ïµ=0.1 (these are\n",
    "        relatively unimportant for such an easy model).\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            import jax\n",
    "            import jax.numpy as np\n",
    "            from jax.experimental import stax, optimizers\n",
    "            from imnn import SimulatorIMNN\n",
    "\n",
    "            rng = jax.random.PRNGKey(0)\n",
    "\n",
    "            n_s = 1000\n",
    "            n_d = 1000\n",
    "            n_params = 2\n",
    "            n_summaries = 2\n",
    "            input_shape = (10,)\n",
    "            simulator_args = {\"input_shape\": input_shape}\n",
    "            Î¸_fid = np.array([0., 1.])\n",
    "\n",
    "            def simulator(rng, Î¸):\n",
    "                return Î¸[0] + jax.random.normal(\n",
    "                    rng, shape=input_shape) * np.sqrt(Î¸[1])\n",
    "\n",
    "            model = stax.serial(\n",
    "                stax.Dense(128),\n",
    "                stax.LeakyRelu,\n",
    "                stax.Dense(128),\n",
    "                stax.LeakyRelu,\n",
    "                stax.Dense(128),\n",
    "                stax.LeakyRelu,\n",
    "                stax.Dense(n_summaries))\n",
    "            optimiser = optimizers.adam(step_size=1e-3)\n",
    "\n",
    "            Î» = 10.\n",
    "            Ïµ = 0.1\n",
    "\n",
    "            model_key, fit_key = jax.random.split(rng)\n",
    "\n",
    "            imnn = SimulatorIMNN(\n",
    "                n_s=n_s, n_d=n_d, n_params=n_params, n_summaries=n_summaries,\n",
    "                input_shape=input_shape, Î¸_fid=Î¸_fid, model=model,\n",
    "                optimiser=optimiser, key_or_state=model_key,\n",
    "                simulator=simulator)\n",
    "\n",
    "            imnn.fit(Î», Ïµ, rng=fit_key, min_iterations=1000, patience=250,\n",
    "                     print_rate=None)\n",
    "\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        A minimum number of interations should be be run before stopping based\n",
    "        on a maximum determinant of the Fisher information achieved since the\n",
    "        loss function has dual objectives. Since the determinant of the\n",
    "        covariance of the network outputs is forced to 1 quickly, this can be\n",
    "        at the detriment to the value of the determinant of the Fisher\n",
    "        information matrix early in the fitting procedure. For this reason\n",
    "        starting early stopping after the covariance has converged is advised.\n",
    "        This is not currently implemented but could be considered in the\n",
    "        future.\n",
    "\n",
    "        The best fit network parameter values are probably not the most\n",
    "        representative set of parameters when simulating on-the-fly since there\n",
    "        is a high chance of a statistically overly-informative set of data\n",
    "        being generated. Instead, if using :func:`~imnn.SimulatorIMNN.fit()`\n",
    "        consider using ``best=False`` which sets ``self.w=self.final_w`` which\n",
    "        are the network parameter values obtained in the last iteration. Also\n",
    "        consider using a larger ``patience`` value if using\n",
    "        :func:`~imnn.SimulatorIMNN.fit()` to overcome the fact that a flukish\n",
    "        high value for the determinant might have been obtained due to the\n",
    "        realisation of the dataset.\n",
    "\n",
    "        Due to some unusual thing, that I can't work out, there is a massive\n",
    "        performance hit when calling ``jax.jit(self._fit)`` compared with\n",
    "        directly decorating ``_fit`` with\n",
    "        ``@partial(jax.jit(static_argnums=0))``. Unfortunately this means\n",
    "        having to duplicate ``_fit`` to include a version where the loop\n",
    "        condition is decorated with a progress bar because the ``tqdm``\n",
    "        module cannot use a jitted tracer. If the progress bar is not used then\n",
    "        the fully decorated jitted ``_fit`` function is used and it is super\n",
    "        quick. Otherwise, just the body of the loop is jitted so that the\n",
    "        condition function can be decorated by the progress bar (at the\n",
    "        expense of a performance hit). I imagine that something can be improved\n",
    "        here.\n",
    "\n",
    "        There is a chance of a ``RET_CHECK`` failure when using the progress\n",
    "        bar on GPUs (this doesn't seem to be a problem on CPUs). If this is the\n",
    "        case then `print_rate=None` should be used\n",
    "\n",
    "        Methods\n",
    "        -------\n",
    "        _fit:\n",
    "            Main fitting function implemented as a ``jax.lax.while_loop``\n",
    "        _fit_pbar:\n",
    "            Main fitting function as a ``jax.lax.while_loop`` with progress bar\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        TypeError\n",
    "            If any input has the wrong type\n",
    "        ValueError\n",
    "            If any input (except ``rng`` and ``print_rate``) are ``None``\n",
    "        ValueError\n",
    "            If ``rng`` has the wrong shape\n",
    "        ValueError\n",
    "            If ``rng`` is ``None`` but simulating on-the-fly\n",
    "        ValueError\n",
    "            If calling fit with ``print_rate=None`` after previous call with\n",
    "            ``print_rate`` as an integer value\n",
    "        ValueError\n",
    "            If calling fit with ``print_rate`` as an integer after previous\n",
    "            call with ``print_rate=None``\n",
    "\n",
    "        Todo\n",
    "        ----\n",
    "        - ``rng`` is currently only used for on-the-fly simulation but could\n",
    "          easily be updated to allow for stochastic models\n",
    "        - Automatic detection of convergence based on value ``r`` when early\n",
    "          stopping can be started\n",
    "        \"\"\"\n",
    "\n",
    "        @jax.jit\n",
    "        def _fit(inputs):\n",
    "\n",
    "            return jax.lax.while_loop(\n",
    "                partial(self._fit_cond, patience=patience,\n",
    "                        max_iterations=max_iterations),\n",
    "                partial(self._fit, Î»=Î», Î±=Î±, Î³=Î³, min_iterations=min_iterations),\n",
    "                inputs)\n",
    "\n",
    "        def _fit_pbar(inputs):\n",
    "\n",
    "            return jax.lax.while_loop(\n",
    "                progress_bar(max_iterations, patience, print_rate)(\n",
    "                    partial(self._fit_cond, patience=patience,\n",
    "                            max_iterations=max_iterations)),\n",
    "                jax.jit(\n",
    "                    partial(self._fit, Î»=Î», Î±=Î±,\n",
    "                            min_iterations=min_iterations)),\n",
    "                inputs)\n",
    "\n",
    "        Î» = _check_type(Î», float, \"Î»\")\n",
    "        Ïµ = _check_type(Ïµ, float, \"Ïµ\")\n",
    "        Î³ = _check_type(Î³, float, \"Î³\")\n",
    "        Î± = self.get_Î±(Î», Ïµ)\n",
    "        patience = _check_type(patience, int, \"patience\")\n",
    "        min_iterations = _check_type(min_iterations, int, \"min_iterations\")\n",
    "        max_iterations = _check_type(max_iterations, int, \"max_iterations\")\n",
    "        best = _check_boolean(best, \"best\")\n",
    "        if self.simulate and (rng is None):\n",
    "            raise ValueError(\"`rng` is necessary when simulating.\")\n",
    "        rng = _check_input(rng, (2,), \"rng\", allow_None=True)\n",
    "        inputs = self._set_inputs(rng, max_iterations)\n",
    "        if print_rate is None:\n",
    "            if self._run_with_pbar:\n",
    "                raise ValueError(\n",
    "                    \"Cannot run IMNN without progress bar after running \" +\n",
    "                    \"with progress bar. Either set `print_rate` to an int \" +\n",
    "                    \"or reinitialise the IMNN.\")\n",
    "            else:\n",
    "                self._run_without_pbar = True\n",
    "                results = _fit(inputs)\n",
    "        else:\n",
    "            if self._run_without_pbar:\n",
    "                raise ValueError(\n",
    "                    \"Cannot run IMNN with progress bar after running \" +\n",
    "                    \"without progress bar. Either set `print_rate` to None \" +\n",
    "                    \"or reinitialise the IMNN.\")\n",
    "            else:\n",
    "                print_rate = _check_type(print_rate, int, \"print_rate\")\n",
    "                self._run_with_pbar = True\n",
    "                results = _fit_pbar(inputs)\n",
    "        self.history[\"max_detF\"] = results[0]\n",
    "        self.best_w = results[1]\n",
    "        self._set_history(\n",
    "            (results[2][:results[7]],\n",
    "             results[3][:results[7]],\n",
    "             results[4][:results[7]],\n",
    "             results[5][:results[7]],\n",
    "             results[6][:results[7]]))\n",
    "        if len(results) == 12:\n",
    "            self.state = results[-3]\n",
    "        self.final_w = results[-2] #self._get_parameters(self.state)\n",
    "        if best:\n",
    "            w = self.best_w\n",
    "        else:\n",
    "            w = self.final_w\n",
    "        self.set_F_statistics(w, key=rng)\n",
    "\n",
    "    def _get_fitting_keys(self, rng):\n",
    "        \"\"\"Generates random numbers for simulation generation if needed\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        rng : int(2,) or None\n",
    "            A random number generator\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int(2,), int(2,), int(2,) or None, None, None:\n",
    "            A new random number generator and random number generators for\n",
    "            training and validation, or empty values\n",
    "        \"\"\"\n",
    "        if rng is not None:\n",
    "            return jax.random.split(rng, num=3)\n",
    "        else:\n",
    "            return None, None, None\n",
    "\n",
    "    def get_Î±(self, Î», Ïµ):\n",
    "        \"\"\"Calculate rate parameter for regularisation from closeness criterion\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Î» : float\n",
    "            coupling strength of the regularisation\n",
    "        Ïµ : float\n",
    "            closeness criterion describing how close to the 1 the determinant\n",
    "            of the covariance (and inverse covariance) of the network outputs\n",
    "            is desired to be\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float:\n",
    "            The steepness of the tanh-like function (or rate) which determines\n",
    "            how fast the determinant of the covariance of the network outputs\n",
    "            should be pushed to 1\n",
    "        \"\"\"\n",
    "        return - math.log(Ïµ * (Î» - 1.) + Ïµ ** 2. / (1 + Ïµ)) / Ïµ\n",
    "\n",
    "    def _fit(self, inputs, Î»=None, Î±=None, Î³=None,  min_iterations=None):\n",
    "        \"\"\"Single iteration fitting algorithm\n",
    "\n",
    "        This function performs the network parameter updates first getting\n",
    "        any necessary random number generators for simulators and then\n",
    "        extracting the network parameters from the state. These parameters\n",
    "        are used to calculate the gradient with respect to the network\n",
    "        parameters of the loss function (see _IMNN class docstrings).\n",
    "        Once the loss function is calculated the gradient is then used to\n",
    "        update the network parameters via the optimiser function and the\n",
    "        current iterations statistics are saved to the history arrays. If\n",
    "        validation is used (recommended for ``GradientIMNN`` and\n",
    "        ``NumericalGradientIMNN``) then all necessary statistics to\n",
    "        calculate the loss function are calculated and pushed to the\n",
    "        history arrays.\n",
    "\n",
    "        The ``patience_counter`` is increased if the value of determinant\n",
    "        of the Fisher information matrix does not increase over the\n",
    "        previous iterations upto ``patience`` number of iterations at which\n",
    "        point early stopping occurs, but only if the number of iterations\n",
    "        so far performed is greater than a specified ``min_iterations``.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : tuple\n",
    "            - **max_detF** *(float)* -- Maximum value of the determinant of the\n",
    "              Fisher information matrix calculated so far\n",
    "            - **best_w** *(list)* -- Value of the network parameters which\n",
    "              obtained the maxmimum determinant of the Fisher information\n",
    "              matrix\n",
    "            - **detF** *(float(max_iterations, 1)\n",
    "              or float(max_iterations, 2))* -- History of the determinant of\n",
    "              the Fisher information matrix\n",
    "            - **detC** *(float(max_iterations, 1) or float(max_iterations, 2))*\n",
    "              -- History of the determinant of the covariance of network\n",
    "              outputs\n",
    "            - **detinvC** *(float(max_iterations, 1)\n",
    "              or float(max_iterations, 2))* -- History of the determinant of\n",
    "              the inverse covariance of network outputs\n",
    "            - **Î2** *(float(max_iterations, 1) or float(max_iterations, 2))*\n",
    "              -- History of the covariance regularisation\n",
    "            - **r** *(float(max_iterations, 1) or float(max_iterations, 2))*\n",
    "              -- History of the regularisation coupling strength\n",
    "            - **counter** *(int)* -- While loop iteration counter\n",
    "            - **patience_counter** *(int)* -- Number of iterations where there\n",
    "              is no increase in the value of the determinant of the Fisher\n",
    "              information matrix\n",
    "            - **state** *(:obj: state)* -- Optimiser state used for updating\n",
    "              the network parameters and optimisation algorithm\n",
    "            - **rng** *(int(2,))* -- Stateless random number generator\n",
    "\n",
    "        Î» : float\n",
    "            Coupling strength of the regularisation\n",
    "        Î± : float\n",
    "            Rate parameter for regularisation coupling\n",
    "        min_iterations : int\n",
    "            Number of iterations that should be run before considering early\n",
    "            stopping using the patience counter\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple:\n",
    "            loop variables (described in Parameters)\n",
    "        \"\"\"\n",
    "\n",
    "        max_detF, best_w, detF, detC, detinvC, Î2, r, \\\n",
    "            counter, patience_counter, state, w, rng = inputs\n",
    "        rng, training_key, validation_key = self._get_fitting_keys(rng)\n",
    "\n",
    "\n",
    "        grad, results = jax.grad(\n",
    "            self._get_loss, argnums=0, has_aux=True)(w, Î», Î±, Î³, training_key)\n",
    "\n",
    "        #if self.pass_params:\n",
    "        updates, state = self._update(grad, state, w)\n",
    "\n",
    "        w = optax.apply_updates(w, updates) # UPDATE PARAMS\n",
    "\n",
    "        detF, detC, detinvC, Î2, r = self._update_history(\n",
    "            results, (detF, detC, detinvC, Î2, r), counter, 0)\n",
    "        if self.validate:\n",
    "            F, C, invC, *_ = self._get_F_statistics(\n",
    "                w, key=validation_key, validate=True)\n",
    "            _Î2 = self._get_regularisation(C, invC)\n",
    "            _r = self._get_regularisation_strength(_Î2, Î», Î±)\n",
    "            results = (F, C, invC, _Î2, _r)\n",
    "            detF, detC, detinvC, Î2, r = self._update_history(\n",
    "                results, (detF, detC, detinvC, Î2, r), counter, 1)\n",
    "        _detF = np.linalg.det(results[0])\n",
    "        patience_counter, counter, _, max_detF, __, best_w = \\\n",
    "            jax.lax.cond(\n",
    "                np.greater(_detF, max_detF),\n",
    "                self._update_loop_vars,\n",
    "                lambda inputs: self._check_loop_vars(inputs, min_iterations),\n",
    "                (patience_counter, counter, _detF, max_detF, w, best_w))\n",
    "        return (max_detF, best_w, detF, detC, detinvC, Î2, r,\n",
    "                counter + np.int32(1), patience_counter, state, w, rng)\n",
    "\n",
    "    def _fit_cond(self, inputs, patience, max_iterations):\n",
    "        \"\"\"Stopping condition for the fitting loop\n",
    "\n",
    "        The stopping conditions due to reaching ``max_iterations`` or the\n",
    "        patience counter reaching ``patience`` due to ``patience_counter``\n",
    "        number of iterations without increasing the determinant of the\n",
    "        Fisher information matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : tuple\n",
    "            - **max_detF** *(float)* -- Maximum value of the determinant of the\n",
    "              Fisher information matrix calculated so far\n",
    "            - **best_w** *(list)* -- Value of the network parameters which\n",
    "              obtained the maxmimum determinant of the Fisher information\n",
    "              matrix\n",
    "            - **detF** *(float(max_iterations, 1)\n",
    "              or float(max_iterations, 2))* -- History of the determinant of\n",
    "              the Fisher information matrix\n",
    "            - **detC** *(float(max_iterations, 1) or float(max_iterations, 2))*\n",
    "              -- History of the determinant of the covariance of network\n",
    "              outputs\n",
    "            - **detinvC** *(float(max_iterations, 1)\n",
    "              or float(max_iterations, 2))* -- History of the determinant of\n",
    "              the inverse covariance of network outputs\n",
    "            - **Î2** *(float(max_iterations, 1) or float(max_iterations, 2))*\n",
    "              -- History of the covariance regularisation\n",
    "            - **r** *(float(max_iterations, 1) or float(max_iterations, 2))*\n",
    "              -- History of the regularisation coupling strength\n",
    "            - **counter** *(int)* -- While loop iteration counter\n",
    "            - **patience_counter** *(int)* -- Number of iterations where there\n",
    "              is no increase in the value of the determinant of the Fisher\n",
    "              information matrix\n",
    "            - **state** *(:obj: state)* -- Optimiser state used for updating\n",
    "              the network parameters and optimisation algorithm\n",
    "            - **rng** *(int(2,))* -- Stateless random number generator\n",
    "\n",
    "        patience : int\n",
    "            Number of iterations to stop the fitting when there is no increase\n",
    "            in the value of the determinant of the Fisher information matrix\n",
    "        max_iterations : int\n",
    "        Maximum number of iterations to run the fitting procedure for\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool:\n",
    "            True if either the ``patience_counter`` has not reached the\n",
    "            ``patience`` criterion or if the ``counter`` has not reached\n",
    "            ``max_iterations``\n",
    "        \"\"\"\n",
    "        return np.logical_and(\n",
    "            np.less(inputs[-4], patience),\n",
    "            np.less(inputs[-5], max_iterations))\n",
    "\n",
    "    def _update_loop_vars(self, inputs):\n",
    "        \"\"\"Updates input parameters if ``max_detF`` is increased\n",
    "\n",
    "        If the determinant of the Fisher information matrix calculated\n",
    "        in a given iteration is larger than the ``max_detF`` calculated\n",
    "        so far then the ``patience_counter`` is reset to zero and the\n",
    "        ``max_detF`` is replaced with the current value of ``detF`` and\n",
    "        the network parameters in this iteration replace the previous\n",
    "        parameters which obtained the highest determinant of the Fisher\n",
    "        information, ``best_w``.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : tuple\n",
    "            - **patience_counter** *(int)* -- Number of iterations where\n",
    "              there is no increase in the value of the determinant of the\n",
    "              Fisher information matrix\n",
    "            - **counter** *(int)* -- While loop iteration counter\n",
    "            - **detF** *(float(max_iterations, 1)\n",
    "              or float(max_iterations, 2))* -- History of the determinant\n",
    "              of the Fisher information matrix\n",
    "            - **max_detF** *(float)* -- Maximum value of the determinant of\n",
    "              the Fisher information matrix calculated so far\n",
    "            - **w** *(list)* -- Value of the network parameters which in\n",
    "              current iteration\n",
    "            - **best_w** *(list)* -- Value of the network parameters which\n",
    "              obtained the maxmimum determinant of the Fisher information\n",
    "              matrix\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple:\n",
    "            (described in Parameters)\n",
    "        \"\"\"\n",
    "        patience_counter, counter, detF, max_detF, w, best_w = inputs\n",
    "        return (np.int32(0), counter, detF, detF, w, w)\n",
    "\n",
    "    def _check_loop_vars(self, inputs, min_iterations):\n",
    "        \"\"\"Updates ``patience_counter`` if ``max_detF`` not increased\n",
    "\n",
    "        If the determinant of the Fisher information matrix calculated\n",
    "        in a given iteration is not larger than the ``max_detF``\n",
    "        calculated so far then the ``patience_counter`` is increased by\n",
    "        one as long as the number of iterations is greater than the\n",
    "        minimum number of iterations that should be run.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : tuple\n",
    "            - **patience_counter** *(int)* -- Number of iterations where\n",
    "              there is no increase in the value of the determinant of the\n",
    "              Fisher information matrix\n",
    "            - **counter** *(int)* -- While loop iteration counter\n",
    "            - **detF** *(float(max_iterations, 1)\n",
    "              or float(max_iterations, 2))* -- History of the determinant\n",
    "              of the Fisher information matrix\n",
    "            - **max_detF** *(float)* -- Maximum value of the determinant of\n",
    "              the Fisher information matrix calculated so far\n",
    "            - **w** *(list)* -- Value of the network parameters which in\n",
    "              current iteration\n",
    "            - **best_w** *(list)* -- Value of the network parameters which\n",
    "              obtained the maxmimum determinant of the Fisher information\n",
    "              matrix\n",
    "        min_iterations : int\n",
    "            Number of iterations that should be run before considering early\n",
    "            stopping using the patience counter\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple:\n",
    "            (described in Parameters)\n",
    "        \"\"\"\n",
    "        patience_counter, counter, detF, max_detF, w, best_w = inputs\n",
    "        patience_counter = jax.lax.cond(\n",
    "            np.greater(counter, min_iterations),\n",
    "            lambda patience_counter: patience_counter + np.int32(1),\n",
    "            lambda patience_counter: patience_counter,\n",
    "            patience_counter)\n",
    "        return (patience_counter, counter, detF, max_detF, w, best_w)\n",
    "\n",
    "    def _update_history(self, inputs, history, counter, ind):\n",
    "        \"\"\"Puts current fitting statistics into history arrays\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : tuple\n",
    "            Fitting statistics calculated on a single iteration\n",
    "                - **F** *(float(n_params, n_params))* -- Fisher information\n",
    "                  matrix\n",
    "                - **C** *(float(n_summaries, n_summaries))* -- Covariance of\n",
    "                  network outputs\n",
    "                - **invC** *(float(n_summaries, n_summaries))* -- Inverse\n",
    "                  covariance of network outputs\n",
    "                - **_Î2** *(float)* -- Covariance regularisation\n",
    "                - **_r** *(float)* -- Regularisation coupling strength\n",
    "        history : tuple\n",
    "            History arrays containing fitting statistics for each iteration\n",
    "                - **detF** *(float(max_iterations, 1) or\n",
    "                  float(max_iterations, 2))* -- Determinant of the Fisher\n",
    "                  information matrix\n",
    "                - **detC** *(float(max_iterations, 1) or\n",
    "                  float(max_iterations, 2))* -- Determinant of the covariance\n",
    "                  of network outputs\n",
    "                - **detinvC** *(float(max_iterations, 1)\n",
    "                  or float(max_iterations, 2))* -- Determinant of the inverse\n",
    "                  covariance of network outputs\n",
    "                - **Î2** *(float(max_iterations, 1) or\n",
    "                  float(max_iterations, 2))* -- Covariance regularisation\n",
    "                - **r** *(float(max_iterations, 1) or\n",
    "                  float(max_iterations, 2))* -- Regularisation coupling\n",
    "                  strength\n",
    "        counter : int\n",
    "            Current iteration to insert a single iteration statistics into the\n",
    "            history\n",
    "        ind : int\n",
    "            Values of either 0 (fitting) or 1 (validation) to separate the\n",
    "            fitting and validation historys\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float(max_iterations, 1) or float(max_iterations, 2):\n",
    "            History of the determinant of the Fisher information matrix\n",
    "        float(max_iterations, 1) or float(max_iterations, 2):\n",
    "            History of the determinant of the covariance of network outputs\n",
    "        float(max_iterations, 1) or float(max_iterations, 2):\n",
    "            History of the determinant of the inverse covariance of network\n",
    "            outputs\n",
    "        float(max_iterations, 1) or float(max_iterations, 2):\n",
    "            History of the covariance regularisation\n",
    "        float(max_iterations, 1) or float(max_iterations, 2):\n",
    "            History of the regularisation coupling strength\n",
    "        \"\"\"\n",
    "        F, C, invC, _Î2, _r = inputs\n",
    "        detF, detC, detinvC, Î2, r = history\n",
    "        detF = detF.at[counter, ind].set(np.linalg.det(F))\n",
    "        detC = detC.at[counter, ind].set(np.linalg.det(C))\n",
    "        detinvC = detinvC.at[counter, ind].set(np.linalg.det(invC))\n",
    "        Î2 = Î2.at[counter, ind].set(_Î2)\n",
    "        r = r.at[counter, ind].set(_r)\n",
    "        return detF, detC, detinvC, Î2, r\n",
    "\n",
    "    def _slogdet(self, matrix):\n",
    "        \"\"\"Combined summed logarithmic determinant\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        matrix : float(n, n)\n",
    "            An n x n matrix to calculate the summed logarithmic determinant of\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float:\n",
    "            The summed logarithmic determinant multiplied by its sign\n",
    "        \"\"\"\n",
    "        lndet = np.linalg.slogdet(matrix)\n",
    "        return lndet[0] * lndet[1]\n",
    "\n",
    "    def _construct_derivatives(self, derivatives):\n",
    "        \"\"\"Builds derivatives of the network outputs wrt model parameters\n",
    "\n",
    "        An empty directive in ``_IMNN``, ``SimulatorIMNN`` and ``GradientIMNN``\n",
    "        but necessary to construct correct shaped derivatives when using\n",
    "        ``NumericalGradientIMNN``.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        derivatives : float(n_d, n_summaries, n_params)\n",
    "            The derivatives of the network ouputs with respect to the model\n",
    "            parameters\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float(n_d, n_summaries, n_params):\n",
    "            The derivatives of the network ouputs with respect to the model\n",
    "            parameters\n",
    "        \"\"\"\n",
    "        return derivatives\n",
    "\n",
    "    def set_F_statistics(self, w=None, key=None, validate=True):\n",
    "        \"\"\"Set necessary attributes for calculating score compressed summaries\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        w : list or None, default=None\n",
    "            The network parameters if wanting to calculate the Fisher\n",
    "            information with a specific set of network parameters\n",
    "        key : int(2,) or None, default=None\n",
    "            A random number generator for generating simulations on-the-fly\n",
    "        validate : bool, default=True\n",
    "            Whether to calculate Fisher information using the validation set\n",
    "        \"\"\"\n",
    "        if validate and ((not self.validate) and (not self.simulate)):\n",
    "            validate = False\n",
    "        if w is not None:\n",
    "            self.w = w\n",
    "        self.F, self.C, self.invC, self.dÎ¼_dÎ¸, self.Î¼, self.F_loss = \\\n",
    "            self._get_F_statistics(key=key, validate=validate)\n",
    "        self.invF = np.linalg.inv(self.F)\n",
    "\n",
    "    def _get_F_statistics(self, w=None, key=None, validate=False):\n",
    "        \"\"\"Calculates the Fisher information and returns all statistics used\n",
    "\n",
    "        First gets the summaries and derivatives and then uses them to\n",
    "        calculate the Fisher information matrix from the outputs and return all\n",
    "        the necessary constituents to calculate the Fisher information (which)\n",
    "        are needed for the score compression or the regularisation of the loss\n",
    "        function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        w : list or None, default=None\n",
    "            The network parameters if wanting to calculate the Fisher\n",
    "            information with a specific set of network parameters\n",
    "        key : int(2,) or None, default=None\n",
    "            A random number generator for generating simulations on-the-fly\n",
    "        validate : bool, default=True\n",
    "            Whether to calculate Fisher information using the validation set\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple:\n",
    "            - **F** *(float(n_params, n_params))* -- Fisher information matrix\n",
    "            - **C** *(float(n_summaries, n_summaries))* -- Covariance of\n",
    "              network outputs\n",
    "            - **invC** *(float(n_summaries, n_summaries))* -- Inverse\n",
    "              covariance of network outputs\n",
    "            - **dÎ¼_dÎ¸** *(float(n_summaries, n_params))* -- The derivative of\n",
    "              the mean of network outputs with respect to model parameters\n",
    "            - **Î¼** *(float(n_summaries,))* -- The mean of the network outputs\n",
    "\n",
    "        \"\"\"\n",
    "        if w is None:\n",
    "            w = self.w\n",
    "        summaries, derivatives = self.get_summaries(\n",
    "            w=w, key=key, validate=validate)\n",
    "        return self._calculate_F_statistics(summaries, derivatives)\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def _calculate_F_statistics(self, summaries, derivatives):\n",
    "        \"\"\"Calculates the Fisher information matrix from network outputs\n",
    "\n",
    "        If the numerical derivative is being calculated then the derivatives\n",
    "        are first constructed. If the mean is to be returned (for use in score\n",
    "        compression), this is calculated and pushed to the results tuple.\n",
    "        Then the covariance of the summaries is taken and inverted and the mean\n",
    "        of the derivative of network summaries with respect to the model\n",
    "        parameters is found and these are used to calculate the Gaussian form\n",
    "        of the Fisher information matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        summaries : float(n_s, n_summaries)\n",
    "            The network outputs\n",
    "        derivatives : float(n_d, n_summaries, n_params)\n",
    "            The derivative of the network outputs wrt the model parameters.\n",
    "            Note that when ``NumericalGradientIMNN`` is being used the shape is\n",
    "            ``float(n_d, 2, n_params, n_summaries)`` which is then constructed\n",
    "            into the the numerical derivative in ``_construct_derivatives``.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple:\n",
    "            - **F** *(float(n_params, n_params))* -- Fisher information matrix\n",
    "            - **C** *(float(n_summaries, n_summaries))* -- Covariance of\n",
    "              network outputs\n",
    "            - **invC** *(float(n_summaries, n_summaries))* -- Inverse\n",
    "              covariance of network outputs\n",
    "            - **dÎ¼_dÎ¸** *(float(n_summaries, n_params))* -- The derivative of\n",
    "              the mean of network outputs with respect to model parameters\n",
    "            - **Î¼** *(float(n_summaries))* -- The mean of the\n",
    "              network outputs\n",
    "        \"\"\"\n",
    "        derivatives = self._construct_derivatives(derivatives)\n",
    "        Î¼ = np.mean(summaries, axis=0)\n",
    "        C = np.cov(summaries, rowvar=False)\n",
    "        if self.n_summaries == 1:\n",
    "            C = np.array([[C]])\n",
    "\n",
    "        invC = np.linalg.inv(C)\n",
    "\n",
    "        if self.no_invC:\n",
    "            invC_loss = np.eye(self.n_summaries)\n",
    "        else:\n",
    "            invC_loss = invC\n",
    "        dÎ¼_dÎ¸ = np.mean(derivatives, axis=0)\n",
    "        F = np.einsum(\"ij,ik,kl->jl\", dÎ¼_dÎ¸, invC, dÎ¼_dÎ¸)\n",
    "\n",
    "        F_loss = np.einsum(\"ij,ik,kl->jl\", dÎ¼_dÎ¸, invC_loss, dÎ¼_dÎ¸)\n",
    "        return (F, C, invC, dÎ¼_dÎ¸, Î¼, F_loss)\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def _get_regularisation_strength(self, Î2, Î», Î±):\n",
    "        \"\"\"Coupling strength of the regularisation (amplified sigmoid)\n",
    "\n",
    "        To dynamically turn off the regularisation when the scale of the\n",
    "        covariance is set to approximately the identity matrix, a smooth\n",
    "        sigmoid conditional on the value of the regularisation is used. The\n",
    "        rate, Î±, is calculated from a closeness condition of the covariance\n",
    "        (and the inverse covariance) to the identity matrix using ``get_Î±``.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Î2 : float\n",
    "            Covariance regularisation\n",
    "        Î» : float\n",
    "            Coupling strength of the regularisation\n",
    "        Î± : float\n",
    "            Calculate rate parameter for regularisation from Ïµ criterion\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float:\n",
    "            Smooth, dynamic regularisation strength\n",
    "        \"\"\"\n",
    "        return Î» * Î2 / (Î2 + np.exp(-Î± * Î2))\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def _get_regularisation(self, C, invC):\n",
    "        \"\"\"Difference of the covariance (and its inverse) from identity\n",
    "\n",
    "        The negative logarithm of the determinant of the Fisher information\n",
    "        matrix needs to be regularised to fix the scale of the network outputs\n",
    "        since any linear rescaling of a sufficient statistic is also a\n",
    "        sufficient statistic. We choose to fix this scale by constraining the\n",
    "        covariance of network outputs as\n",
    "\n",
    "        .. math::\n",
    "            \\\\Lambda_2 = ||\\\\bf{C}-\\\\bf{I}|| + ||\\\\bf{C}^{-1}-\\\\bf{I}||\n",
    "\n",
    "        One benefit of choosing this constraint is that it forces the\n",
    "        covariance to be approximately parameter independent which justifies\n",
    "        choosing the covariance independent Gaussian Fisher information.\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        C : float(n_summaries, n_summaries)\n",
    "            Covariance of the network ouputs\n",
    "        invC : float(n_summaries, n_summaries)\n",
    "            Inverse covariance of the network ouputs\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float:\n",
    "            Regularisation loss terms for the distance of the covariance and\n",
    "            its determinant from the identity matrix\n",
    "        \"\"\"\n",
    "        if self.no_invC:\n",
    "            if self.evidence:\n",
    "                #reg = -(np.log(np.linalg.det(C)) - np.trace(C) + self.n_params)\n",
    "                reg = np.trace(C)\n",
    "            else:\n",
    "                reg = np.linalg.norm(C - np.eye(self.n_summaries))\n",
    "\n",
    "        else:\n",
    "            reg = np.linalg.norm(C - np.eye(self.n_summaries)) + \\\n",
    "                np.linalg.norm(invC - np.eye(self.n_summaries))\n",
    "        return reg\n",
    "\n",
    "    def _get_loss(self, w, Î», Î±, Î³, key=None):\n",
    "        \"\"\"Calculates the loss function and returns auxillary variables\n",
    "\n",
    "        First gets the summaries and derivatives and then uses them to\n",
    "        calculate the loss function. This function is separated to be able to\n",
    "        use ``jax.grad`` directly rather than calculating the derivative of the\n",
    "        summaries as is done with ``_AggregatedIMNN``.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        w : list\n",
    "            The network parameters if wanting to calculate the Fisher\n",
    "            information with a specific set of network parameters\n",
    "        Î» : float\n",
    "            Coupling strength of the regularisation\n",
    "        Î± : float\n",
    "            Calculate rate parameter for regularisation from Ïµ criterion\n",
    "        key : int(2,) or None, default=None\n",
    "            A random number generator for generating simulations on-the-fly\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float:\n",
    "            Value of the regularised loss function\n",
    "        tuple:\n",
    "            Fitting statistics calculated on a single iteration\n",
    "                - **F** *(float(n_params, n_params))* -- Fisher information\n",
    "                  matrix\n",
    "                - **C** *(float(n_summaries, n_summaries))* -- Covariance of\n",
    "                  network outputs\n",
    "                - **invC** *(float(n_summaries, n_summaries))* -- Inverse\n",
    "                  covariance of network outputs\n",
    "                - **Î2** *(float)* -- Covariance regularisation\n",
    "                - **r** *(float)* -- Regularisation coupling strength\n",
    "\n",
    "        \"\"\"\n",
    "        summaries, derivatives = self.get_summaries(w=w, key=key)\n",
    "        return self._calculate_loss(summaries, derivatives, Î», Î±, Î³)\n",
    "\n",
    "    def _calculate_loss(self, summaries, derivatives, Î», Î±, Î³):\n",
    "        \"\"\"Calculates the loss function from network summaries and derivatives\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        summaries : float(n_s, n_summaries)\n",
    "            The network outputs\n",
    "        derivatives : float(n_d, n_summaries, n_params)\n",
    "            The derivative of the network outputs wrt the model parameters.\n",
    "            Note that when ``NumericalGradientIMNN`` is being used the shape is\n",
    "            ``float(n_d, 2, n_params, n_summaries)`` which is then constructed\n",
    "            into the the numerical derivative in ``_construct_derivatives``.\n",
    "        Î» : float\n",
    "            Coupling strength of the regularisation\n",
    "        Î± : float\n",
    "            Calculate rate parameter for regularisation from Ïµ criterion\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float:\n",
    "            Value of the regularised loss function\n",
    "        tuple:\n",
    "            Fitting statistics calculated on a single iteration\n",
    "                - **F** *(float(n_params, n_params))* -- Fisher information\n",
    "                  matrix\n",
    "                - **C** *(float(n_summaries, n_summaries))* -- Covariance of\n",
    "                  network outputs\n",
    "                - **invC** *(float(n_summaries, n_summaries))* -- Inverse\n",
    "                  covariance of network outputs\n",
    "                - **Î2** *(float)* -- Covariance regularisation\n",
    "                - **r** *(float)* -- Regularisation coupling strength\n",
    "\n",
    "        \"\"\"\n",
    "        F, C, invC, dÎ¼_dÎ¸, _, F_loss = self._calculate_F_statistics(\n",
    "            summaries, derivatives)\n",
    "        lndetF = self._slogdet(F_loss)\n",
    "        Î2 = self._get_regularisation(C, invC)\n",
    "        if self.do_reg:\n",
    "            r = self._get_regularisation_strength(Î2, Î», Î±)\n",
    "        else:\n",
    "            r = Î³*0.5\n",
    "        return - lndetF + r * Î2, (F, C, invC, Î2, r)\n",
    "\n",
    "    def get_summaries(self, w=None, key=None, validate=False):\n",
    "        \"\"\"Gets all network outputs and derivatives wrt model parameters\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        w : list or None, default=None\n",
    "            The network parameters if wanting to calculate the Fisher\n",
    "            information with a specific set of network parameters\n",
    "        key : int(2,) or None, default=None\n",
    "            A random number generator for generating simulations on-the-fly\n",
    "        validate : bool, default=False\n",
    "            Whether to get summaries of the validation set\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "        \"\"\"\n",
    "        raise ValueError(\"`get_summaries` not implemented\")\n",
    "\n",
    "    def get_estimate(self, d):\n",
    "        \"\"\"Calculate score compressed parameter estimates from network outputs\n",
    "\n",
    "        Using score compression we can get parameter estimates under the\n",
    "        transformation\n",
    "\n",
    "        .. math::\n",
    "            \\\\hat{\\\\boldsymbol{\\\\theta}}_\\\\alpha=\\\\theta^{\\\\rm{fid}}_\\\\alpha+\n",
    "            \\\\bf{F}^{-1}_{\\\\alpha\\\\beta}\\\\frac{\\\\partial\\\\mu_i}{\\\\partial\n",
    "            \\\\theta_\\\\beta}\\\\bf{C}^{-1}_{ij}(x(\\\\bf{w}, \\\\bf{d})-\\\\mu)_j\n",
    "\n",
    "        where :math:`x_j` is the :math:`j` output of the network with network\n",
    "        parameters :math:`\\\\bf{w}` and input data :math:`\\\\bf{d}`.\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        Assuming that an IMNN has been fit (as in the example in\n",
    "        :py:meth:`imnn.imnn._imnn.IMNN.fit`) then we can obtain a\n",
    "        pseudo-maximum likelihood estimate of some target data (which is\n",
    "        generated with parameter values Î¼=1, Î£=2) using\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            rng, target_key = jax.random.split(rng)\n",
    "            target_data = model_simulator(target_key, np.array([1., 2.]))\n",
    "\n",
    "            imnn.get_estimate(target_data)\n",
    "            >>> DeviceArray([0.1108716, 1.7881424], dtype=float32)\n",
    "\n",
    "        The one standard deviation uncertainty on these parameter estimates\n",
    "        (assuming the fiducial is at the maximum-likelihood estimate - which we\n",
    "        know it isn't here) estimated by the square root of the inverse Fisher\n",
    "        information matrix is\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            np.sqrt(np.diag(imnn.invF))\n",
    "            >>> DeviceArray([0.31980422, 0.47132865], dtype=float32)\n",
    "\n",
    "        Note that we can compare the values estimated by the IMNN to the value\n",
    "        of the mean and the variance of the target data itself, which is what\n",
    "        the IMNN should be summarising\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            np.mean(target_data)\n",
    "            >>> DeviceArray(0.10693721, dtype=float32)\n",
    "\n",
    "            np.var(target_data)\n",
    "            >>> DeviceArray(1.70872, dtype=float32)\n",
    "\n",
    "        Note that batches of data can be summarised at once using\n",
    "        ``get_estimate``. In this example we will draw 10 different values of Î¼\n",
    "        from between :math:`-10 < \\\\mu < 10` and 10 different values of Î£ from\n",
    "        between :math:`0 < \\\\Sigma < 10` and generate a batch of 10 different\n",
    "        input data which we can summarise using the IMNN.\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            rng, mean_keys, var_keys = jax.random.split(rng, num=3)\n",
    "\n",
    "            mean_vals = jax.random.uniform(\n",
    "                mean_keys, minval=-10, maxval=10, shape=(10,))\n",
    "            var_vals = jax.random.uniform(\n",
    "                var_keys, minval=0, maxval=10, shape=(10,))\n",
    "\n",
    "            np.stack([mean_vals, var_vals], -1)\n",
    "            >>> DeviceArray([[ 3.8727236,  1.6727388],\n",
    "                             [-3.1113386,  8.14554  ],\n",
    "                             [ 9.87299  ,  1.4134324],\n",
    "                             [ 4.4837523,  1.5812075],\n",
    "                             [-9.398947 ,  3.5737753],\n",
    "                             [-2.0789695,  9.978279 ],\n",
    "                             [-6.2622285,  6.828809 ],\n",
    "                             [ 4.6470118,  6.0823894],\n",
    "                             [ 5.7369494,  8.856505 ],\n",
    "                             [ 4.248898 ,  5.114669 ]], dtype=float32)\n",
    "\n",
    "            batch_target_keys = np.array(jax.random.split(rng, num=10))\n",
    "\n",
    "            batch_target_data = jax.vmap(model_simulator)(\n",
    "                batch_target_keys, (mean_vals, var_vals))\n",
    "\n",
    "            imnn.get_estimate(batch_target_data)\n",
    "            >>> DeviceArray([[ 4.6041985,  8.344688 ],\n",
    "                             [-3.5172062,  7.7219954],\n",
    "                             [13.229679 , 23.668312 ],\n",
    "                             [ 5.745726 , 10.020965 ],\n",
    "                             [-9.734651 , 21.076218 ],\n",
    "                             [-1.8083427,  6.1901293],\n",
    "                             [-8.626409 , 18.894459 ],\n",
    "                             [ 5.7684307,  9.482665 ],\n",
    "                             [ 6.7861238, 14.128591 ],\n",
    "                             [ 4.900367 ,  9.472563 ]], dtype=float32)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        d : float(None, input_shape)\n",
    "            Input data to be compressed to score compressed parameter estimates\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float(None, n_params):\n",
    "            Score compressed parameter estimates\n",
    "\n",
    "        Methods\n",
    "        -------\n",
    "        single_element:\n",
    "            Returns a single score compressed summary\n",
    "        multiple_elements:\n",
    "            Returns a batch of score compressed summaries\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If the Fisher statistics are not set after running ``fit`` or\n",
    "            ``set_F_statistics``.\n",
    "\n",
    "        Todo\n",
    "        ----\n",
    "        - Do proper checking on input shape (should just be a call to\n",
    "          ``_check_input``)\n",
    "        \"\"\"\n",
    "        @jax.jit\n",
    "        def single_element(d):\n",
    "            \"\"\"Returns a single score compressed summary\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            d : float(input_shape)\n",
    "                Input data to be compressed to score compressed parameter\n",
    "                estimate\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            float(n_params,):\n",
    "                Score compressed parameter estimate\n",
    "            \"\"\"\n",
    "            return self.Î¸_fid + np.einsum(\n",
    "                \"ij,kj,kl,l->i\",\n",
    "                self.invF,\n",
    "                self.dÎ¼_dÎ¸,\n",
    "                self.invC,\n",
    "                self.model(self.w, d) - self.Î¼)\n",
    "\n",
    "        @jax.jit\n",
    "        def multiple_elements(d):\n",
    "            \"\"\"Returns a batch of score compressed summaries\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            d : float(None, input_shape)\n",
    "                Input data to be compressed to score compressed parameter\n",
    "                estimates\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            float(None, n_params):\n",
    "                Score compressed parameter estimates\n",
    "\n",
    "            Methods\n",
    "            -------\n",
    "            fn:\n",
    "                Returns the output of the evaluated model\n",
    "            \"\"\"\n",
    "            def fn(d):\n",
    "                \"\"\"Returns the output of the evaluated model\n",
    "\n",
    "                Parameters\n",
    "                ----------\n",
    "                d : float(input_shape)\n",
    "                    Input data to the neural network\n",
    "\n",
    "                Returns\n",
    "                -------\n",
    "                float(None, n_summaries):\n",
    "                    Neural network output\n",
    "                \"\"\"\n",
    "                return self.model(self.w, d)\n",
    "            return self.Î¸_fid + np.einsum(\n",
    "                \"ij,kj,kl,ml->mi\",\n",
    "                self.invF,\n",
    "                self.dÎ¼_dÎ¸,\n",
    "                self.invC,\n",
    "                jax.vmap(fn)(d) - self.Î¼)\n",
    "\n",
    "        _check_statistics_set(self.invF, self.dÎ¼_dÎ¸, self.invC, self.Î¼)\n",
    "        # check shape: array or graph ?\n",
    "        if self.dummy_input is None:\n",
    "          if len(d.shape) == 1:\n",
    "              return single_element(d)\n",
    "          else:\n",
    "              return multiple_elements(d)\n",
    "        else:\n",
    "            return single_element(d)\n",
    "\n",
    "    def _setup_plot(self, ax=None, expected_detF=None, figsize=(5, 15)):\n",
    "        \"\"\"Builds axes for history plot\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ax : mpl.axes or None, default=None\n",
    "            An axes object of predefined axes to be labelled\n",
    "        expected_detF : float or None, default=None\n",
    "            Value of the expected determinant of the Fisher information to plot\n",
    "            a horizontal line at to check fitting progress\n",
    "        figsize : tuple, default=(5, 15)\n",
    "            The size of the figure to be produced\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mpl.axes:\n",
    "            An axes object of labelled axes\n",
    "        \"\"\"\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(3, 1, sharex=True, figsize=figsize)\n",
    "            plt.subplots_adjust(hspace=0.05)\n",
    "        ax = [x for x in ax] + [ax[2].twinx()]\n",
    "        if expected_detF is not None:\n",
    "            ax[0].axhline(expected_detF, linestyle=\"dashed\", color=\"black\")\n",
    "        ax[0].set_ylabel(r\"$|{\\bf F}|$\")\n",
    "        ax[1].axhline(1, linestyle=\"dashed\", color=\"black\")\n",
    "        ax[1].set_ylabel(r\"$|{\\bf C}|$ and $|{\\bf C}^{-1}|$\")\n",
    "        ax[1].set_yscale(\"log\")\n",
    "        ax[2].set_xlabel(\"Number of iterations\")\n",
    "        ax[2].set_ylabel(r\"$\\Lambda_2$\")\n",
    "        ax[3].set_ylabel(r\"$r$\")\n",
    "        return ax\n",
    "\n",
    "    def plot(self, ax=None, expected_detF=None, colour=\"C0\", figsize=(5, 15),\n",
    "             label=\"\", filename=None, ncol=1):\n",
    "        \"\"\"Plot fitting history\n",
    "\n",
    "        Plots a three panel vertical plot with the determinant of the Fisher\n",
    "        information matrix in the first sublot, the covariance and the inverse\n",
    "        covariance in the second and the regularisation term and the\n",
    "        regularisation coupling strength in the final subplot.\n",
    "\n",
    "        A predefined axes can be passed to fill, and these axes can be\n",
    "        decorated via a call to ``_setup_plot`` (for horizonal plots for\n",
    "        example).\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "\n",
    "        Assuming that an IMNN has been fit (as in the example in\n",
    "        :py:meth:`imnn.imnn._imnn.IMNN.fit`) then we can make a training plot\n",
    "        of the history by simply running\n",
    "\n",
    "        .. code-block::\n",
    "\n",
    "            imnn.fit(expected_detF=50, filename=\"history_plot.png\")\n",
    "\n",
    "        .. image:: /_images/history_plot.png\n",
    "\n",
    "        Note we know the analytic value of the determinant of the Fisher\n",
    "        information for this problem (:math:`|\\\\bf{F}|=50`) so we can add this\n",
    "        line to the plot too, and save the output as a png named\n",
    "        ``history_plot``.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ax : mpl.axes or None, default=None\n",
    "            An axes object of predefined axes to be labelled\n",
    "        expected_detF : float or None, default=None\n",
    "            Value of the expected determinant of the Fisher information to plot\n",
    "            a horizontal line at to check fitting progress\n",
    "        colour : str or rgb/a value or list, default=\"C0\"\n",
    "            Colour to plot the lines\n",
    "        figsize : tuple, default=(5, 15)\n",
    "            The size of the figure to be produced\n",
    "        label : str, default=\"\"\n",
    "            Name to add to description in legend\n",
    "        filename : str or None, default=None\n",
    "            Filename to save plot to\n",
    "        ncol : int, default=1\n",
    "            Number of columns to have in the legend\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mpl.axes:\n",
    "            An axes object of the filled plot\n",
    "        \"\"\"\n",
    "        if ax is None:\n",
    "            ax = self._setup_plot(expected_detF=expected_detF, figsize=figsize)\n",
    "        ax[0].set_xlim(\n",
    "            0, max(self.history[\"detF\"].shape[0] - 1, ax[0].get_xlim()[-1]))\n",
    "        ax[0].plot(self.history[\"detF\"], color=colour,\n",
    "                   label=r\"{} $|F|$ (training)\".format(label))\n",
    "        ax[1].set_xlim(\n",
    "            0, max(self.history[\"detF\"].shape[0] - 1, ax[0].get_xlim()[-1]))\n",
    "        ax[1].plot(self.history[\"detC\"], color=colour,\n",
    "                   label=r\"{} $|C|$ (training)\".format(label))\n",
    "        ax[1].plot(self.history[\"detinvC\"], linestyle=\"dotted\", color=colour,\n",
    "                   label=label + r\" $|C^{-1}|$ (training)\")\n",
    "        ax[3].set_xlim(\n",
    "            0, max(self.history[\"detF\"].shape[0] - 1, ax[0].get_xlim()[-1]))\n",
    "        ax[2].plot(self.history[\"Î2\"], color=colour,\n",
    "                   label=r\"{} $\\Lambda_2$ (training)\".format(label))\n",
    "        ax[3].plot(self.history[\"r\"], color=colour, linestyle=\"dashed\",\n",
    "                   label=r\"{} $r$ (training)\".format(label))\n",
    "        if self.validate:\n",
    "            ax[0].plot(self.history[\"val_detF\"], color=colour,\n",
    "                       label=r\"{} $|F|$ (validation)\".format(label),\n",
    "                       linestyle=\"dotted\")\n",
    "            ax[1].plot(self.history[\"val_detC\"], color=colour,\n",
    "                       label=r\"{} $|C|$ (validation)\".format(label),\n",
    "                       linestyle=\"dotted\")\n",
    "            ax[1].plot(self.history[\"val_detinvC\"],\n",
    "                       color=colour,\n",
    "                       label=label + r\" $|C^{-1}|$ (validation)\",\n",
    "                       linestyle=\"dashdot\")\n",
    "            ax[2].plot(self.history[\"val_Î2\"], color=colour,\n",
    "                       label=r\"{} $\\Lambda_2$ (validation)\".format(label),\n",
    "                       linestyle=\"dotted\")\n",
    "            ax[3].plot(self.history[\"val_r\"], color=colour,\n",
    "                       label=r\"{} $r$ (validation)\".format(label),\n",
    "                       linestyle=\"dashdot\")\n",
    "        h1, l1 = ax[2].get_legend_handles_labels()\n",
    "        h2, l2 = ax[3].get_legend_handles_labels()\n",
    "        ax[0].legend(bbox_to_anchor=(1.0, 1.0), frameon=False, ncol=ncol)\n",
    "        ax[1].legend(frameon=False, bbox_to_anchor=(1.0, 1.0), ncol=ncol * 2)\n",
    "        ax[3].legend(h1 + h2, l1 + l2, bbox_to_anchor=(1.05, 1.0),\n",
    "                     frameon=False, ncol=ncol * 2)\n",
    "\n",
    "        if filename is not None:\n",
    "            plt.savefig(filename, bbox_inches=\"tight\", transparent=True)\n",
    "        return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6916b16f-f37c-419d-9076-32aca1b6e9eb",
   "metadata": {
    "cellView": "form",
    "id": "6916b16f-f37c-419d-9076-32aca1b6e9eb",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Numerical Gradient IMNN <font color='lightgreen'>[run me]</font>\n",
    "import jax.numpy as np\n",
    "# from imnn.imnn._imnn import _IMNN\n",
    "#from imnn.utils.utils import _check_input\n",
    "\n",
    "class myNumericalGradientIMNN(_myIMNN):\n",
    "    \"\"\"Information maximising neural network fit using numerical derivatives\n",
    "\n",
    "    The outline of the fitting procedure is that a set of :math:`i\\\\in[1, n_s]`\n",
    "    simulations :math:`{\\\\bf d}^i` originally generated at fiducial model\n",
    "    parameter :math:`{\\\\bf\\\\theta}^\\\\rm{fid}`, and a set of\n",
    "    :math:`i\\\\in[1, n_d]` simulations,\n",
    "    :math:`\\\\{{\\\\bf d}_{\\\\alpha^-}^i, {\\\\bf d}_{\\\\alpha^+}^i\\\\}`, generated\n",
    "    with the same seed at each :math:`i` generated at\n",
    "    :math:`{\\\\bf\\\\theta}^\\\\rm{fid}` apart from at parameter label\n",
    "    :math:`\\\\alpha` with values\n",
    "\n",
    "    .. math::\n",
    "        \\\\theta_{\\\\alpha^-} = \\\\theta_\\\\alpha^\\\\rm{fid}-\\\\delta\\\\theta_\\\\alpha\n",
    "\n",
    "    and\n",
    "\n",
    "    .. math::\n",
    "        \\\\theta_{\\\\alpha^+} = \\\\theta_\\\\alpha^\\\\rm{fid}+\\\\delta\\\\theta_\\\\alpha\n",
    "\n",
    "    where :math:`\\\\delta\\\\theta_\\\\alpha` is a :math:`n_{params}` length vector\n",
    "    with the :math:`\\\\alpha` element having a value which perturbs the\n",
    "    parameter :math:`\\\\theta^{\\\\rm fid}_\\\\alpha`. This means there are\n",
    "    :math:`2\\\\times n_{params}\\\\times n_d` simulations used to calculate the\n",
    "    numerical derivatives (this is extremely cheap compared to other machine\n",
    "    learning methods). All these simulations are passed through a network\n",
    "    :math:`f_{{\\\\bf w}}({\\\\bf d})` with network parameters :math:`{\\\\bf w}` to\n",
    "    obtain network outputs :math:`{\\\\bf x}^i` and\n",
    "    :math:`\\\\{{\\\\bf x}_{\\\\alpha^-}^i,{\\\\bf x}_{\\\\alpha^+}^i\\\\}`. These\n",
    "    perturbed values are combined to obtain\n",
    "\n",
    "    .. math::\n",
    "        \\\\frac{\\\\partial{{\\\\bf x}^i}}{\\\\partial\\\\theta_\\\\alpha} =\n",
    "        \\\\frac{{\\\\bf x}_{\\\\alpha^+}^i - {\\\\bf x}_{\\\\alpha^-}^i}\n",
    "        {\\\\delta\\\\theta_\\\\alpha}\n",
    "\n",
    "    With :math:`{\\\\bf x}^i` and\n",
    "    :math:`\\\\partial{{\\\\bf x}^i}/\\\\partial\\\\theta_\\\\alpha` the covariance\n",
    "\n",
    "    .. math::\n",
    "        C_{ab} = \\\\frac{1}{n_s-1}\\\\sum_{i=1}^{n_s}(x^i_a-\\\\mu^i_a)\n",
    "        (x^i_b-\\\\mu^i_b)\n",
    "\n",
    "    and the derivative of the mean of the network outputs with respect to the\n",
    "    model parameters\n",
    "\n",
    "    .. math::\n",
    "        \\\\frac{\\\\partial\\\\mu_a}{\\\\partial\\\\theta_\\\\alpha} = \\\\frac{1}{n_d}\n",
    "        \\\\sum_{i=1}^{n_d}\\\\frac{\\\\partial{x^i_a}}{\\\\partial\\\\theta_\\\\alpha}\n",
    "\n",
    "    can be calculated and used form the Fisher information matrix\n",
    "\n",
    "    .. math::\n",
    "        F_{\\\\alpha\\\\beta} = \\\\frac{\\\\partial\\\\mu_a}{\\\\partial\\\\theta_\\\\alpha}\n",
    "        C^{-1}_{ab}\\\\frac{\\\\partial\\\\mu_b}{\\\\partial\\\\theta_\\\\beta}.\n",
    "\n",
    "    The loss function is then defined as\n",
    "\n",
    "    .. math::\n",
    "        \\\\Lambda = -\\\\log|{\\\\bf F}| + r(\\\\Lambda_2) \\\\Lambda_2\n",
    "\n",
    "    Since any linear rescaling of a sufficient statistic is also a sufficient\n",
    "    statistic the negative logarithm of the determinant of the Fisher\n",
    "    information matrix needs to be regularised to fix the scale of the network\n",
    "    outputs. We choose to fix this scale by constraining the covariance of\n",
    "    network outputs as\n",
    "\n",
    "    .. math::\n",
    "        \\\\Lambda_2 = ||{\\\\bf C}-{\\\\bf I}|| + ||{\\\\bf C}^{-1}-{\\\\bf I}||\n",
    "\n",
    "    Choosing this constraint is that it forces the covariance to be\n",
    "    approximately parameter independent which justifies choosing the covariance\n",
    "    independent Gaussian Fisher information as above. To avoid having a dual\n",
    "    optimisation objective, we use a smooth and dynamic regularisation strength\n",
    "    which turns off the regularisation to focus on maximising the Fisher\n",
    "    information when the covariance has set the scale\n",
    "\n",
    "    .. math::\n",
    "        r(\\\\Lambda_2) = \\\\frac{\\\\lambda\\\\Lambda_2}{\\\\Lambda_2-\\\\exp\n",
    "        (-\\\\alpha\\\\Lambda_2)}.\n",
    "\n",
    "    Once the loss function is calculated the automatic gradient is then\n",
    "    calculated and used to update the network parameters via the optimiser\n",
    "    function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Î´Î¸ : float(n_params,)\n",
    "        Size of perturbation to model parameters for the numerical derivative\n",
    "    fiducial : float(n_s, input_shape)\n",
    "        The simulations generated at the fiducial model parameter values used\n",
    "        for calculating the covariance of network outputs (for fitting)\n",
    "    derivative : float(n_d, 2, n_params, input_shape)\n",
    "        The simulations generated at parameter values perturbed from the\n",
    "        fiducial used to calculate the numerical derivative of network outputs\n",
    "        with respect to model parameters (for fitting)\n",
    "    validation_fiducial : float(n_s, input_shape) or None\n",
    "        The simulations generated at the fiducial model parameter values used\n",
    "        for calculating the covariance of network outputs (for validation)\n",
    "    validation_derivative : float(n_d, 2, n_params, input_shape) or None\n",
    "        The simulations generated at parameter values perturbed from the\n",
    "        fiducial used to calculate the numerical derivative of network outputs\n",
    "        with respect to model parameters (for validation)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_s, n_d, n_params, n_summaries, input_shape, Î¸_fid,\n",
    "                 model, optimiser, key_or_state, fiducial, derivative, Î´Î¸,\n",
    "                 validation_fiducial=None, validation_derivative=None, \n",
    "                 dummy_input=None,\n",
    "                 no_invC=False, do_reg=True, evidence=False):\n",
    "        \"\"\"Constructor method\n",
    "\n",
    "        Initialises all IMNN attributes, constructs neural network and its\n",
    "        initial parameter values and creates history dictionary. Also fills the\n",
    "        simulation attributes (and validation if available).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_s : int\n",
    "            Number of simulations used to calculate summary covariance\n",
    "        n_d : int\n",
    "            Number of simulations used to calculate mean of summary derivative\n",
    "        n_params : int\n",
    "            Number of model parameters\n",
    "        n_summaries : int\n",
    "            Number of summaries, i.e. outputs of the network\n",
    "        input_shape : tuple\n",
    "            The shape of a single input to the network\n",
    "        Î¸_fid : float(n_params,)\n",
    "            The value of the fiducial parameter values used to generate inputs\n",
    "        model : tuple, len=2\n",
    "            Tuple containing functions to initialise neural network\n",
    "            ``fn(rng: int(2), input_shape: tuple) -> tuple, list`` and the\n",
    "            neural network as a function of network parameters and inputs\n",
    "            ``fn(w: list, d: float(None, input_shape)) -> float(None, n_summari\n",
    "            es)``.\n",
    "            (Essentibly stax-like, see `jax.experimental.stax <https://jax.read\n",
    "            thedocs.io/en/stable/jax.experimental.stax.html>`_))\n",
    "        optimiser : tuple, len=3\n",
    "            Tuple containing functions to generate the optimiser state\n",
    "            ``fn(x0: list) -> :obj:state``, to update the state from a list of\n",
    "            gradients ``fn(i: int, g: list, state: :obj:state) -> :obj:state``\n",
    "            and to extract network parameters from the state\n",
    "            ``fn(state: :obj:state) -> list``.\n",
    "            (See `jax.experimental.optimizers <https://jax.readthedocs.io/en/st\n",
    "            able/jax.experimental.optimizers.html>`_)\n",
    "        key_or_state : int(2) or :obj:state\n",
    "            Either a stateless random number generator or the state object of\n",
    "            an preinitialised optimiser\n",
    "        fiducial : float(n_s, input_shape)\n",
    "            The simulations generated at the fiducial model parameter values\n",
    "            used for calculating the covariance of network outputs\n",
    "            (for fitting)\n",
    "        derivative : float(n_d, 2, n_params, input_shape)\n",
    "            The simulations generated at parameter values perturbed from the\n",
    "            fiducial used to calculate the numerical derivative of network\n",
    "            outputs with respect to model parameters (for fitting)\n",
    "        Î´Î¸ : float(n_params,)\n",
    "            Size of perturbation to model parameters for the numerical\n",
    "            derivative\n",
    "        validation_fiducial : float(n_s, input_shape) or None, default=None\n",
    "            The simulations generated at the fiducial model parameter values\n",
    "            used for calculating the covariance of network outputs\n",
    "            (for validation)\n",
    "        validation_derivative : float(n_d, 2, n_params, input_shape) or None\n",
    "            The simulations generated at parameter values perturbed from the\n",
    "            fiducial used to calculate the numerical derivative of network\n",
    "            outputs with respect to model parameters (for validation)\n",
    "        dummy_input : jraph.GraphsTuple or jax.numpy.DeviceArray\n",
    "            Either a (padded) graph input or device array. If supplied ignores \n",
    "            `input_shape` parameter\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            n_s=n_s,\n",
    "            n_d=n_d,\n",
    "            n_params=n_params,\n",
    "            n_summaries=n_summaries,\n",
    "            input_shape=input_shape,\n",
    "            Î¸_fid=Î¸_fid,\n",
    "            model=model,\n",
    "            key_or_state=key_or_state,\n",
    "            optimiser=optimiser,\n",
    "            dummy_input=dummy_input,\n",
    "            no_invC=no_invC,\n",
    "            do_reg=do_reg,\n",
    "            evidence=evidence)\n",
    "        self._set_data(Î´Î¸, fiducial, derivative, validation_fiducial,\n",
    "                       validation_derivative)\n",
    "        self.dummy_input = dummy_input\n",
    "\n",
    "    def _set_data(self, Î´Î¸, fiducial, derivative, validation_fiducial,\n",
    "                  validation_derivative):\n",
    "        \"\"\"Checks and sets data attributes with the correct shape\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Î´Î¸ : float(n_params,)\n",
    "            Size of perturbation to model parameters for the numerical\n",
    "            derivative\n",
    "        fiducial : float(n_s, input_shape)\n",
    "            The simulations generated at the fiducial model parameter values\n",
    "            used for calculating the covariance of network outputs\n",
    "            (for fitting)\n",
    "        derivative : float(n_d, input_shape, n_params)\n",
    "            The derivative of the simulations with respect to the model\n",
    "            parameters (for fitting)\n",
    "        validation_fiducial : float(n_s, input_shape) or None, default=None\n",
    "            The simulations generated at the fiducial model parameter values\n",
    "            used for calculating the covariance of network outputs\n",
    "            (for validation). Sets ``validate = True`` attribute if provided\n",
    "        validation_derivative : float(n_d, input_shape, n_params) or None\n",
    "            The derivative of the simulations with respect to the model\n",
    "            parameters (for validation). Sets ``validate = True`` attribute if\n",
    "            provided\n",
    "        \"\"\"\n",
    "        self.Î´Î¸ = np.expand_dims(\n",
    "            _check_input(Î´Î¸, (self.n_params,), \"Î´Î¸\"), (0, 1))\n",
    "        if self.dummy_input is None:\n",
    "          self.fiducial = _check_input(\n",
    "              fiducial, (self.n_s,) + self.input_shape, \"fiducial\")\n",
    "          self.derivative = _check_input(\n",
    "              derivative, (self.n_d, 2, self.n_params) + self.input_shape,\n",
    "              \"derivative\")\n",
    "          if ((validation_fiducial is not None)\n",
    "                  and (validation_derivative is not None)):\n",
    "              self.validation_fiducial = _check_input(\n",
    "                  validation_fiducial, (self.n_s,) + self.input_shape,\n",
    "                  \"validation_fiducial\")\n",
    "              self.validation_derivative = _check_input(\n",
    "                  validation_derivative,\n",
    "                  (self.n_d, 2, self.n_params) + self.input_shape,\n",
    "                  \"validation_derivative\")\n",
    "              self.validate = True\n",
    "        else:\n",
    "          self.fiducial = fiducial\n",
    "          self.derivative = derivative\n",
    "\n",
    "          if ((validation_fiducial is not None)\n",
    "                  and (validation_derivative is not None)):\n",
    "              self.validation_fiducial = validation_fiducial\n",
    "              self.validation_derivative =  validation_derivative\n",
    "              self.validate = True\n",
    "\n",
    "\n",
    "    def _collect_input(self, key, validate=False):\n",
    "        \"\"\" Returns validation or fitting sets\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        key : None or int(2,)\n",
    "            Random number generators not used in this case\n",
    "        validate : bool\n",
    "            Whether to return the set for validation or for fitting\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float(n_s, input_shape):\n",
    "            The fiducial simulations for fitting or validation\n",
    "        float(n_d, 2, n_params, input_shape):\n",
    "            The derivative simulations for fitting or validation\n",
    "        \"\"\"\n",
    "        if validate:\n",
    "            fiducial = self.validation_fiducial\n",
    "            derivative = self.validation_derivative\n",
    "        else:\n",
    "            fiducial = self.fiducial\n",
    "            derivative = self.derivative\n",
    "        return fiducial, derivative\n",
    "\n",
    "    def get_summaries(self, w, key=None, validate=False):\n",
    "        \"\"\"Gets all network outputs and derivatives wrt model parameters\n",
    "\n",
    "        Selects either the fitting or validation sets and passes them through\n",
    "        the network to get the network outputs. For the numerical derivatives,\n",
    "        the array is first flattened along the batch axis before being passed\n",
    "        through the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        w : list or None, default=None\n",
    "            The network parameters if wanting to calculate the Fisher\n",
    "            information with a specific set of network parameters\n",
    "        key : int(2,) or None, default=None\n",
    "            A random number generator for generating simulations on-the-fly\n",
    "        validate : bool, default=False\n",
    "            Whether to get summaries of the validation set\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float(n_s, n_summaries):\n",
    "            The set of all network outputs used to calculate the covariance\n",
    "        float(n_d, 2, n_params, n_summaries):\n",
    "            The outputs of the network of simulations made at perturbed\n",
    "            parameter values to construct the derivative of the network outputs\n",
    "            with respect to the model parameters numerically\n",
    "        \"\"\"\n",
    "        d, d_mp = self._collect_input(key, validate=validate)\n",
    "        \n",
    "        \n",
    "        if self.dummy_input is None:\n",
    "          x = self.model(w, d)\n",
    "          x_mp = np.reshape(\n",
    "              self.model(\n",
    "                  w, d_mp.reshape(\n",
    "                      (self.n_d * 2 * self.n_params,) + self.input_shape)),\n",
    "              (self.n_d, 2, self.n_params, self.n_summaries))\n",
    "        else:\n",
    "          # if operating on graph data, we need to vmap the implicit\n",
    "          # batch dimension\n",
    "          _model = lambda d: self.model(w, d)\n",
    "          x = jax.vmap(_model)(d)\n",
    "          x_mp = np.reshape(\n",
    "              jax.vmap(_model)(d_mp),\n",
    "              (self.n_d, 2, self.n_params, self.n_summaries))\n",
    "\n",
    "        return x, x_mp\n",
    "\n",
    "    def _construct_derivatives(self, x_mp):\n",
    "        \"\"\"Builds derivatives of the network outputs wrt model parameters\n",
    "\n",
    "        The network outputs from the simulations generated with model parameter\n",
    "        values above and below the fiducial are subtracted from each other and\n",
    "        divided by the perturbation size in each model parameter value. The\n",
    "        axes are swapped such that the derivatives with respect to parameters\n",
    "        are in the last axis.\n",
    "\n",
    "        .. math::\n",
    "            \\\\frac{\\\\partial{\\\\bf x}^i}{\\\\partial\\\\theta_\\\\alpha} =\n",
    "            \\\\frac{{\\\\bf x}^i_{\\\\alpha^+}-{\\\\bf x}^i_{\\\\alpha^+}}{\n",
    "            \\\\delta\\\\theta_\\\\alpha}\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        derivatives : float(n_d, 2, n_params, n_summaries)\n",
    "            The outputs of the network of simulations made at perturbed\n",
    "            parameter values to construct the derivative of the network outputs\n",
    "            with respect to the model parameters numerically\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float(n_d, n_summaries, n_params):\n",
    "            The numerical derivatives of the network ouputs with respect to the\n",
    "            model parameters\n",
    "        \"\"\"\n",
    "        return np.swapaxes(x_mp[:, 1] - x_mp[:, 0], 1, 2) / self.Î´Î¸\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ca028099-d989-4df0-8579-205491b64512",
   "metadata": {
    "cellView": "form",
    "id": "ca028099-d989-4df0-8579-205491b64512",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Noise Numerical Gradient IMNN <font color='lightgreen'>[run me]</font>\n",
    "\n",
    "import optax\n",
    "from functools import partial\n",
    "from imnn.utils import value_and_jacrev\n",
    "from imnn.utils.utils import _check_simulator\n",
    "\n",
    "\n",
    "class NoiseNumericalGradientIMNN(_myIMNN):\n",
    "    \"\"\"Information maximising neural network fit with simulations on-the-fly\n",
    "\n",
    "    Defines the function to get simulations and compress them using an XLA\n",
    "    compilable simulator.\n",
    "\n",
    "    The outline of the fitting procedure is that a set of :math:`i\\\\in[1, n_s]`\n",
    "    random number generators are generated and used to generate a set of\n",
    "    :math:`n_s` simulations,\n",
    "    :math:`{\\\\bf d}^i={\\\\rm simulator}({\\\\rm seed}^i, \\\\theta^\\\\rm{fid})` at\n",
    "    the fiducial model parameters, :math:`\\\\theta^\\\\rm{fid}`, and these are\n",
    "    passed direrectly through a network :math:`f_{{\\\\bf w}}({\\\\bf d})` with\n",
    "    network parameters :math:`{\\\\bf w}` to obtain network outputs\n",
    "    :math:`{\\\\bf x}^i` and autodifferentiation is used to get the derivative of\n",
    "    :math:`n_d` of these outputs with respect to the physical model parameters,\n",
    "    :math:`\\\\partial{{\\\\bf x}^i}/\\\\partial\\\\theta_\\\\alpha`, where\n",
    "    :math:`\\\\alpha` labels the physical parameter. With :math:`{\\\\bf x}^i` and\n",
    "    :math:`\\\\partial{{\\\\bf x}^i}/\\\\partial\\\\theta_\\\\alpha` the covariance\n",
    "\n",
    "    .. math::\n",
    "        C_{ab} = \\\\frac{1}{n_s-1}\\\\sum_{i=1}^{n_s}(x^i_a-\\\\mu^i_a)\n",
    "        (x^i_b-\\\\mu^i_b)\n",
    "\n",
    "    and the derivative of the mean of the network outputs with respect to the\n",
    "    model parameters\n",
    "\n",
    "    .. math::\n",
    "        \\\\frac{\\\\partial\\\\mu_a}{\\\\partial\\\\theta_\\\\alpha} = \\\\frac{1}{n_d}\n",
    "        \\\\sum_{i=1}^{n_d}\\\\frac{\\\\partial{x^i_a}}{\\\\partial\\\\theta_\\\\alpha}\n",
    "\n",
    "    can be calculated and used form the Fisher information matrix\n",
    "\n",
    "    .. math::\n",
    "        F_{\\\\alpha\\\\beta} = \\\\frac{\\\\partial\\\\mu_a}{\\\\partial\\\\theta_\\\\alpha}\n",
    "        C^{-1}_{ab}\\\\frac{\\\\partial\\\\mu_b}{\\\\partial\\\\theta_\\\\beta}.\n",
    "\n",
    "    The loss function is then defined as\n",
    "\n",
    "    .. math::\n",
    "        \\\\Lambda = -\\\\log|{\\\\bf F}| + r(\\\\Lambda_2) \\\\Lambda_2\n",
    "\n",
    "    Since any linear rescaling of a sufficient statistic is also a sufficient\n",
    "    statistic the negative logarithm of the determinant of the Fisher\n",
    "    information matrix needs to be regularised to fix the scale of the network\n",
    "    outputs. We choose to fix this scale by constraining the covariance of\n",
    "    network outputs as\n",
    "\n",
    "    .. math::\n",
    "        \\\\Lambda_2 = ||{\\\\bf C}-{\\\\bf I}|| + ||{\\\\bf C}^{-1}-{\\\\bf I}||\n",
    "\n",
    "    Choosing this constraint is that it forces the covariance to be\n",
    "    approximately parameter independent which justifies choosing the covariance\n",
    "    independent Gaussian Fisher information as above. To avoid having a dual\n",
    "    optimisation objective, we use a smooth and dynamic regularisation strength\n",
    "    which turns off the regularisation to focus on maximising the Fisher\n",
    "    information when the covariance has set the scale\n",
    "\n",
    "    .. math::\n",
    "        r(\\\\Lambda_2) = \\\\frac{\\\\lambda\\\\Lambda_2}{\\\\Lambda_2-\\\\exp\n",
    "        (-\\\\alpha\\\\Lambda_2)}.\n",
    "\n",
    "    Once the loss function is calculated the automatic gradient is then\n",
    "    calculated and used to update the network parameters via the optimiser\n",
    "    function.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    simulator:\n",
    "        A function for generating a simulation on-the-fly (XLA compilable)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_s, n_d, n_params, n_summaries, input_shape, Î¸_fid, Î´Î¸,\n",
    "                 model, optimiser, key_or_state, dummy_input, noise_simulator, \n",
    "                 fiducial, derivative,\n",
    "                 validation_fiducial=None, validation_derivative=None, \n",
    "                 no_invC=False, do_reg=True,\n",
    "                 evidence=False):\n",
    "        \"\"\"Constructor method\n",
    "\n",
    "        Initialises all IMNN attributes, constructs neural network and its\n",
    "        initial parameter values and creates history dictionary. Also checks\n",
    "        validity of simulator and sets the ``simulate`` attribute to ``True``.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_s : int\n",
    "            Number of simulations used to calculate summary covariance\n",
    "        n_d : int\n",
    "            Number of simulations used to calculate mean of summary derivative\n",
    "        n_params : int\n",
    "            Number of model parameters\n",
    "        n_summaries : int\n",
    "            Number of summaries, i.e. outputs of the network\n",
    "        input_shape : tuple\n",
    "            The shape of a single input to the network\n",
    "        Î¸_fid : float(n_params,)\n",
    "            The value of the fiducial parameter values used to generate inputs\n",
    "        model : tuple, len=2\n",
    "            Tuple containing functions to initialise neural network\n",
    "            ``fn(rng: int(2), input_shape: tuple) -> tuple, list`` and the\n",
    "            neural network as a function of network parameters and inputs\n",
    "            ``fn(w: list, d: float(None, input_shape)) -> float(None, n_summari\n",
    "            es)``.\n",
    "            (Essentibly stax-like, see `jax.experimental.stax <https://jax.read\n",
    "            thedocs.io/en/stable/jax.experimental.stax.html>`_))\n",
    "        optimiser : tuple, len=3\n",
    "            Tuple containing functions to generate the optimiser state\n",
    "            ``fn(x0: list) -> :obj:state``, to update the state from a list of\n",
    "            gradients ``fn(i: int, g: list, state: :obj:state) -> :obj:state``\n",
    "            and to extract network parameters from the state\n",
    "            ``fn(state: :obj:state) -> list``.\n",
    "            (See `jax.experimental.optimizers <https://jax.readthedocs.io/en/st\n",
    "            able/jax.experimental.optimizers.html>`_)\n",
    "        key_or_state : int(2) or :obj:state\n",
    "            Either a stateless random number generator or the state object of\n",
    "            an preinitialised optimiser\n",
    "        simulator : fn\n",
    "            A function that generates a single simulation from a random number\n",
    "            generator and a tuple (or array) of parameter values at which to\n",
    "            generate the simulations. For the purposes of use in LFI/ABC\n",
    "            afterwards it is also useful for the simulator to be able to\n",
    "            broadcast to a batch of simulations on the zeroth axis\n",
    "            ``fn(int(2,), float([None], n_params)) ->\n",
    "            float([None], input_shape)``\n",
    "        dummy_input : jraph.GraphsTuple or 'jax.numpy.DeviceArray'\n",
    "            Either a (padded) graph input or device array. If supplied ignores \n",
    "            `input_shape` parameter\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            n_s=n_s,\n",
    "            n_d=n_d,\n",
    "            n_params=n_params,\n",
    "            n_summaries=n_summaries,\n",
    "            input_shape=input_shape,\n",
    "            Î¸_fid=Î¸_fid,\n",
    "            model=model,\n",
    "            optimiser=optimiser,\n",
    "            key_or_state=key_or_state,\n",
    "            dummy_input=dummy_input,\n",
    "            no_invC=no_invC,\n",
    "            do_reg=do_reg,\n",
    "            evidence=evidence)\n",
    "        \n",
    "        self.simulator = _check_simulator(noise_simulator)\n",
    "        #self.simulate = True\n",
    "        self.dummy_input = dummy_input\n",
    "        self.Î¸_der = (Î¸_fid + np.einsum(\"i,jk->ijk\", np.array([-1., 1.]), \n",
    "                                        np.diag(Î´Î¸) / 2.)).reshape((-1, 2))\n",
    "        self.Î´Î¸ = np.expand_dims(\n",
    "            _check_input(Î´Î¸, (self.n_params,), \"Î´Î¸\"), (0, 1))\n",
    "        \n",
    "        # NUMERICAL GRADIENT SETUP\n",
    "        self._set_data(Î´Î¸, fiducial, derivative, validation_fiducial,\n",
    "                       validation_derivative)\n",
    "\n",
    "\n",
    "    def _set_data(self, Î´Î¸, fiducial, derivative, validation_fiducial,\n",
    "                  validation_derivative):\n",
    "        \"\"\"Checks and sets data attributes with the correct shape\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Î´Î¸ : float(n_params,)\n",
    "            Size of perturbation to model parameters for the numerical\n",
    "            derivative\n",
    "        fiducial : float(n_s, input_shape)\n",
    "            The simulations generated at the fiducial model parameter values\n",
    "            used for calculating the covariance of network outputs\n",
    "            (for fitting)\n",
    "        derivative : float(n_d, input_shape, n_params)\n",
    "            The derivative of the simulations with respect to the model\n",
    "            parameters (for fitting)\n",
    "        validation_fiducial : float(n_s, input_shape) or None, default=None\n",
    "            The simulations generated at the fiducial model parameter values\n",
    "            used for calculating the covariance of network outputs\n",
    "            (for validation). Sets ``validate = True`` attribute if provided\n",
    "        validation_derivative : float(n_d, input_shape, n_params) or None\n",
    "            The derivative of the simulations with respect to the model\n",
    "            parameters (for validation). Sets ``validate = True`` attribute if\n",
    "            provided\n",
    "        \"\"\"\n",
    "        self.Î´Î¸ = np.expand_dims(\n",
    "            _check_input(Î´Î¸, (self.n_params,), \"Î´Î¸\"), (0, 1))\n",
    "        if self.dummy_input is None:\n",
    "          self.fiducial = _check_input(\n",
    "              fiducial, (self.n_s,) + self.input_shape, \"fiducial\")\n",
    "          self.derivative = _check_input(\n",
    "              derivative, (self.n_d, 2, self.n_params) + self.input_shape,\n",
    "              \"derivative\")\n",
    "          if ((validation_fiducial is not None)\n",
    "                  and (validation_derivative is not None)):\n",
    "              self.validation_fiducial = _check_input(\n",
    "                  validation_fiducial, (self.n_s,) + self.input_shape,\n",
    "                  \"validation_fiducial\")\n",
    "              self.validation_derivative = _check_input(\n",
    "                  validation_derivative,\n",
    "                  (self.n_d, 2, self.n_params) + self.input_shape,\n",
    "                  \"validation_derivative\")\n",
    "              self.validate = True\n",
    "        else:\n",
    "          self.fiducial = fiducial\n",
    "          self.derivative = derivative\n",
    "\n",
    "          if ((validation_fiducial is not None)\n",
    "                  and (validation_derivative is not None)):\n",
    "              self.validation_fiducial = validation_fiducial\n",
    "              self.validation_derivative =  validation_derivative\n",
    "              self.validate = True\n",
    "\n",
    "\n",
    "    def _collect_input(self, key, validate=False):\n",
    "        \"\"\" Returns validation or fitting sets\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        key : None or int(2,)\n",
    "            Random number generators not used in this case\n",
    "        validate : bool\n",
    "            Whether to return the set for validation or for fitting\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float(n_s, input_shape):\n",
    "            The fiducial simulations for fitting or validation\n",
    "        float(n_d, 2, n_params, input_shape):\n",
    "            The derivative simulations for fitting or validation\n",
    "        \"\"\"\n",
    "        if validate:\n",
    "            fiducial = self.validation_fiducial\n",
    "            derivative = self.validation_derivative\n",
    "        else:\n",
    "            fiducial = self.fiducial\n",
    "            derivative = self.derivative\n",
    "            \n",
    "        # add noise to data and make cuts\n",
    "        keys = np.array(jax.random.split(key, num=self.n_s))\n",
    "        fiducial = jax.vmap(self.simulator)(keys, fiducial)\n",
    "        \n",
    "        derivative = jax.vmap(self.simulator)(\n",
    "                np.repeat(keys[:self.n_d], self.Î¸_der.shape[0], axis=0),\n",
    "                derivative)\n",
    "        \n",
    "        return fiducial, derivative\n",
    "\n",
    "    def _get_fitting_keys(self, rng):\n",
    "        \"\"\"Generates random numbers for simulation\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        rng : int(2,)\n",
    "            A random number generator\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int(2,), int(2,), int(2,)\n",
    "            A new random number generator and random number generators for\n",
    "            fitting (and validation)\n",
    "        \"\"\"\n",
    "        return jax.random.split(rng, num=3)\n",
    "\n",
    "    def get_summaries(self, w, key=None, validate=False):\n",
    "        \"\"\"Gets all network outputs and derivatives wrt model parameters\n",
    "\n",
    "        Selects either the fitting or validation sets and passes them through\n",
    "        the network to get the network outputs. For the numerical derivatives,\n",
    "        the array is first flattened along the batch axis before being passed\n",
    "        through the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        w : list or None, default=None\n",
    "            The network parameters if wanting to calculate the Fisher\n",
    "            information with a specific set of network parameters\n",
    "        key : int(2,) or None, default=None\n",
    "            A random number generator for generating simulations on-the-fly\n",
    "        validate : bool, default=False\n",
    "            Whether to get summaries of the validation set\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float(n_s, n_summaries):\n",
    "            The set of all network outputs used to calculate the covariance\n",
    "        float(n_d, 2, n_params, n_summaries):\n",
    "            The outputs of the network of simulations made at perturbed\n",
    "            parameter values to construct the derivative of the network outputs\n",
    "            with respect to the model parameters numerically\n",
    "        \"\"\"\n",
    "        d, d_mp = self._collect_input(key, validate=validate)\n",
    "        \n",
    "        \n",
    "        if self.dummy_input is None:\n",
    "          x = self.model(w, d)\n",
    "          x_mp = np.reshape(\n",
    "              self.model(\n",
    "                  w, d_mp.reshape(\n",
    "                      (self.n_d * 2 * self.n_params,) + self.input_shape)),\n",
    "              (self.n_d, 2, self.n_params, self.n_summaries))\n",
    "        else:\n",
    "          # if operating on graph data, we need to vmap the implicit\n",
    "          # batch dimension\n",
    "          _model = lambda d: self.model(w, d)\n",
    "          x = jax.vmap(_model)(d)\n",
    "          x_mp = np.reshape(\n",
    "              jax.vmap(_model)(d_mp),\n",
    "              (self.n_d, 2, self.n_params, self.n_summaries))\n",
    "\n",
    "        return x, x_mp\n",
    "\n",
    "    def _construct_derivatives(self, x_mp):\n",
    "        \"\"\"Builds derivatives of the network outputs wrt model parameters\n",
    "\n",
    "        The network outputs from the simulations generated with model parameter\n",
    "        values above and below the fiducial are subtracted from each other and\n",
    "        divided by the perturbation size in each model parameter value. The\n",
    "        axes are swapped such that the derivatives with respect to parameters\n",
    "        are in the last axis.\n",
    "\n",
    "        .. math::\n",
    "            \\\\frac{\\\\partial{\\\\bf x}^i}{\\\\partial\\\\theta_\\\\alpha} =\n",
    "            \\\\frac{{\\\\bf x}^i_{\\\\alpha^+}-{\\\\bf x}^i_{\\\\alpha^+}}{\n",
    "            \\\\delta\\\\theta_\\\\alpha}\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        derivatives : float(n_d, 2, n_params, n_summaries)\n",
    "            The outputs of the network of simulations made at perturbed\n",
    "            parameter values to construct the derivative of the network outputs\n",
    "            with respect to the model parameters numerically\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float(n_d, n_summaries, n_params):\n",
    "            The numerical derivatives of the network ouputs with respect to the\n",
    "            model parameters\n",
    "        \"\"\"\n",
    "        return np.swapaxes(x_mp[:, 1] - x_mp[:, 0], 1, 2) / self.Î´Î¸\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c512b97f-95e7-42a6-8eb5-cd7ce979b0f0",
   "metadata": {
    "id": "c512b97f-95e7-42a6-8eb5-cd7ce979b0f0"
   },
   "outputs": [],
   "source": [
    "model_key = jax.random.PRNGKey(33)\n",
    "fit_key = jax.random.PRNGKey(44)\n",
    "np = jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d36436c3-3933-4fa7-8238-8054ad30b33f",
   "metadata": {
    "id": "d36436c3-3933-4fa7-8238-8054ad30b33f"
   },
   "outputs": [],
   "source": [
    "optimiser = optax.adam(learning_rate=1e-4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2112e756-9c23-4226-b6c1-70ab3f12f0a3",
   "metadata": {
    "id": "2112e756-9c23-4226-b6c1-70ab3f12f0a3"
   },
   "outputs": [],
   "source": [
    "rng, key = jax.random.split(key)\n",
    "\n",
    "IMNN = NoiseNumericalGradientIMNN(\n",
    "    n_s=n_s, n_d=n_d, n_params=n_params, n_summaries=n_summaries,\n",
    "    input_shape=(10,), Î¸_fid=Î¸_fid, Î´Î¸=Î´Î¸, model=model,\n",
    "    optimiser=optimiser, key_or_state=np.array(model_key),\n",
    "    noise_simulator=jax.jit(lambda rng, g: noise_simulator(\n",
    "            rng, g)),\n",
    "    fiducial=noisefree_fid, \n",
    "    derivative=noisefree_derv,\n",
    "    validation_fiducial=noisefree_val_fid,\n",
    "    validation_derivative=noisefree_val_derv, \n",
    "    dummy_input=graph,  # dummy graph input\n",
    "    no_invC=False,\n",
    "    do_reg=True,\n",
    "    evidence=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77185d1e-1f09-4a3d-b446-7663b39f046e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Î´Î¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9m3CZjjWb_Wn",
   "metadata": {
    "id": "9m3CZjjWb_Wn"
   },
   "source": [
    "# fitting the gIMNN \n",
    "-- don't worry, this will only take a minute, we're just fitting the universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "52f601b9-d10c-4d97-b4c4-08074001eb63",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52f601b9-d10c-4d97-b4c4-08074001eb63",
    "outputId": "02714da7-e24b-45e5-b27e-7de57742cb37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitted IMNN Fisher : [[2.6419747e-04 6.6774934e-05]\n",
      " [6.6774926e-05 2.2169806e-05]]\n",
      "CPU times: user 16min 59s, sys: 3.47 s, total: 17min 2s\n",
      "Wall time: 16min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "key,rng = jax.random.split(fit_key)\n",
    "IMNN.fit(10.0, 0.1, Î³=100000., rng=np.array(key), patience=1000, best=True)\n",
    "gc.collect()\n",
    "print('fitted IMNN Fisher :', IMNN.F)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BvlCI4HeNPW_",
   "metadata": {
    "id": "BvlCI4HeNPW_"
   },
   "source": [
    "Let's plot the training trajectory. For reference, we'll plot the information computed from the two-point correlation function from the *same catalogue* at $M_{\\rm cut}=1.5\\times 10^{15} M_{odot}$ as a dashed line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "H4ZEGVfGNqRk",
   "metadata": {
    "id": "H4ZEGVfGNqRk"
   },
   "outputs": [],
   "source": [
    "F_2PCF = np.array(([[ 2275.48905284, -2304.95181558],\n",
    "       [-2304.95181558,  2976.81677819]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9c3c2698-d3b1-4aaa-b07e-b05e9882b81a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 877
    },
    "id": "9c3c2698-d3b1-4aaa-b07e-b05e9882b81a",
    "outputId": "b5206457-e7f7-4c93-d0cc-7f22e7d20a98"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x1080 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = IMNN.plot(label='gIMNN with A_noise=%.2f'%(simulator_args['noise_scale']), \n",
    "               expected_detF=None)\n",
    "ax[0].set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lZocQuBUNkjy",
   "metadata": {
    "id": "lZocQuBUNkjy"
   },
   "source": [
    "to load a pre-trained IMNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BmWY9D-ovaO_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BmWY9D-ovaO_",
    "outputId": "dfdf7431-2042-4f01-81d7-318c69b93095"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 2980.8682,  6240.8496],\n",
       "             [ 6240.8496, 18846.783 ]], dtype=float32)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uncomment below to load weights\n",
    "# IMNN_w = load_obj('/content/cosmicGraphs/tutorial/model/IMNN_w_tutorial.pkl')\n",
    "# start_key = np.load('/content/cosmicGraphs/tutorial/model/model_key.npy')\n",
    "# IMNN.set_F_statistics(IMNN_w, start_key)\n",
    "# IMNN.F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "J0JeMNSeL19_",
   "metadata": {
    "id": "J0JeMNSeL19_"
   },
   "source": [
    "# Plot Fisher Contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7ae63ce9-98c5-4b47-adb1-d5ee74d2c446",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ae63ce9-98c5-4b47-adb1-d5ee74d2c446",
    "outputId": "f5438469-1797-48fc-e547-8f546b05ed23"
   },
   "outputs": [],
   "source": [
    "from tensorflow_probability.substrates import jax as tfp\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "\n",
    "\n",
    "lo = [-300, -550]\n",
    "hi = [-300, 550]\n",
    "\n",
    "prior = tfp.distributions.Blockwise(\n",
    "    [tfp.distributions.Uniform(low=low, high=high)\n",
    "     for low, high in zip(lo, hi)])\n",
    "prior.low = np.array(lo)\n",
    "prior.high = np.array(hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a7b1465d-eb91-413d-93c3-92103b0f1e69",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 678
    },
    "id": "a7b1465d-eb91-413d-93c3-92103b0f1e69",
    "outputId": "9125d22a-7315-4871-f75f-cad7cc318142"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data80/makinen/venvs/testjax/lib/python3.7/site-packages/imnn/lfi/lfi.py:202: UserWarning: Attempting to set identical left == right == -300.0 results in singular transformations; automatically expanding.\n",
      "  ranges[column][-1])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnMAAAJTCAYAAABjHJA4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAA4EUlEQVR4nO3dd5RV1cGG8WcD0qUoiNLEjqCAMIgFY0GjKIIYjWDDaDSJJTaiUVP4YkyzxZYYrGAjtoi9RY2JikoREAVFgwoSBMUCirT9/XEuOiKdO3Punnl+a901955zy+s46/q6z9lnhxgjkiRJSlONvANIkiRp3VnmJEmSEmaZkyRJSphlTpIkKWGWOUmSpITVyjtAnpo1axbbtWuXdwxJJWrMmDFzYozN884hSatSrctcu3btGD16dN4xJJWoEMI7eWeQpNXxMKskSVLCLHOSJEkJs8xJkiQlzDInSZKUMMucJElSwixzkiRJCbPMSZIkJcwyJ0mSlDDLnKRK8cwzz9CnT5/VPu+4447j7rvvBmCvvfaibdu2xBi/2n/IIYfQsGFDAKZNm0YIgauuuuqr/aeeeio333zzV+/VqlUrvvzySwDmzJmDq75IqmoscxLAxLthSGP48K28kyRh8eLFlfZZTZo04bnnngPg448/ZubMmd/Yv8kmm3DFFVewcOHCFb6+Zs2a3HjjjRWeU5LyYpmTAF66Lvv59tP55igBF154Idtttx09e/Zk4MCBXHLJJUA2SnbGGWdQVlbGFVdcwQMPPECPHj3Yaaed2HfffZk1axYAQ4YM4ZhjjmHXXXdlm2224brrrvvqvefNm8dhhx1G+/btOeqoo74x4rYyAwYMYMSIEQDce++9HHrood/Y37x5c3r16sWwYcNW+PozzjiDyy+/vFILqCRVpmq9Nqv0lRYd4b1RsAblotI88nP438TivuemO0LvP6x098svv8w999zD+PHjWbRoEV27dqVbt25f7V+4cOFX6xnPnTuXUaNGEULg+uuv509/+hOXXnopABMmTGDUqFHMnz+fnXbaiYMOOgiAcePGMWnSJFq2bMnuu+/Oc889R8+ePVcZuVevXpx44oksWbKEESNGMHToUC688MJvPOfcc8+ld+/eHH/88d96fdu2benZsye33HILBx988Jr9niQpIZY5SV957rnn6NevH3Xr1qVu3brfKj9HHHHEV/enT5/OEUccwcyZM1m4cCFbbLHFV/v69etHvXr1qFevHnvvvTcvvfQSTZo0Yeedd6Z169YAdOnShWnTpq22zNWsWZOePXsyYsQIvvjiixWe87blllvSo0cPbr/99hW+x3nnnUe/fv2+KpWSVJVY5qRStYoRtLw0aNDgq/unnXYaZ511Fn379uWZZ55hyJAhX+0LIXzjdcse16lT56ttNWvWXONDnwMGDKB///7f+IzlnX/++Rx22GHsueee39q3zTbb0KVLF+688841+jxJSonnzEn6yu67784DDzzAggULmDdvHg8++OBKn/vJJ5/QqlUrgG+drzZy5EgWLFjAhx9+yDPPPEP37t3XK9cee+zBeeedx8CBA1f6nPbt29OhQwceeOCBFe6/4IILvjr/T5KqEsucpK90796dvn370qlTJ3r37s2OO+5I48aNV/jcIUOGcPjhh9OtWzeaNWv2jX2dOnVi7733ZpddduGXv/wlLVu2XK9cIQQGDx78rc9Z3gUXXMD06dNXuK9jx4507dp1vXJIUikKazKbrKoqKyuLy07mVjX34Fkw+gY48BLY+cS80+Rq3rx5NGzYkM8//5zvfOc7DB06dK1K0JAhQ2jYsCGDBw+uwJSVI4QwJsZYlncOSVoVz5mT9A0nnXQSr732GgsWLGDQoEGOZklSibPMSfqGlc0IXVOrmqQgSSo+z5mTJElKmGVOkiQpYZY5SZKkhFnmJEmSEmaZkyRJSphlTpIkKWGWOUmSpIRZ5iRJkhJmmZMkSUqYZU6SJClhljlJkqSEWeYkSZISZpmTJElKmGVOkiQpYZY5SZKkhFnmJEmSEmaZkyRJSphlTpIkKWGWOUmSpIRZ5iRJkhJmmZMkSUqYZU6SJClhljlJkqSEWeYkSZISZpmTJElKmGVOkiQpYZY5SZKkhFnmJEmSEmaZkyRJSphlTpIkKWGWOUmSpIRZ5iRJkhJmmZMkSUqYZU6SJClhljlJkqSEWeYkSZISZpmTJElKmGVOkiQpYZY5SZKkhFnmJEmSEmaZkyRJSphlTpIkKWGWOUmSpIRZ5iRJkhJmmZMkSUqYZU6SJClhljlJkqSEWeYkSZISZpmTJElKmGVOkiQpYZY5SZKkhFnmJEmSEmaZkyRJSphlTpIkKWGWOUmSpIRZ5iRJkhJmmZMkSUqYZU6SJClhljlJkqSEWeYkSZISZpmTJElKmGVOkiQpYZY5SZKkhFnmJEmSEmaZkyRJSphlTpIkKWGWOUmSpIRZ5iRJkhJmmZMkSUqYZU6SJClhljlJkqSEWeYkSZISZpmTJElKmGVOkiQpYZY5SZKkhFnmJEmSEmaZkyRJSphlTpIkKWGWOUmSpIRZ5iRJkhJmmZMkSUqYZU6SJClhljlJkqSEWeYkSZISZpmTJElKmGVOkiQpYZY5SZKkhFnmJEmSEmaZkyRJSphlTpIkKWGWOUmSpIRZ5iRJkhJmmZMkSUqYZU6SJClhljlJkqSEWeYkSZISZpmTJElKmGVOkiQpYZY5SZKkhFnmJEmSEmaZkyRJSphlTpIkKWGWOUmSpIRZ5iRJkhJmmZMkSUqYZU6SJClhljlJkqSEWeYkSZISZpmTJElKmGVOkiQpYZY5SZKkhFnmJEmSEmaZkyRJSphlTpIkKWGWOUmSpIRZ5iRJkhJmmZMkSUqYZU6SJClhljlJkqSEWeYkSZISZpmTJElKmGVOkiQpYZY5SZKkhFnmJEmSEmaZkyRJSphlTpIkKWGWOUmSpIRZ5iRJkhJmmZMkSUqYZU6SJClhljlJkqSEWeYkSZISZpmTJElKmGVOkiQpYZY5SZKkhFnmJEmSEmaZkyRJSphlTpIkKWGWOUmSpIRZ5iRJkhJmmZMkSUqYZU6SJClhljlJkqSEWeYkSZISZpmTJElKmGVOkiQpYZY5SZKkhFnmJEmSEmaZkyRJSphlTpIkKWGWOUmSpIRZ5iRJkhJmmZMkSUqYZU6SJClhljlJkqSEWeYkSZISZpmTJElKmGVOkiQpYZY5SZKkhFnmJEmSEmaZkyRJSphlTpIkKWGWOUmSpIRZ5iRJkhJmmZMkSUqYZU6SJClhljlJkqSEWeYkSZISZpmTJElKmGVOkiQpYZY5SZKkhFnmJEmSEmaZkyRJSphlTpIkKWGWOUmSpIRZ5iRJkhJmmZMkSUqYZU6SJClhljlJkqSEWeYkSZISZpmTJElKmGVOkiQpYZY5SZKkhFnmJEmSEmaZkyRJSphlTpIkKWGWOUmSpIRZ5iRJkhJmmZMkSUqYZU6SJClhljlJkqSEWeYkSZISZpmTJElKmGVOkiQpYZY5SZKkhFnmJEmSEmaZkyRJSphlTpIkKWGWOUmSpIRZ5iRJkhJmmZMkSUqYZU6SJClhljlJkqSEWeYkSZISZpmTJElKmGVOkiQpYZY5SZKkhFnmJEmSEmaZkyRJSphlTpIkKWGWOUmSpIRZ5iRJkhJmmZMkSUqYZU6SJClhljlJkqSEWeYkSZISZpmTJElKmGVOkiQpYZY5SZKkhFnmJEmSEmaZkyRJSphlTpIkKWGWOUmSpIRZ5iRJkhJmmZMkSUqYZU6SJClhljkJoM3O2c9NOuSbQ5KktVQr7wBSSeg8ADbfHZq0yTuJJElrxZE5aRmLnCQpQZY5SZKkhFnmJEmSEmaZkyRJSphlTpIkKWGWOUmSpIRZ5iRJkhJmmZMkSUqYZU6SJClhljlJkqSEhRhj3hlyE0KYDbyTdw5JJWvzGGPzvENI0qpU6zInSZKUOg+zSpIkJcwyJ0mSlDDLnCRJUsIsc5IkSQmzzEmSJCXMMidJkpQwy5wkSVLCLHOSJEkJs8xJkiQlzDInSZKUMMucJElSwixzkiRJCbPMSZIkJcwyJ0mSlDDLnCRJUsIsc5IkSQmzzEmSJCXMMidJkpQwy5wkSVLCLHOSJEkJs8xJkiQlzDInSZKUMMucJElSwixzkiRJCauVd4A8NWvWLLZr1y7vGJJK1JgxY+bEGJtXxmet1ffRR2/Bgk8h1IB6TaB+M6jdoCLjSSoBK/tOqtZlrl27dowePTrvGJJKVAjhncr6rLX+Ppo1CUbfBONHwML3ocUOUPYD6HQE1Nmw4oJKys3KvpM8zCpJKWrREQ66BM6eDAdfkY3SPXQ2XLId3P9TeH9c3gklVZJqPTInScmr0xC6HQddB8GMsTD6RphwJ4wdBpt1zvbteLijdVIV5sicJFUFIUDrbnDINTB4Chx4CSxdAg+e+fVo3YyxEGPeSSUVmSNzklTV1G0MO58I3X8IM8bAmJtg4l3ZaN2mnb4eravbKO+kkorAkTlJqqpCgNZl0O+a7Ny6gy7NRuYeOgsu3Q5GnpqVPUfrpKQ5MidJ1UHdxtlIXdkJ2eHWMTfBq/fAuFtg0x3LjdY1zjuppLXkyJwkVSfLzq3rdzWcPaUwWkc2E/bS9tlo3XRH66SUODInSdVV3UZfj9a9Pza7bt2y0boWO0K3QdDp+47WSSXOkTlJqu5CgFblR+sugwA8PDgbrbvvFHj3RUfrpBLlyJwk6Wt1G0H3E6Ds+Gy0bszNMPEeeOVWaLYd7HQ0dB4IDStllTNJa8CROUnSty0bret7VXbdur5XZUXviV/CZe3h70fDG49n17KTlCtH5iRJq1ZnQ+h6bHb7YHJ2Tt34O+D1B2DDltDlyGzEbqMt8k4qVUuOzEmS1twm7WH/i+CsyfD94dkasf+5DK7sAjf3gQl3waIv8k4pVSuOzEmS1l6t2tChX3b7ZDq8cns2YnfvD7PZrzt+H7oek60PK6lCWeYkSeuncWvY8xzYYzBMexbG3gJjh8PL12XLh3U9FnY8DOo1zTupVCVZ5iRJxVGjBmy5V3b7/COYeDeMG55d4uTxX8D2fbPRus17Zs+VVBSWOUlS8dXfCHqclN3efyU7BDvhLph4JzRtl02Y6HIUNGqZd1Ipef6vkSSpYrXski0bNngK9B8KjdvAU7+FyzvCbYfDa/fD4oV5p5SS5cicJKlybFAPOh+R3T58C165LZs4cecxUL8ZdB6QnV/XfLu8k0pJcWROklT5Nt4Kev0KzngVjrwT2u4CL14L1+wMN3w3m0Tx5by8U0pJcGROkpSfmrVg2/2z27wPsosRj70F7j8VHv05dOyfjda17p6tSiHpWyxzkqTS0HAT2P102O2n8N6LWal79Z5s8kTz9rDTMdmh2AbN8k4qlRQPs0qSSksI2WHXQ66BwW/AwVdmS4o9fgFc2h7+fgy8+YTrwkoFjsxJkkpXnQ2h26Ds9sHr2WjdhBHw+v3QqNXX68I2bZd3Uik3jsxJktKwyfZwwO+ydWEPH5Y9fvYSuKIzDOubXaR40YK8U0qVzpE5SVJaatWGjodkt/Lrwt5zAtRtAp2+n51ft1mnnINKlcMyJ0lK14rWhR0zDF4aCpt1zmbC7nAY1GuSd1KpwljmJEnp+9a6sHdlxe6hs+GxC6BDv2y0rl1PL3GiKscyJ0mqWupvBD1+BDufBDNfyUrdxLthwt+h6RbQ9RjofCQ02izvpFJROAFCklQ1hQAtd4I+l8HZk7N1YRu1gn/+Bi7vALcfAa8/CEsW5Z1UWi8lPTIXQqgJjAZmxBj7hBC2AEYAGwNjgGNijAtDCHWA4UA34EPgiBjjtJxiS5JKTe3631wXdtyt2cSJNx6FBptAl4HZYdhm2+SdVFprpT4ydzrwernHfwQujzFuDcwFTihsPwGYW9h+eeF5kiR928Zbwb6/hjMnwcAR2VJhz18NV5fBjQfAuNtg4fy8U0prrGTLXAihNXAQcH3hcQD2Ae4uPGUYcEjhfr/CYwr7exWeL0nSitWsBdv1hoG3w1mvw77/B/Nnw8iT4ZLt4IHTYfoYiDHvpNIqlfJh1j8D5wAbFh5vDHwcY1xceDwdaFW43wp4DyDGuDiE8Enh+XOWf9MQwknASQBt27atqOySpJRs2AJ6npGtDfvuC9mkifF/hzE3wyYdskOwnY6ABhvnnVT6lpIcmQsh9AE+iDGOKfZ7xxiHxhjLYoxlzZs3L/bbS5JSFgJsvhv0/ysMngJ9LodadeGx8+Cy9nDXcTD1n7B0ad5Jpa+U6sjc7kDfEMKBQF2gEXAF0CSEUKswOtcamFF4/gygDTA9hFALaEw2EUKSpHVTtzGUHZ/dZk36el3YSf+Axm2gy1Gw01HQxKM8yldJjszFGM+LMbaOMbYDBgBPxRiPAp4GDis8bRAwsnD//sJjCvufitGTHCRJRdKiI/T+A5w9BQ67ETbeGv71R/hzJ7ilP7x6Lyz+Mu+UqqZKdWRuZc4FRoQQfguMA24obL8BuCWEMBX4iKwASpJUXLXqwA7fy25z34FXbstmv979A6i3EXQekJ1f16JD3klVjYTqPIBVVlYWR48enXcMSSUqhDAmxlhWGZ/l91HCli6Bt5+GscNh8sOwdBG06gZdB2Wlr07DvBOqiljZd1JqI3OSJJWWGjVh632z2/w52bJhY4fDAz+Fx86HHQ+HbsdByy55J1UVZZmTJKlYGjSDXU+BXU6G917KLm0y/g4YcxNs1jkrdTscBnUb5Z1UVUhJToCQJClpIUDbHtklTs6eAgdeAksWw4NnwqXtYeSpXpBYRePInCRJFaleE9j5ROj+Q5gxJhute/UeGHcLtNgRug2CTt/PLoUirQNH5iRJqgwhQOsy6Hd1Nlp30GXZtocHZ8uH3Vc4NOtondaSI3OSJFW2uo2g+wnZ7f1x2WjdxLuzS5003z47t67zEVCvad5JlQBH5iRJylPLneDgK7LRuoOvhA3qwaPnZufW3XsSvPO8o3VaJUfmJEkqBXUaZufPdRsEMyfA2GEw4c7sUifNtsuWFes8IDsHTyrHkTlJkkrNZp3goEvh7MnQ75qs6D16Lly2fTYT9v1xeSdUCXFkTpKkUlW7Aex0dHZ7/xUYfUN2bt24W6Bl1+ycu46HQu36eSdVjhyZkyQpBS27QN+r4KzXofefYOF8GHkKXNYeHj0P5ryZd0LlxDInSVJK6jWBHj+CU16EQQ/CVvvAS0Ph6jIY1hdeGwlLFuWdUpXIw6ySJKUoBNhij+z22SwYNxzGDIM7j4WGm2YTKboOgsat8k6qCubInCRJqduwBXznZ3D6eBg4AjbdEf71J/jzjjDiKHjrKS9vUoU5MidJUlVRoyZs1zu7zZ0Go2/KJktMfjC7vMnOJ0LngdnsWFUZjsxJklQVNW0H+/0fnPkaHHJtNuP14cHZ5U0e+Tl8+FbeCVUkljlJkqqyDepCl4Fw4tNwwpOw7f7w8vVwVVe49TB48wlYujTvlFoPljlJkqqDEKBNd/je9XDmq7DXefC/CXDbYdlM2FHXwoJP806pdWCZkySputlwU9jr53DGq3Do9VB/o69XmHhoMMx+I++EWgtOgJAkqbqqVRs6HZ7dZozNrlc3dhi8fB1suTfsegps1QtqOPZTyvy3I0mSoFVX6H9tNmFi71/A7MnZIdi/7JJdv27RF3kn1EpY5iRJ0tcaNoc9fwanT4D+Q7PRuwd+CpfvAE//HubNzjuhlmOZkyRJ31arNnQ+An7072zZsNbd4V9/gMs7wshT4YPX806oAs+ZkyRJK1d+2bA5b8Kov8Ard2QXI9563+y8ui33zp6nXDgyJ0mS1kyzbaDP5XDmJNjnF/C/iXBLf/jrbjDuVli8MO+E1ZJlTpIkrZ0GG2drwZ4xEQ75KxBg5ClwRWd4/mr48rO8E1YrljlJkrRuatWBLkfCT56Do++BjbeCxy/IJks8dRHMn5N3wmrBMidJktZPCNn5c8c9CD/8J7TrCc/+KSt1D/8M5r6Td8IqzTInSZKKp3UZDLgNTnkJdvgejL4JrtwJ7j0JZk3KO12VZJmTJEnF13w7OOQaOH087PITeP3BbKLE7QNgxpi801UpljlJklRxGreC/S+CM1+Fvc6Hd1+A6/aBWw+D917KO12VYJmTJEkVr/5GsNe52QzYXr/ORudu2A+GHwLvvJB3uqRZ5iRJUuWp2wj2OCsrdftdCLNehZsOgJv7wH//DTHmnTA5ljlJklT56jSE3X+arQG7/+9hzhswrA/cdCC8/Yylbi1Y5iRJUn5q14ddT84mSvS+GOZOg+H9YNjB8O6LeadLgmVOkiTlb4N60OMkOP0V6P0nmD0Zbvwu3PZ9mDkh73QlzTInSZJKR6060ONH2Uhdr1/Dey/C3/aAu46DOW/mna4kWeYkSVLpqd0gmyhx+vhsHdg3n4Brdob7ToaP38s7XUkpyTIXQmgTQng6hPBaCGFSCOH0wvaNQghPhBDeLPxsWtgeQghXhhCmhhAmhBC65vtPIEmSiqJeE9jnF4WLD58ME++Gq7rBE7+GBZ/kna4klGSZAxYDZ8cYOwC7AKeEEDoAPwf+GWPcBvhn4TFAb2Cbwu0k4K+VH1mSJFWYBs2yiw+fNgY69ofnroArusCLf4PFC/NOl6uSLHMxxpkxxrGF+58BrwOtgH7AsMLThgGHFO73A4bHzCigSQhhs8pNLUmSKlyTNnDo3+BH/4JNd4BHzoG/9IDXRlbby5mUZJkrL4TQDtgJeBFoEWOcWdj1P6BF4X4roPwB9OmFbZIkqSrarDMcez8ceRfUrAN3Hgs37l8t130t6TIXQmgI3AOcEWP8tPy+GGME1rqChxBOCiGMDiGMnj17dpGSSpKkShcCbPtd+PF/4OArs2vUXbcPjDwF5lWf/8aXbJkLIWxAVuRuizHeW9g8a9nh08LPDwrbZwBtyr28dWHbt8QYh8YYy2KMZc2bN6+Y8JIkqfLUrAXdBsGpo2G302D8iGySxKi/wpLFeaercCVZ5kIIAbgBeD3GeFm5XfcDgwr3BwEjy20/tjCrdRfgk3KHYyVJUnVQtxF897fwkxegdTd49OdwbU/477N5J6tQJVnmgN2BY4B9QgivFG4HAn8A9gshvAnsW3gM8DDwNjAVuA44OYfMkiSpFDTfFo6+F464DRbNz5YGu+s4+Ox/eSerELXyDrAiMcb/AGElu3ut4PkROKVCQ0mSpHSEANv3ga17wXNXwr8vhalPwX5DoOtxUKNUx7PWXtX5J5EkSVreBvVgr3Ph5Bdgs07w4JlwU2/4YHLeyYrGMidJkqq+jbeCQQ9Av7/AnCnZuXRP/w4Wf5l3svVmmZMkSdVDCLDTUdms14794V9/zC5l8r+JeSdbL5Y5SZJUvTRoBt+7Dgb+HebPhqF7w78uTvYyJpY5SZJUPW13AJw8Cjr0had/Czfsm+S5dJY5SZJUfdXfCA67EQ6/Gea+A0P3hNE3JbXOq2VOkiSpY/9slG7z3eDBM7Lr0n3xcc6h1oxlTpIkCWDDFnDUPbDv/8HkB+Fve8B7L+edarUsc5IkScvUqAE9z4AfPJo9vumAbI3XEj7sapmTJElaXpvu8KN/wzb7Z2u83vcTWPRF3qlWyDInSZK0IvWawBG3wl7nw/g7spUjPpmed6pvscxJkiStTI0a2XJgA+6AOVNh6F7w3kt5p/oGy5wkSdLqtD8QTnwK6mwIww6G1x/MO9FXLHOSJElrovm2cMIT0GIHuPMYeOm6vBMBljlJkqQ116AZDHogmxjx8GB48v9yn+lqmZMkSVobtetnEyO6HQf/uQweOTfXQlcrt0+WJElKVc1a0OfPsEEDGHUNLF0MB16STZioZJY5SZKkdREC7H8R1KgJz1+ZbTvo0mx7JbLMSZIkrasQYL/fZPefvzI7p27v8ys1gmVOkiRpfSwrdF98BP/6I9TfGHr8qNI+3jInSZK0vkKAPlfA53OzCRFN28G2+1fKRzubVZIkqRhq1oLvXQ+bdYJ7fgizp1TKx1rmJEmSiqV2fTjiNqhVB+4YCAs+rfCPtMxJkiQVU5M28P3hMHcaPPyzCv84y5wkSVKxbb4b7HkOTBgBE++u0I+yzEmSJFWEPQZD653hwbPgs1kV9jGWOUmSpIpQsxb0vxYWfwFP/LLCPsYyJ0mSVFE23gp2Px0m/B2m/adCPsIyJ0mSVJF6ngWN28ATv4IYi/72ljlJkqSKVLs+fOdnMGMMTH2y6G9vmZMkSaponQdCk7bZcl9FZpmTJEmqaLVqwy6nwPSX4X8Ti/rWljlJkqTK0On7ULMOjBlW1Le1zEmSJFWG+hvB9gfDq3fDksVFe1vLnCRJUmVpfyB8MRfeH1u0t7TMSZIkVZYt94ZQA958omhvuUZlLoRg6ZMkSVpf9TeCTXfMJkIUSa1V7Qwh7AScBywOIWwGXA9MjDFOKFoCSZKk6qTFDkW93tzqRtxOAQbGGI8E9gN+AhyzsieHEAYXLdk6CCEcEEKYEkKYGkL4eZ5ZJEmSVmiT7WHeLPj8o6K83erK3GcxxiUAMcbFwJgY489W8fwti5JqHYQQagLXAL2BDsDAEEKHvPJIkiStUMNNs5+ff1iUt1vlYVagZghhf+BloIzlyl8I4cDyD4G2RUm1bnYGpsYY3wYIIYwA+gGv5ZhJkiTpm+o1yX5+Mbcob7e6Mnc28EPgYLJSdNZy+5sDy1aMDcCdRUm1bloB75V7PB3okVMWSZKkFdugXvZz0RdFebvVHWY9J8b4V2ByjPEvwJDl9rcDtijcNi88LmkhhJNCCKNDCKNnz56ddxxJklTdLCtxG9QvytutbmSuVeHnsnPPNlpu/4jCz7rAT4EmRUm1bmYAbco9bl3Y9g0xxqHAUICysrK4/H5JkqQKtXB+9nPZCN16Wt3I3OaF8+LaFn6WL0vEGKeQnat2HnBjjPF7RUm1bl4GtgkhbBFCqA0MAO7PMY8kSdK3fVoYa2rUsihvt7oydyfZeXF7FH4+VH5nCGE0sDdwC9B4uQkRlaow2/ZU4DHgdeDOGOOkvPJIkiSt0IdToV7T7ALCRbDKw6wxxmEAIYQzY4zDQghjgL+We8pVhZ/NipJmPcUYHwYezjuHJEnSSs2aBM22Ldrbre6cuWWeDCG8AGwaQjgeGA+8uqzsSZIkaQ0snA8zxsBupxXtLdeozMUYB4cQtgKeJpu52hfoGEJYSFbqjihaIkmSpKrqnRdg6WLY4jtFe8s1HZkjxvhWCGHfGOMby7aFEBoCOxQtjSRJUlX22n1QuyG02aVob7nGZQ6gfJErPJ4HjCpaGkmSpKpq0Rcw6T7ocAjULs415mD1s1klSZJUDJPug4WfQefinp1mmZMkSapoS5fCc3+GTTrA5j2L+taWOUmSpIr2xiMwezL0PBNqFLd+WeYkSZIq0pLF8M/fQNMtoOOhRX/7tZoAIUmSpLU0+oZsVG7A7VCz+NXLkTlJkqSK8tksePp3sOVesF3FrHpqmZMkSaoIMcIDp2eXJOl9MYRQIR/jYVZJkqSKMO7WbOLD/r+H5sVbi3V5jsxJkiQV2wevwyPnQrs9oMePK/SjLHOSJEnFtOATGHEU1GkIh15X9EuRLM/DrJIkScWydAncexJ8/A4MehAabVbhH2mZkyRJKoYY4eGfwRuPwoGXwOa7VsrHephVkiSpGP59aXZNud3PgJ1PrLSPtcxJkiStr5eug6cuhE5HQK9fV+pHW+YkSZLWx4tD4eHBsN1B0PfqCp/wsDzLnCRJ0rp6cSg88jNo3wcOvxlq1a70CJY5SZKktRUjPHfl10XusJtyKXLgbFZJkqS1s3QJPHY+vHgtdOwP/YfmVuTAMidJkrTmFn2RXUfu9fthl1Pgu7+t9HPklmeZkyRJWhPz58Dfj4Z3X4D9fwe7npJ3IsAyJ0mStHrvv5IVufmz4bAbYYfv5Z3oK5Y5SZKkVZlwJ9x/GtRvBsc/Ci13yjvRN1jmJEmSVmTJInhyCLxwNWy+Oxw+DBo2zzvVt1jmJEmSljf3HbjnBJj+Mux8UnaOXM0N8k61QpY5SZKk8l4bCSNPA2LJnR+3IpY5SZIkyC478tj5MPpGaNk1K3IbbZF3qtWyzEmSJM0YA//4Mcx5A3b7Kezzy1wvBLw2LHOSJKn6WrwQnv0T/Psy2HBTOPpe2LpX3qnWimVOkiRVT/97NRuNmzUROh8JB/we6jXJO9Vas8xJkqTqZfGX2Ujcvy+Fek1hwB3Q/sC8U60zy5wkSao+pv0HHjgDPnwTdjwcDvgjNNg471TrxTInSZKqvs8/gsd/Ca/cCk3bJXlu3MpY5iRJUtUVY7Yc12Pnw4KPoeeZ8J1zoHb9vJMVjWVOkiRVTbMmwSPnwrR/Q+vucPAV0KJj3qmKrkbeAZYXQrg4hDA5hDAhhPCPEEKTcvvOCyFMDSFMCSHsX277AYVtU0MIP88luCRJKg2ffwQPDYZre8KsV+Ggy+D4x6tkkYMSLHPAE8AOMcZOwBvAeQAhhA7AAKAjcADwlxBCzRBCTeAaoDfQARhYeK4kSapOliyGl66Dq7rC6Bug+w/htLHQ/QSoUYqVpzhK7jBrjPHxcg9HAYcV7vcDRsQYvwT+G0KYCuxc2Dc1xvg2QAhhROG5r1VSZEmSlLdp/8kOqc56FdrtAb3/WGVH4pZXcmVuOccDfy/cb0VW7paZXtgG8N5y23tUfDRJkpS72W/Ak0NgykPQuA0cPgw69IMQ8k5WaXIpcyGEJ4FNV7DrghjjyMJzLgAWA7cV+bNPAk4CaNu2bTHfWpIkVZbPZsEzv4exw2GD+rD3L2DXU6rULNU1lUuZizHuu6r9IYTjgD5ArxhjLGyeAbQp97TWhW2sYvuKPnsoMBSgrKwsrux5kiSpBH35GTx/NTx/FSz5Mjsvbs9zoEGzvJPlpuQOs4YQDgDOAfaMMX5ebtf9wO0hhMuAlsA2wEtAALYJIWxBVuIGAEdWbmpJklShlizKRuGe+QPM/wA6HAK9fgUbb5V3styVXJkDrgbqAE+E7Hj3qBjjj2OMk0IId5JNbFgMnBJjXAIQQjgVeAyoCdwYY5yUT3RJklRUS5dkF/391x9g7jRouxsMuB3adM87WckouTIXY9x6FfsuAi5awfaHgYcrMpckSapES5fCa//IRuLmvAGb7ggDR8C2B1SryQ1rouTKnCRJqsZihMkPZZMbZr0KzbeH7w+H9gdX6WvFrQ/LnCRJyl+MMPVJePoieH8cbLQVHHo97HAo1KiZd7qSZpmTJEn5iRHe+if860/w3ovQpC30+wt0OgJqWlPWhL8lSZJU+ZYuhTcegWcvzkbiGrWCPpdDl6OhVu280yXFMidJkirP0iXw2n3w7KXwwSRo2g76XgWdBlji1pFlTpIkVbwli2DiXfDvS+HDqdBsW+g/FHb4nodT15O/PUmSVHEWfwmv3Ab/uRw+fhda7Jitn7p9X2enFollTpIkFd8XH8OYm2DUX2HeLGjVDXpfDNvu73XiiswyJ0mSiueTGTDqLzBmGCz8DLbcC/pfC1vubYmrIJY5SZK0/ma9Bs9fBRPvzC430rE/7P5T2Kxz3smqPMucJElaNzHCO8/Bc1fAm4/DBvWh+w9hl5Oh6eZ5p6s2LHOSJGntLF0Ckx/MStyMMVC/Gex9QVbk6m+Ud7pqxzInSZLWzJefwbjb4MVrYe5/oekWcNBl0OVI2KBe3umqLcucJElatbnT4KXrYOxw+PJTaNMD9h0C2x/suqklwDInSZK+LUZ494VsZurkhyDUgA6HZOfDte6WdzqVY5mTJElfW7wQJv0jK3EzX4F6TWH3M7Lz4Rq3yjudVsAyJ0mSYP4cGH0TvHxddpHfZttmC993GgC16+edTqtgmZMkqTqbOR5eGgoT74bFC2CrXtDvL7DVPi63lQjLnCRJ1c3ihfDayKzETX8puz5c5wHQ4yewSfu802ktWeYkSaouPpmRrZc65maYPxs22hL2/312aZF6TfJOp3VkmZMkqSqLEab9JxuFm/wQxKXZYvc7nwhbeii1KrDMSZJUFX05DyaMgJeuh9mvZ7NSdz0Fup8ATdvlnU5FZJmTJKkqmf0GvHw9jL8ju8DvZp2h3zWww/dcpaGKssxJkpS6JYvhjUezy4q8/QzUrA0d+0P3E6F1GYSQd0JVIMucJEmp+mRGtsTW2OHw2fvQqBXs80voOggaNs87nSqJZU6SpJQsXQJvPQWjb8xG42KErXvBgRfDtgdATf/TXt34b1ySpBR8NgvG3QJjhsEn70KD5tkyW90GOaGhmrPMSZJUqpYuhWnPZqNwkx+CpYthi+/Ad38D2x0EtWrnnVAlwDInSVKpmf8hvHJbdoHfj97OLivS48fQ7QfQbOu806nEWOYkSSoFMcI7z2ejcK/fD0sWQttdYa/zYPu+sEHdvBOqRFnmJEnK0xdzYfwIGH0TzJkCdRpnI3BlP4BNts87nRJgmZMkqbItW2Jr7PBswfslX0Krsuzivh0Phdr1806ohFjmJEmqLJ/NgvG3ZyXuo7ezUbiux0DXY7OVGqR1YJmTJKkiLV0CU/8JY4dl14Vbuhg23x32PDc7F85ROK0ny5wkSRVh7jsw7tZsVuqnM6B+M9jl5GwUrtk2eadTFWKZkySpWBYvhCkPZYdR33o627b1vnDAH7LVGbwunCqAZU6SpPU1e0pW4MbfAZ9/CI3bwF4/hy5HQZM2eadTFWeZkyRpXSycD5Puy0rce6OgRi1of1B2GHXLvaFGzbwTqpqwzEmStDbeH5cVuIl3w5efwsbbwH4XQueB0LB53ulUDZV0mQshnA1cAjSPMc4JIQTgCuBA4HPguBjj2MJzBwG/KLz0tzHGYXlkliRVQV98DBPvykrc/yZArXrQ8ZBsFK7trhBC3glVjZVsmQshtAG+C7xbbnNvYJvCrQfwV6BHCGEj4NdAGRCBMSGE+2OMcys3tSSpyogR3n0BxgyD1+6DxQtg005w0KWww2FQr0neCSWghMsccDlwDjCy3LZ+wPAYYwRGhRCahBA2A/YCnogxfgQQQngCOAC4o3IjS5KS99msbCLDuFvhwzehTqNsIkPXY6Fll7zTSd9SkmUuhNAPmBFjHB++OXTdCniv3OPphW0r276i9z4JOAmgbdu2RUwtSUrWksXw5uMw7hZ44zGIS7LDp3ucDR36eWFflbTcylwI4Ulg0xXsugA4n+wQa9HFGIcCQwHKyspiRXyGJCkRc97MCtz4ETBvFjRsAbudBjsdA822zjudtEZyK3Mxxn1XtD2EsCOwBbBsVK41MDaEsDMwAyh/wZ7WhW0zyA61lt/+TNFDS5LS9+W8bHH7cbdk58SFmtkFfbseA1vvBzVL8qCVtFIl9xcbY5wIbLLscQhhGlBWmM16P3BqCGEE2QSIT2KMM0MIjwG/CyE0Lbzsu8B5lRxdklSqYoTpo2HccHj1Xlg4r3BJkd9ApwGwYYu8E0rrrOTK3Go8THZZkqlklyb5AUCM8aMQwoXAy4Xn/WbZZAhJUjU2bzZMGJFNZpg9GTZoAB37Z6NwbXp4SRFVCSVf5mKM7crdj8ApK3nejcCNlRRLklSqliyGt/6ZXRPujUdh6WJovTP0vSorcnU2zDuhVFQlX+YkSVojH72djcC9cjt8NhMaNIddfgJdjoZN2uedTqowljlJUroWfg6v3w9jb4F3/gOhRjaJ4cCLs0kNNTfIO6FU4SxzkqS0xAjvj80K3Kv3ZOujbrQl9PpVtj5qo5Z5J5QqlWVOkpSG+R/CxDuzEvfBpK/XR93paNh8dyczqNqyzEmSStfSJfD201mBm/IwLFkILbtCn8thh+9B3cZ5J5RyZ5mTJJWeudNg3G3ZZIZPp0O9jaD7D7NRuBYd804nlRTLnCSpNCxaAK8/kK3M8N9/AQG27gX7/xa2OxBq1ck7oVSSLHOSpHzNHJ8dRp14Jyz4BJq0hb0vgC5HQuPWeaeTSp5lTpJU+b6YCxPuypbX+t9EqFkHOvTNFrhvtwfUqJF3QikZljlJUuVYujQ7fDruFnj9QVjyJWzWGQ68BHY8DOo1Xf17SPoWy5wkqWJ9/B68cls2oeGTd6FuE+g2KBuF26xT3umk5FnmJEnFt3ghTHkoWx/1raeBCFvuBfv+Gtr3gQ3q5p1QqjIsc5Kk4pn9BowdBuPvgM8/hEatYc9zoMtR0HTzvNNJVZJlTpK0fhZ+Dq/dl43CvfsC1KgF2/WGrsfBVntDjZp5J5SqNMucJGndzBwPY4bBxLuy9VE33hr2+022PmrDTfJOJ1UbljlJ0ppb8AlMvDs7lDpzPNSqCx36QddBsPluro8q5cAyJ0latRjhvZeyAjfpH7Doc2ixA/S+GDod7iVFpJxZ5iRJKzb/Q5gwIjsXbvZkqN0QOn0fuh6bLXbvKJxUEixzkqSvLbuw79jhMPlBWLIQWneHvldDx/5Qp2HeCSUtxzInSYJPZ8Irt2ZrpH78TnbotOyEbBSuRYe800laBcucJFVXS5fAm0/AmJvhzccgLoUtvgO9fuWFfaWEWOYkqbr5dGa2PuqYYfDpdGjYAnY/A3Y6GjbeKu90ktaSZU6SqoOlS+Htp2D0TTDlEYhLYMu94YDfZxf4rblB3gklrSPLnCRVZfM+gHG3ZodSP34H6jeD3U7LFrrfaMu800kqAsucJFU1McJ/n4XRN8Lkh2DpImi3x9eL3Neqk3dCSUVkmZOkqmL+hzD+9uxQ6kdvZTNSe/wIuh0HzbbJO52kCmKZk6SUxZgtbj/6RnhtZHZduLa7wp7nZstsOSNVqvIsc5KUoi/mwvgR2SjcnClQpzF0+wGU/QA22T7vdJIqkWVOklISIzx4Joy/AxYvgFZl0O8a6Hgo1K6fdzpJOQgxxrwz5CaE8BkwJe8ca6EZMCfvEGvBvBUvtcyp5d0uxrhhZXxQCGE28E5lfJakZG0eY2y+/MbqPjI3JcZYlneINRVCGG3eipNaXkgvc4p5K+uzVvQFLUlrokbeASRJkrTuLHOSJEkJq+5lbmjeAdaSeStWankhvczmlaQiq9YTICRJklJX3UfmJEmSkmaZkyRJSli1KHMhhAtDCBNCCK+EEB4PIbQsbG8fQnghhPBlCGHwcq+ZFkKYWHhNpV2eYD3yHhBCmBJCmBpC+Hll5l1N5hBCuLKQa0IIoWu51ywpPP+VEML9CeQdFEJ4s3AbVMl5Lw4hTC5k+kcIoUlhe+0Qwk2Fv9XxIYS9yr3mmcLfxLLf8SYlnrdbYfvUwr+DUFl5V5N5gxDCsEK210MI55V7TW7fE5L0lRhjlb8Bjcrd/ylwbeH+JkB34CJg8HKvmQY0SyEvUBN4C9gSqA2MBzqUSOYDgUeAAOwCvFjuefNK8G9ihXmBjYC3Cz+bFu43rcS83wVqFe7/Efhj4f4pwE3l/j7GADUKj58BynL6/a5L3pcKv/NQ+HfQu0QyHwmMKNyvX/huaFd4nNv3hDdv3rwtu1WLkbkY46flHjYAYmH7BzHGl4FFuQRbiXXIuzMwNcb4doxxITAC6FcpYQtWlrmQY3jMjAKahBA2q8xsK7IOefcHnogxfhRjnAs8ARxQiXkfjzEuLjwcBbQu3O8APFV4zgfAx0DuF+Vd27yF33GjGOOoGGMEhgOHlEjmCDQIIdQC6gELgU9X8BaSlItqUeYAQggXhRDeA44CfrUGL4nA4yGEMSGEkyo23betZd5WwHvlHk8vbKtUK8m8qmx1QwijQwijQgiHVF7SzFrmLYnfccHxZCNXkI3C9g0h1AohbAF0A9qUe+5NhUOAv6zsw5blrEneVmS/02Xy/P3CNzPfDcwHZgLvApfEGD8q7Mv1e0KSoAqVuRDCkyGEV1dw6wcQY7wgxtgGuA04dQ3esmeMsSvQGzglhPCdEs9b4Sog8+YxW9rpSODPIYStSjxvhVpd3sJzLgAWk2UGuJGs+IwG/gw8Dywp7DsqxrgjsEfhdkyJ561w65h550LGlsAWwNkhhC0L+yr0e0KS1kSVWZs1xrjvGj71NuBh4Nereb8ZhZ8fhBD+QfaF/ux6hfzm+xcz7wy+ORrTurCtqNYx80qzlfsdvx1CeAbYiezcv1LMOwPYa7ntz6x3yHJWlzeEcBzQB+hVOBRJ4bDgmeWe8zzwRmHfst/vZyGE28n+hoeXaN65fH1YE3L6G15RZrL/2Xg0xrgI+CCE8BzZoey3K/p7QpLWRJUZmVuVEMI25R72Ayav5vkNQggbLrtPdmL0qxWX8Fufv1Z5gZeBbUIIW4QQagMDgMqeHbqyzPcDx4bMLsAnMcaZIYSmIYQ6hdc2A3YHXivVvMBjwHcLuZuS/U08Vol5DwDOAfrGGD8vt71+4W+UEMJ+wOIY42uFw5jNCts3ICsolfk3vFZ5C7/jT0MIuxQOBx8LjKysvKvKTHZodZ/CcxqQTdKYnPf3hCQtUy1WgAgh3ANsBywF3gF+HGOcEULYlOxwT6PCvnlkJ2g3A/5ReHkt4PYY40WlmjfG+GkI4UCyw1Y1gRsrM+9qMgfgarLJAp8DP4gxjg4h7Ab8rfD8GsCfY4w3lGrewmuOB84vvMVFMcabKjHvVKAO8GFh06gY449DCO3ISuVSspGsE2KM7xTKxbPABmR/E08CZ8UYK+WQ5trmLbymDLiZbJLBI8BpsRK/oFaRuSFwE9l3QyCbjXtx4VBrbt8TkrRMtShzkiRJVVW1OMwqSZJUVVnmJEmSEmaZkyRJSphlTpIkKWGWOUmSpIRZ5iRJkhJmmZMkSUqYZU7VTgjhuBBCn8p6nSRJFanKrM0qrU4I4VJgX2BLYGAIYWtgCLAAeAB4Bfgt8AHZlf3nAkcBLYDrKz+xJEmrZ5lTtRBC2ArYPcbYubCYOsCPgV/GGP8bQriLbH3Y38QY3yy8ZhugLjALOAZ4qfKTS5K0ah5mVZUXQtgOeAbYPIQwDmiwbBewbD27WHi8tNxLf0q23u3fgPqVkVWSpLXlyJyqvBjjlBDCMGBajPH6ciNzfwMuDCF8DtxBdph1SAhhJnA/8DRwLtnInCRJJSnEGFf/LClxIYSRwG9jjC/nnUWSpGLyMKuqi47Aq3mHkCSp2CxzqvJCCBsCi2KMXxQuLzImhFAjhNA+hDDES45IklJmmVN1sAPfHJWbCBydUxZJkorKMqcqL8b4Qozx8HKb7gb6kF12RJKkpFnmVF1dRXbpEUmSkmaZU7UUY/w30LTcppNDCNeGEM7JK5MkSevCS5NIkiQlzJE5SZKkhFnmJEmSEmaZkyRJSphlTpIkKWGWOUmSpIRZ5iRJkhJmmZMkSUqYZU6SJClh/w9XCsObTlNCvwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GA1 = imnn.lfi.GaussianApproximation(\n",
    "#     parameter_estimates=Î¸_fid,#IMNN.get_estimate(target_data),\n",
    "#     invF=np.expand_dims(np.linalg.inv(F_2PCF), 0),\n",
    "#     prior=prior,\n",
    "    \n",
    "#     gridsize=500)\n",
    "# ax = GA1.marginal_plot(\n",
    "#     known=Î¸_fid,\n",
    "#     label=\"2-pt function\",\n",
    "#     axis_labels=[\"$\\Omega_m$\", \"$\\sigma_8$\"],\n",
    "#     colours=\"k\", linestyle='-');\n",
    "\n",
    "GA = imnn.lfi.GaussianApproximation(\n",
    "    parameter_estimates=Î¸_fid,\n",
    "    invF=np.expand_dims(np.linalg.inv(IMNN.F), 0),\n",
    "    prior=prior,\n",
    "    gridsize=400)\n",
    "ax = GA.marginal_plot(\n",
    "    #ax=ax,\n",
    "    label=\"graph IMNN\",\n",
    "    axis_labels=[r\"$f^{\\rm local}_{\\rm NL}$\", r\"$f^{\\rm EQ}_{\\rm NL}$\"],\n",
    "    colours=\"C1\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iOrmL0jBarYo",
   "metadata": {
    "id": "iOrmL0jBarYo"
   },
   "source": [
    "# compute score estimates over some test data at $\\vartheta^\\pm$\n",
    "\n",
    "Beyond information quantification, the goal of gIMNNs over cosmic graphs is to condense massive catalogues into a couple of numbers to do density estimation for cosmological parameter constraints, like those done in [MÃ¤kinen et al 2021](https://arxiv.org/abs/2107.07405). To do this effectively, the network *doesn't* need to spit out *exact* predictions for the cosmological parameters like regression networks. Rather, it just needs to be able to distinguish a simulation at one parameter value from another consistently. We're going to compress our test graphs down to parameter estimates using the IMNN method `IMNN.get_estimate(d)`. What this code does is returns the score estimator for the parameters, obtained via the transformation        \n",
    "$$ \\hat{\\theta}_{\\alpha} = \\theta^{\\rm fid}_\\alpha + \\textbf{F}^{-1}_{\\alpha \\beta} \\frac{\\partial \\mu_i}{\\partial \\theta_\\beta} \\textbf{C}^{-1}_{ij} \\textbf({x}(\\textbf{w}, \\textbf{d}) - {\\mu})_j $$\n",
    "where ${x}(\\textbf{w}, \\textbf{d})$ are the network summaries.\n",
    "\n",
    "\n",
    "We show this below by computing score summaries over graphs obtained from the *Quijote* suite at $\\vartheta^\\pm$ *with* cosmic variance -- e.g. non seed-matched simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e721342-8173-4a5a-a194-dfeefdcea66b",
   "metadata": {
    "id": "5e721342-8173-4a5a-a194-dfeefdcea66b"
   },
   "outputs": [],
   "source": [
    "_test_derv = load_obj('testgraphs/test_derv_graphs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe28c36-f5ea-44f2-b83a-197b58d5ab18",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4fe28c36-f5ea-44f2-b83a-197b58d5ab18",
    "outputId": "86d780b4-811c-4578-bc39-d74971fdbcff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 800, 1)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_test_derv.n_node.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9879431-b648-4a1d-b3b6-c4cd9d55ca9e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9879431-b648-4a1d-b3b6-c4cd9d55ca9e",
    "outputId": "774a4f07-9fb0-4ddd-811e-f8ea75a185f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assembling with r_connect =  0.2\n"
     ]
    }
   ],
   "source": [
    "_test_derv = getgraphs(key, _test_derv, r_connect, num_halos, simulator_args={**simulator_args, \"include_pos\": True, \n",
    "                                                                              'mass_cut': 1.4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xN-bpFhDa_XJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xN-bpFhDa_XJ",
    "outputId": "66e72e01-1cb9-4ee2-ecf9-acbb4b713b11"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 200, 7)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_test_derv.nodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BoiprKskZ_gB",
   "metadata": {
    "id": "BoiprKskZ_gB"
   },
   "outputs": [],
   "source": [
    "# add noise and re-compute noise\n",
    "keys = jax.random.split(key, num=800)\n",
    "# vmap over noise-free fiducial simulation\n",
    "noisyderv = jax.vmap(noise_simulator)(keys, _test_derv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yE1Dz22KadMq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yE1Dz22KadMq",
    "outputId": "47664839-46db-4ff2-86a2-8f7597de67a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 200, 4)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisyderv.nodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0a1e42-4419-4676-b167-f6282a8ce9da",
   "metadata": {
    "id": "6f0a1e42-4419-4676-b167-f6282a8ce9da"
   },
   "outputs": [],
   "source": [
    "np = jnp\n",
    "# vmap the network score estimator over the test dataset !\n",
    "test_derv_estimates = jax.vmap(IMNN.get_estimate)(noisyderv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "M9ThUksC4usK",
   "metadata": {
    "id": "M9ThUksC4usK"
   },
   "source": [
    "Make scatterplot of test dataset summaries (cosmic variance *and* noisy masses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2f794d-034d-4cd2-8e5b-208adb84bfe5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "id": "0a2f794d-034d-4cd2-8e5b-208adb84bfe5",
    "outputId": "e230f4bb-faa4-4f9b-f994-cfdf2be1545e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,axs = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(7,3))\n",
    "_maxnodes = np.max(noisyderv.n_node)\n",
    "\n",
    "ax = axs[0]\n",
    "ax.scatter(test_derv_estimates[::4, 0], test_derv_estimates[::4, 1], color='k', \n",
    "           label=r'$\\Omega_m^-$', marker='+')\n",
    "ax.scatter(test_derv_estimates[2::4, 0], test_derv_estimates[2::4, 1], color='green', \n",
    "           label=r'$\\Omega_m^+$', marker='+')\n",
    "ax.set_xlabel(r'$x_1 = \\hat{\\Omega}_m$')\n",
    "ax.set_ylabel(r'$x_2 = \\hat{\\sigma}_8$')\n",
    "ax.legend(framealpha=0.)\n",
    "\n",
    "ax = axs[1]\n",
    "ax.scatter(test_derv_estimates[1::4, 0], test_derv_estimates[1::4, 1], color='red', \n",
    "           label=r'$\\sigma_8^-$')\n",
    "\n",
    "ax.scatter(test_derv_estimates[3::4, 0], test_derv_estimates[3::4, 1], color='orange', \n",
    "           label=r'$\\sigma_8^+$')\n",
    "ax.legend(framealpha=0.)\n",
    "ax.set_xlabel(r'$x_1 = \\hat{\\Omega}_m$')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WOqfC0pDZZ5e",
   "metadata": {
    "id": "WOqfC0pDZZ5e"
   },
   "source": [
    "We see that the network is indeed able to distinguish between the different simulations, even with added noise, in line with the degeneracy expected between the two parameters $\\Omega_m$ and $\\sigma_8$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OpuWdMELc4vc",
   "metadata": {
    "id": "OpuWdMELc4vc"
   },
   "source": [
    "# congrats !\n",
    "You made it through the cosmicGraphs walkthrough. The full analysis done in the paper can be repeated here, on a single GPU ! We hope this helps illustrate our approach to information extraction from large-scale structure"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
